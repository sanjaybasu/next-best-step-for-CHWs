{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Block 1: Basic Imports and Configuration\n",
    "\n",
    "# Basic imports\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional, Any, Set, Union\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Add these imports for time and other operations\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "import random\n",
    "\n",
    "# Progress tracking\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    from tqdm import tqdm as std_tqdm\n",
    "except ImportError:\n",
    "    # Define simple fallback if tqdm not available\n",
    "    def tqdm(iterable, *args, **kwargs):\n",
    "        return iterable\n",
    "    std_tqdm = tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Block 2: Device Configuration and Constants\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Intervention mapping (corresponds to the 9 interventions in your paper)\n",
    "INTERVENTIONS = {\n",
    "    'SUBSTANCE_USE_SUPPORT': 0,\n",
    "    'MENTAL_HEALTH_SUPPORT': 1,\n",
    "    'CHRONIC_CONDITION_MANAGEMENT': 2,\n",
    "    'FOOD_ASSISTANCE': 3,\n",
    "    'HOUSING_ASSISTANCE': 4,\n",
    "    'TRANSPORTATION_ASSISTANCE': 5,\n",
    "    'UTILITY_ASSISTANCE': 6,\n",
    "    'CHILDCARE_ASSISTANCE': 7,\n",
    "    'WATCHFUL_WAITING': 8\n",
    "}\n",
    "\n",
    "# Running statistics for normalization\n",
    "class RunningStats:\n",
    "    \"\"\"Track running statistics for normalization.\"\"\"\n",
    "    def __init__(self, epsilon=1e-4):\n",
    "        self.mean = 0\n",
    "        self.var = 1\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        \"\"\"Update statistics with new data.\"\"\"\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.detach().cpu().numpy()\n",
    "\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "\n",
    "        delta = batch_mean - self.mean\n",
    "        new_count = self.count + batch_count\n",
    "\n",
    "        self.mean = self.mean + delta * batch_count / new_count\n",
    "        m_a = self.var * self.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + delta**2 * self.count * batch_count / new_count\n",
    "        self.var = M2 / new_count\n",
    "        self.count = new_count\n",
    "\n",
    "    def normalize(self, x):\n",
    "        \"\"\"Normalize data using stored statistics.\"\"\"\n",
    "        return (x - self.mean) / (np.sqrt(self.var) + 1e-8)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Block 3: ClinicalState Class\n",
    "\n",
    "@dataclass\n",
    "class ClinicalState:\n",
    "    \"\"\"Representation of a patient's clinical state.\"\"\"\n",
    "    patient_id: str\n",
    "    timestamp: datetime\n",
    "    features: Dict[str, Any]  # Demographic, clinical, and social features\n",
    "    risk_summary: Dict[str, float]  # Risk assessments across domains\n",
    "    history: List[Dict]  # Previous encounters and interventions\n",
    "\n",
    "    def to_tensor(self, cache=None):\n",
    "        \"\"\"Convert state to tensor representation with dynamic risk factors.\"\"\"\n",
    "        # Create feature vector\n",
    "        feature_vector = []\n",
    "\n",
    "        # Add demographic features\n",
    "        feature_vector.extend([\n",
    "            self.features.get('age', 30) / 100,  # Normalize age\n",
    "            1.0 if self.features.get('gender') == 'Male' else 0.0,\n",
    "        ])\n",
    "\n",
    "        # Add risk scores - ensure variation by adding noise\n",
    "        risk_score = self.features.get('riskScore', 0.5)\n",
    "\n",
    "        # Add small random noise to risk score to break symmetry\n",
    "        if risk_score == 0.5:  # If default value\n",
    "            risk_score += random.uniform(-0.1, 0.1)\n",
    "\n",
    "        feature_vector.extend([\n",
    "            max(0.0, min(1.0, self.risk_summary.get('medical_risk_mentions', 0) / 5.0)),\n",
    "            max(0.0, min(1.0, self.risk_summary.get('behavioral_risk_mentions', 0) / 5.0)),\n",
    "            max(0.0, min(1.0, self.risk_summary.get('social_risk_mentions', 0) / 5.0)),\n",
    "            max(0.0, min(1.0, risk_score))\n",
    "        ])\n",
    "\n",
    "        # Add historical features\n",
    "        recent_interventions = [0] * len(INTERVENTIONS)\n",
    "        recent_outcomes = [0, 0]  # [positive, negative]\n",
    "\n",
    "        for encounter in self.history[-5:]:  # Last 5 encounters\n",
    "            if 'intervention' in encounter:\n",
    "                intervention_idx = INTERVENTIONS.get(encounter['intervention'], -1)\n",
    "                if intervention_idx >= 0:\n",
    "                    recent_interventions[intervention_idx] += 1\n",
    "\n",
    "            if encounter.get('isAcuteEvent', False):\n",
    "                recent_outcomes[0] += 1\n",
    "            else:\n",
    "                recent_outcomes[1] += 1\n",
    "\n",
    "        feature_vector.extend(recent_interventions)\n",
    "        feature_vector.extend(recent_outcomes)\n",
    "\n",
    "        # Convert to tensor\n",
    "        return torch.tensor(feature_vector, dtype=torch.float32, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualStreamNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual-stream neural network with separate paths for risk score processing\n",
    "    and other features. This architecture helps the model learn risk-specific\n",
    "    representations more effectively.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, n_actions, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        # Risk-specific stream\n",
    "        self.risk_stream = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim//4),\n",
    "            nn.LayerNorm(hidden_dim//4),\n",
    "            nn.SiLU(), # Swish activation function for better gradient properties\n",
    "            nn.Linear(hidden_dim//4, hidden_dim//4),\n",
    "            nn.LayerNorm(hidden_dim//4),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        # Main feature stream for all other features\n",
    "        self.feature_stream = nn.Sequential(\n",
    "            nn.Linear(state_dim-1, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Combination layer\n",
    "        self.combine = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim//4, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split risk score from other features\n",
    "        # Assuming risk score is at index 3\n",
    "        risk = x[:, 3:4]  # Extract risk score (dim slice to keep dimension)\n",
    "        features = torch.cat([x[:, :3], x[:, 4:]], dim=1)  # All other features\n",
    "\n",
    "        # Process through streams\n",
    "        risk_features = self.risk_stream(risk)\n",
    "        main_features = self.feature_stream(features)\n",
    "\n",
    "        # Combine and output\n",
    "        combined = torch.cat([main_features, risk_features], dim=1)\n",
    "        return self.combine(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Block 4: Helper Functions for Rewards and Conditions\n",
    "\n",
    "def some_condition_for_risk_change(state, action):\n",
    "    \"\"\"\n",
    "    Determines if the risk change condition is met.\n",
    "    For testing purposes, always returns True, but logs the risk value\n",
    "    using state.features['riskScore'] if available.\n",
    "    \"\"\"\n",
    "    # Try to retrieve risk from state.features first; fallback to state.risk.\n",
    "    if hasattr(state, 'features'):\n",
    "        risk_value = state.features.get('riskScore', None)\n",
    "    else:\n",
    "        risk_value = getattr(state, 'risk', None)\n",
    "    # If risk_value is still None, use a default (e.g., 0.5)\n",
    "    if risk_value is None:\n",
    "        risk_value = 0.5\n",
    "    print(f\"[DEBUG] Risk condition: risk_value = {risk_value} -> True\")\n",
    "    return True\n",
    "\n",
    "def compute_risk_change(state, action):\n",
    "    \"\"\"\n",
    "    Computes a reward based on risk reduction using the riskScore from state.features if available.\n",
    "    \"\"\"\n",
    "    if hasattr(state, 'features'):\n",
    "        current_risk = state.features.get('riskScore', 0.5)\n",
    "    else:\n",
    "        current_risk = getattr(state, 'risk', 0) or 0\n",
    "    reward = -1.0 * current_risk\n",
    "    print(f\"[DEBUG] Computed risk change: -1.0 * {current_risk} = {reward}\")\n",
    "    return reward\n",
    "\n",
    "def some_condition_for_acute_penalty(state, action):\n",
    "    \"\"\"\n",
    "    Returns True if the state indicates an acute event.\n",
    "    Assumes state has an 'acute_event' attribute.\n",
    "    \"\"\"\n",
    "    acute = getattr(state, 'acute_event', False)\n",
    "    print(f\"[DEBUG] Acute penalty condition: state.acute_event = {acute}\")\n",
    "    return acute\n",
    "\n",
    "def some_condition_for_exploration_bonus(state, action):\n",
    "    \"\"\"\n",
    "    Determines whether an exploration bonus should be applied.\n",
    "    For now, returns False.\n",
    "    \"\"\"\n",
    "    bonus_condition = False\n",
    "    print(f\"[DEBUG] Exploration bonus condition: {bonus_condition}\")\n",
    "    return bonus_condition\n",
    "\n",
    "def compute_exploration_bonus(state, action):\n",
    "    \"\"\"\n",
    "    Returns a constant bonus for exploration.\n",
    "    \"\"\"\n",
    "    bonus = 0.1\n",
    "    print(f\"[DEBUG] Computed exploration bonus: {bonus}\")\n",
    "    return bonus\n",
    "\n",
    "def some_condition_for_intervention_match(state, action):\n",
    "    \"\"\"\n",
    "    Checks whether the selected action matches the recommended intervention.\n",
    "    Assumes state has an attribute 'recommended_intervention'.\n",
    "    \"\"\"\n",
    "    recommended = getattr(state, 'recommended_intervention', None)\n",
    "    condition = recommended == action\n",
    "    print(f\"[DEBUG] Intervention match condition: state.recommended_intervention = {recommended}, action = {action} -> {condition}\")\n",
    "    return condition\n",
    "\n",
    "def compute_intervention_match_bonus(state, action):\n",
    "    \"\"\"\n",
    "    Computes a bonus for matching the recommended intervention.\n",
    "    \"\"\"\n",
    "    bonus = 10.0\n",
    "    print(f\"[DEBUG] Computed intervention match bonus: {bonus}\")\n",
    "    return bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Block 5: ClinicalEnvironment Class\n",
    "\n",
    "class ClinicalEnvironment:\n",
    "    \"\"\"Simulation environment for clinical decision-making with realistic patient dynamics.\"\"\"\n",
    "\n",
    "    def __init__(self, max_sequence_length=50):\n",
    "        \"\"\"\n",
    "        Initialize the clinical environment.\n",
    "\n",
    "        Args:\n",
    "            max_sequence_length: Maximum number of steps in an episode\n",
    "        \"\"\"\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.sequences = []\n",
    "        self.current_sequence = 0\n",
    "        self.current_step = 0\n",
    "        self.state_cache = {}  # Cache for state tensors\n",
    "\n",
    "        # Track intervention statistics for masking and exploration\n",
    "        self.action_counts = np.zeros(len(INTERVENTIONS))\n",
    "\n",
    "        # Track clinical outcomes for analysis\n",
    "        self.outcome_history = {\n",
    "            'acute_events': 0,\n",
    "            'risk_reductions': [],\n",
    "            'effective_interventions': defaultdict(int)\n",
    "        }\n",
    "\n",
    "    def set_sequences(self, sequences):\n",
    "        \"\"\"\n",
    "        Set patient sequences for simulation.\n",
    "\n",
    "        Args:\n",
    "            sequences: List of patient data sequences\n",
    "        \"\"\"\n",
    "        self.sequences = sequences\n",
    "        logging.info(f\"Loaded {len(sequences)} patient sequences into environment\")\n",
    "        print(f\"Sequence list length after setting: {len(self.sequences)}\")\n",
    "\n",
    "    def reset(self, sequence_idx=None):\n",
    "        \"\"\"\n",
    "        Reset environment to initial state with randomized risk profiles.\n",
    "\n",
    "        Args:\n",
    "            sequence_idx: Optional index to specify which sequence to use\n",
    "\n",
    "        Returns:\n",
    "            ClinicalState object representing initial state\n",
    "        \"\"\"\n",
    "        if sequence_idx is not None:\n",
    "            self.current_sequence = sequence_idx\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Check if sequences exist\n",
    "        if not self.sequences:\n",
    "            raise ValueError(\"No sequences loaded. Call set_sequences() first.\")\n",
    "\n",
    "        if self.current_sequence >= len(self.sequences):\n",
    "            self.current_sequence = 0\n",
    "\n",
    "        # Get sequence\n",
    "        sequence = self.sequences[self.current_sequence]\n",
    "\n",
    "        # Handle dict format\n",
    "        if isinstance(sequence, dict):\n",
    "            # Extract patient information\n",
    "            patient_id = sequence.get('patient_id', f\"patient_{self.current_sequence}\")\n",
    "\n",
    "            # Create randomized risk summary for variability\n",
    "            risk_summary = {\n",
    "                'medical_risk_mentions': random.uniform(0, 3.0),\n",
    "                'behavioral_risk_mentions': random.uniform(0, 3.0),\n",
    "                'social_risk_mentions': random.uniform(0, 3.0)\n",
    "            }\n",
    "\n",
    "            # Create initial risk score with some variability\n",
    "            risk_score = random.uniform(0.3, 0.7)\n",
    "            features = sequence.get('features', {})\n",
    "            features['riskScore'] = risk_score\n",
    "\n",
    "            # Create initial state\n",
    "            initial_state = ClinicalState(\n",
    "                patient_id=patient_id,\n",
    "                timestamp=datetime.now(),\n",
    "                features=features,\n",
    "                risk_summary=risk_summary,\n",
    "                history=sequence.get('history', [])\n",
    "            )\n",
    "\n",
    "            # Occasionally create high-risk patients (20% of the time)\n",
    "            if random.random() < 0.2:\n",
    "                # Increase risk score\n",
    "                high_risk_score = random.uniform(0.7, 0.9)\n",
    "                initial_state.features['riskScore'] = high_risk_score\n",
    "\n",
    "                # Increase a specific risk domain\n",
    "                domain = random.choice(['medical', 'behavioral', 'social'])\n",
    "                initial_state.risk_summary[f'{domain}_risk_mentions'] += random.uniform(2.0, 4.0)\n",
    "\n",
    "                logging.debug(f\"Created high-risk patient with {domain} risk\")\n",
    "\n",
    "            return initial_state\n",
    "        else:\n",
    "            logging.warning(f\"Unexpected sequence format: {type(sequence)}\")\n",
    "            # Create a dummy state as fallback with randomized risks\n",
    "            initial_state = ClinicalState(\n",
    "                patient_id=f\"dummy_{self.current_sequence}\",\n",
    "                timestamp=datetime.now(),\n",
    "                features={'riskScore': random.uniform(0.3, 0.7)},\n",
    "                risk_summary={\n",
    "                    'medical_risk_mentions': random.uniform(0, 3.0),\n",
    "                    'behavioral_risk_mentions': random.uniform(0, 3.0),\n",
    "                    'social_risk_mentions': random.uniform(0, 3.0)\n",
    "                },\n",
    "                history=[]\n",
    "            )\n",
    "            return initial_state\n",
    "\n",
    "    def generate_action_mask(self, state):\n",
    "        \"\"\"\n",
    "        Generate mask for valid actions in current state with clinical constraints.\n",
    "\n",
    "        Args:\n",
    "            state: Current clinical state\n",
    "\n",
    "        Returns:\n",
    "            Binary tensor mask of allowed actions\n",
    "        \"\"\"\n",
    "        # Default: all actions available\n",
    "        mask = torch.ones(len(INTERVENTIONS), dtype=torch.bool, device=DEVICE)\n",
    "\n",
    "        # Get recent interventions to avoid repetition\n",
    "        recent_interventions = [0] * len(INTERVENTIONS)\n",
    "        if hasattr(state, 'history') and len(state.history) > 0:\n",
    "            for encounter in state.history[-3:]:  # Last 3 encounters\n",
    "                if 'intervention' in encounter:\n",
    "                    intervention_idx = INTERVENTIONS.get(encounter['intervention'], -1)\n",
    "                    if intervention_idx >= 0:\n",
    "                        recent_interventions[intervention_idx] += 1\n",
    "\n",
    "        # Get risk assessments\n",
    "        medical_risk = state.risk_summary.get('medical_risk_mentions', 0)\n",
    "        behavioral_risk = state.risk_summary.get('behavioral_risk_mentions', 0)\n",
    "        social_risk = state.risk_summary.get('social_risk_mentions', 0)\n",
    "        risk_score = state.features.get('riskScore', 0.5)\n",
    "\n",
    "        # 1. Avoid repeating the exact same intervention back-to-back\n",
    "        if len(state.history) > 0 and 'intervention' in state.history[-1]:\n",
    "            last_intervention = state.history[-1]['intervention']\n",
    "            last_idx = INTERVENTIONS.get(last_intervention, -1)\n",
    "            if last_idx >= 0:\n",
    "                # Don't completely disable, but make it less likely (for exploration)\n",
    "                if random.random() < 0.8:  # 80% chance to mask the last action\n",
    "                    mask[last_idx] = False\n",
    "\n",
    "        # 2. Risk-based action recommendations\n",
    "        # For high-risk patients, constrain watchful waiting\n",
    "        if risk_score > 0.7 and random.random() < 0.8:\n",
    "            mask[INTERVENTIONS['WATCHFUL_WAITING']] = False\n",
    "\n",
    "        # Enable appropriate interventions based on risk profile\n",
    "        if medical_risk > 1.5:\n",
    "            mask[INTERVENTIONS['CHRONIC_CONDITION_MANAGEMENT']] = True\n",
    "        if behavioral_risk > 1.5:\n",
    "            mask[INTERVENTIONS['MENTAL_HEALTH_SUPPORT']] = True\n",
    "            mask[INTERVENTIONS['SUBSTANCE_USE_SUPPORT']] = True\n",
    "        if social_risk > 1.5:\n",
    "            mask[INTERVENTIONS['HOUSING_ASSISTANCE']] = True\n",
    "            mask[INTERVENTIONS['FOOD_ASSISTANCE']] = True\n",
    "\n",
    "        # For low medical risk patients, reduce likelihood of medical interventions\n",
    "        if medical_risk < 1.0 and random.random() < 0.7:\n",
    "            mask[INTERVENTIONS['CHRONIC_CONDITION_MANAGEMENT']] = False\n",
    "\n",
    "        # 3. Clinical pattern matching from notes\n",
    "        if hasattr(state, 'history') and len(state.history) > 0:\n",
    "            # Extract recent notes\n",
    "            recent_notes = ' '.join([str(h.get('encounter_note', '')) for h in state.history[-3:]])\n",
    "\n",
    "            # More specific triggers for interventions based on notes\n",
    "            if 'suicidal' in recent_notes or 'crisis' in recent_notes:\n",
    "                # Always enable mental health for crisis situations\n",
    "                mask[INTERVENTIONS['MENTAL_HEALTH_SUPPORT']] = True\n",
    "\n",
    "            if 'homeless' in recent_notes or 'evict' in recent_notes:\n",
    "                mask[INTERVENTIONS['HOUSING_ASSISTANCE']] = True\n",
    "\n",
    "            if 'hunger' in recent_notes or 'food' in recent_notes:\n",
    "                mask[INTERVENTIONS['FOOD_ASSISTANCE']] = True\n",
    "\n",
    "            if 'transport' in recent_notes or 'bus' in recent_notes or 'car' in recent_notes:\n",
    "                mask[INTERVENTIONS['TRANSPORTATION_ASSISTANCE']] = True\n",
    "\n",
    "            if 'child' in recent_notes or 'kids' in recent_notes or 'daycare' in recent_notes:\n",
    "                mask[INTERVENTIONS['CHILDCARE_ASSISTANCE']] = True\n",
    "\n",
    "            if 'utility' in recent_notes or 'electric' in recent_notes or 'water' in recent_notes:\n",
    "                mask[INTERVENTIONS['UTILITY_ASSISTANCE']] = True\n",
    "\n",
    "            if 'alcohol' in recent_notes or 'drug' in recent_notes or 'substance' in recent_notes:\n",
    "                mask[INTERVENTIONS['SUBSTANCE_USE_SUPPORT']] = True\n",
    "\n",
    "        # 4. Ensure action diversity - force at least 3 actions to be available\n",
    "        if torch.sum(mask) < 3:\n",
    "            # Find which actions to enable\n",
    "            masked_indices = torch.where(~mask)[0]\n",
    "            # Randomly select actions to enable until we have at least 3\n",
    "            indices_to_enable = masked_indices[torch.randperm(len(masked_indices))][:3-torch.sum(mask)]\n",
    "            for idx in indices_to_enable:\n",
    "                mask[idx] = True\n",
    "\n",
    "        # 5. Always allow watchful waiting as a fallback for low-risk patients\n",
    "        if risk_score < 0.5:\n",
    "            mask[INTERVENTIONS['WATCHFUL_WAITING']] = True\n",
    "\n",
    "        # Log mask statistics occasionally for debugging\n",
    "        print(f\"Action mask: {mask.cpu().numpy()} (allowing {torch.sum(mask).item()}/{len(INTERVENTIONS)} actions)\")\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def step(self, state, action):\n",
    "        \"\"\"Take action in environment and return next state, reward, done, info.\"\"\"\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Get sequence data\n",
    "        sequence = self.sequences[self.current_sequence]\n",
    "\n",
    "        # Check if episode is done\n",
    "        encounters = sequence.get('encounters', [])\n",
    "        done = (self.current_step >= len(encounters)) or (self.current_step >= self.max_sequence_length)\n",
    "\n",
    "        # Get next encounter data\n",
    "        if not done and len(encounters) > self.current_step:\n",
    "            next_encounter = encounters[self.current_step]\n",
    "        else:\n",
    "            next_encounter = {}  # Empty encounter if done\n",
    "            done = True\n",
    "\n",
    "        # Determine intervention from action\n",
    "        intervention = list(INTERVENTIONS.keys())[action]\n",
    "\n",
    "        # Extract risk factors\n",
    "        pre_risk = state.features.get('riskScore', 0.5)\n",
    "        behavioral_risk = state.risk_summary.get('behavioral_risk_mentions', 0)\n",
    "        medical_risk = state.risk_summary.get('medical_risk_mentions', 0)\n",
    "        social_risk = state.risk_summary.get('social_risk_mentions', 0)\n",
    "\n",
    "        # IMPROVED: Calculate intervention effectiveness based on matched needs\n",
    "        risk_change = 0.0\n",
    "\n",
    "        # Mapping interventions to appropriate risk domains with stronger effect when matched\n",
    "        if intervention == 'SUBSTANCE_USE_SUPPORT':\n",
    "            # More effective if behavioral risk is high\n",
    "            risk_change = 0.05 + (0.04 * behavioral_risk)\n",
    "        elif intervention == 'MENTAL_HEALTH_SUPPORT':\n",
    "            # More effective if behavioral risk is high\n",
    "            risk_change = 0.05 + (0.04 * behavioral_risk)\n",
    "        elif intervention == 'CHRONIC_CONDITION_MANAGEMENT':\n",
    "            # More effective if medical risk is high\n",
    "            risk_change = 0.05 + (0.04 * medical_risk)\n",
    "        elif intervention == 'HOUSING_ASSISTANCE':\n",
    "            # More effective if social risk is high\n",
    "            risk_change = 0.05 + (0.04 * social_risk)\n",
    "        elif intervention in ['FOOD_ASSISTANCE', 'TRANSPORTATION_ASSISTANCE', 'UTILITY_ASSISTANCE', 'CHILDCARE_ASSISTANCE']:\n",
    "            # More effective if social risk is high\n",
    "            risk_change = 0.04 + (0.03 * social_risk)\n",
    "        else:  # WATCHFUL_WAITING\n",
    "            # More effective for very low-risk patients\n",
    "            if pre_risk < 0.3:\n",
    "                risk_change = 0.03\n",
    "            else:\n",
    "                risk_change = 0.01  # Minimal impact for higher risk patients\n",
    "\n",
    "        # Add small randomness to risk changes\n",
    "        risk_change += random.uniform(-0.01, 0.01)\n",
    "\n",
    "        # Sometimes interventions are less effective\n",
    "        if random.random() < 0.1:  # 10% chance\n",
    "            risk_change *= 0.5\n",
    "\n",
    "        # Calculate post-risk and ensure bounds\n",
    "        post_risk = max(0.1, min(0.9, pre_risk - risk_change))\n",
    "\n",
    "        # Determine if acute event occurs (more likely with higher post-risk)\n",
    "        is_acute = random.random() < post_risk * 0.3\n",
    "\n",
    "        # REVISED REWARD FUNCTION - More heavily penalize acute events\n",
    "\n",
    "        # Base reward for reducing risk\n",
    "        risk_reduction_reward = risk_change * 10.0  # Reduced from 25.0\n",
    "\n",
    "        # Heavily penalize acute events and strongly reward prevention\n",
    "        if is_acute:\n",
    "            acute_penalty = -200.0  # Dramatically increased negative reward for acute events\n",
    "            prevention_bonus = 0.0\n",
    "        else:\n",
    "            acute_penalty = 0.0\n",
    "            prevention_bonus = 50.0  # Significantly increased reward for avoiding acute events\n",
    "\n",
    "        # Different penalties and bonuses based on risk level\n",
    "        if pre_risk > 0.7:  # High risk\n",
    "            prevention_bonus *= 1.5  # Increased bonus for high-risk prevention\n",
    "        elif pre_risk > 0.3:  # Medium risk\n",
    "            prevention_bonus *= 1.2  # Slight increase for medium risk\n",
    "\n",
    "        # Reward for diversity of interventions\n",
    "        diversity_bonus = 0.0\n",
    "        if hasattr(self, 'action_counts'):\n",
    "            total_actions = np.sum(self.action_counts) + 1  # Add 1 to avoid division by zero\n",
    "            # Calculate action frequency\n",
    "            action_frequency = self.action_counts[action] / total_actions\n",
    "            # Higher bonus for less frequent actions\n",
    "            diversity_bonus = 5.0 * (1.0 - action_frequency)\n",
    "\n",
    "        # Intervention matching bonus - reward for appropriate intervention-risk matching\n",
    "        intervention_match_bonus = 0.0\n",
    "        if (intervention in ['SUBSTANCE_USE_SUPPORT', 'MENTAL_HEALTH_SUPPORT'] and behavioral_risk > 1.5) or \\\n",
    "        (intervention == 'CHRONIC_CONDITION_MANAGEMENT' and medical_risk > 1.5) or \\\n",
    "        (intervention in ['HOUSING_ASSISTANCE', 'FOOD_ASSISTANCE', 'TRANSPORTATION_ASSISTANCE',\n",
    "                            'UTILITY_ASSISTANCE', 'CHILDCARE_ASSISTANCE'] and social_risk > 1.5):\n",
    "            intervention_match_bonus = 10.0\n",
    "\n",
    "        # Combine reward components\n",
    "        reward = risk_reduction_reward + acute_penalty + prevention_bonus + diversity_bonus + intervention_match_bonus\n",
    "\n",
    "        # Debug reward components\n",
    "        components = {\n",
    "            'risk': risk_reduction_reward,\n",
    "            'acute': acute_penalty if is_acute else prevention_bonus,\n",
    "            'diversity': diversity_bonus,\n",
    "            'match': intervention_match_bonus,\n",
    "            'total': reward\n",
    "        }\n",
    "        print(f\"Reward: {components}\")\n",
    "\n",
    "        # Update next_encounter with calculated values\n",
    "        next_encounter = {\n",
    "            **next_encounter,\n",
    "            'riskScore': post_risk,\n",
    "            'isAcuteEvent': is_acute\n",
    "        }\n",
    "\n",
    "        # Track action counts for diversity bonus\n",
    "        if not hasattr(self, 'action_counts'):\n",
    "            self.action_counts = np.zeros(len(INTERVENTIONS))\n",
    "        self.action_counts[action] += 1\n",
    "\n",
    "        # Construct next state\n",
    "        next_state = ClinicalState(\n",
    "            patient_id=state.patient_id,\n",
    "            timestamp=state.timestamp + timedelta(days=next_encounter.get('daysSinceLastEncounter', 7)),\n",
    "            features={**state.features, 'riskScore': post_risk},  # Update risk score in features\n",
    "            risk_summary=self._update_risk_summary(state.risk_summary, next_encounter, intervention),\n",
    "            history=state.history + [{'intervention': intervention, **next_encounter}]\n",
    "        )\n",
    "\n",
    "        # Track outcomes for analysis\n",
    "        if is_acute:\n",
    "            self.outcome_history['acute_events'] += 1\n",
    "        if risk_change > 0:\n",
    "            self.outcome_history['risk_reductions'].append(risk_change)\n",
    "            self.outcome_history['effective_interventions'][intervention] += 1\n",
    "\n",
    "        # Information dictionary\n",
    "        info = {\n",
    "            'intervention': intervention,\n",
    "            'is_acute': is_acute,\n",
    "            'pre_risk': pre_risk,\n",
    "            'post_risk': post_risk,\n",
    "            'risk_reduction': risk_change,\n",
    "            'safety_violation': False\n",
    "        }\n",
    "\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def _update_risk_summary(self, current_summary, new_encounter, intervention=None):\n",
    "        \"\"\"\n",
    "        Update risk summary based on new encounter data and intervention.\n",
    "\n",
    "        Args:\n",
    "            current_summary: Current risk summary dictionary\n",
    "            new_encounter: New encounter data dictionary\n",
    "            intervention: Intervention that was performed\n",
    "\n",
    "        Returns:\n",
    "            Updated risk summary dictionary\n",
    "        \"\"\"\n",
    "        updated_summary = current_summary.copy()\n",
    "\n",
    "        # Get current risk levels with safe defaults\n",
    "        medical_risk = current_summary.get('medical_risk_mentions', 0)\n",
    "        behavioral_risk = current_summary.get('behavioral_risk_mentions', 0)\n",
    "        social_risk = current_summary.get('social_risk_mentions', 0)\n",
    "\n",
    "        # Intervention effects - each intervention affects different risk domains\n",
    "        if intervention:\n",
    "            if intervention == 'CHRONIC_CONDITION_MANAGEMENT':\n",
    "                medical_risk = max(0, medical_risk - random.uniform(0.2, 0.5))\n",
    "            elif intervention in ['MENTAL_HEALTH_SUPPORT', 'SUBSTANCE_USE_SUPPORT']:\n",
    "                behavioral_risk = max(0, behavioral_risk - random.uniform(0.2, 0.5))\n",
    "            elif intervention in ['HOUSING_ASSISTANCE', 'FOOD_ASSISTANCE', 'TRANSPORTATION_ASSISTANCE',\n",
    "                                'UTILITY_ASSISTANCE', 'CHILDCARE_ASSISTANCE']:\n",
    "                social_risk = max(0, social_risk - random.uniform(0.2, 0.5))\n",
    "\n",
    "        # Add some natural progression - risks may increase slightly over time\n",
    "        if random.random() < 0.3:  # 30% chance\n",
    "            medical_risk += random.uniform(0, 0.2)\n",
    "        if random.random() < 0.3:\n",
    "            behavioral_risk += random.uniform(0, 0.2)\n",
    "        if random.random() < 0.3:\n",
    "            social_risk += random.uniform(0, 0.2)\n",
    "\n",
    "        # Extract additional information from encounter note if available\n",
    "        if 'encounter_note' in new_encounter and new_encounter['encounter_note']:\n",
    "            note_text = str(new_encounter['encounter_note'])\n",
    "\n",
    "            # Check for mentions of different risk categories\n",
    "            if re.search(r'breath|asthma|heart|pressure|sugar|pain|medication', note_text, re.IGNORECASE):\n",
    "                medical_risk += random.uniform(0.1, 0.3)\n",
    "\n",
    "            if re.search(r'anxious|depressed|sad|angry|alcohol|drug|substance', note_text, re.IGNORECASE):\n",
    "                behavioral_risk += random.uniform(0.1, 0.3)\n",
    "\n",
    "            if re.search(r'house|home|food|money|transport|bills|work', note_text, re.IGNORECASE):\n",
    "                social_risk += random.uniform(0.1, 0.3)\n",
    "\n",
    "        # Ensure risk values stay within reasonable bounds\n",
    "        updated_summary['medical_risk_mentions'] = min(5.0, max(0, medical_risk))\n",
    "        updated_summary['behavioral_risk_mentions'] = min(5.0, max(0, behavioral_risk))\n",
    "        updated_summary['social_risk_mentions'] = min(5.0, max(0, social_risk))\n",
    "\n",
    "        return updated_summary\n",
    "\n",
    "    def get_statistics(self):\n",
    "        \"\"\"\n",
    "        Get environment statistics for analysis.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of environment statistics\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            'total_episodes': self.current_sequence,\n",
    "            'acute_events': self.outcome_history['acute_events'],\n",
    "            'avg_risk_reduction': np.mean(self.outcome_history['risk_reductions']) if self.outcome_history['risk_reductions'] else 0,\n",
    "            'action_distribution': {action: float(count) for action, count in\n",
    "                                  enumerate(self.action_counts) if count > 0},\n",
    "            'effective_interventions': dict(self.outcome_history['effective_interventions'])\n",
    "        }\n",
    "\n",
    "        return stats\n",
    "    \n",
    "def get_recommendation(patient, sarsa_agent, status_quo_function):\n",
    "    \"\"\"Get optimal recommendation using risk-stratified hybrid approach.\"\"\"\n",
    "    # Extract risk score\n",
    "    risk_score = patient.features.get('riskScore', 0.5)\n",
    "\n",
    "    # Create state representation\n",
    "    state = patient.to_tensor()\n",
    "    action_mask = generate_action_mask(patient)\n",
    "\n",
    "    # Use SARSA only for high-risk patients where it shows some benefit\n",
    "    if risk_score > 0.7:  # High risk only\n",
    "        action, _ = sarsa_agent.select_action(state, action_mask, training=False)\n",
    "        return list(INTERVENTIONS.keys())[action.item()]\n",
    "    else:\n",
    "        # Use status quo for low/medium risk\n",
    "        action = status_quo_function(patient, action_mask)\n",
    "        return list(INTERVENTIONS.keys())[action.item()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Block 6: SARSAAgent Class\n",
    "\n",
    "class SARSAAgent:\n",
    "    \"\"\"SARSA reinforcement learning agent for clinical decision support.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        n_actions: int,\n",
    "        hidden_dim: int = 256,\n",
    "        learning_rate: float = 3e-4,\n",
    "        gamma: float = 0.99,\n",
    "        epsilon_start: float = 1.0,\n",
    "        epsilon_end: float = 0.1,\n",
    "        epsilon_decay: float = 0.995,\n",
    "        max_buffer_size: int = 100000\n",
    "    ):\n",
    "        self.state_dim = state_dim\n",
    "        self.n_actions = n_actions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.max_buffer_size = max_buffer_size\n",
    "\n",
    "        # Initialize Q-network using DualStreamNetwork\n",
    "        self.q_network = DualStreamNetwork(\n",
    "            state_dim=state_dim,\n",
    "            n_actions=n_actions,\n",
    "            hidden_dim=hidden_dim\n",
    "        ).to(DEVICE)\n",
    "\n",
    "\n",
    "        # Use a different initialization for better gradient flow\n",
    "        for m in self.q_network.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=1.414)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.q_network.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=1e-5,  # Add weight decay for regularization\n",
    "            eps=1e-8\n",
    "        )\n",
    "\n",
    "        # Initialize replay buffer\n",
    "        self.replay_buffer = []\n",
    "\n",
    "        # Initialize statistics tracking\n",
    "        self.state_rms = RunningStats()\n",
    "        self.reward_rms = RunningStats()\n",
    "        self.training_steps = 0\n",
    "        self.losses = []\n",
    "        self.q_values = []\n",
    "\n",
    "        # Value clipping for stability\n",
    "        self.min_value = -50.0\n",
    "        self.max_value = 50.0\n",
    "\n",
    "    def evaluate_risk_strata(self, env, num_episodes=30):\n",
    "        \"\"\"Evaluate model performance across different risk strata.\"\"\"\n",
    "        self.q_network.eval()\n",
    "\n",
    "        # Initialize containers for results\n",
    "        results = {\n",
    "            'low_risk': {'rewards': [], 'acute_events': [], 'count': 0},\n",
    "            'med_risk': {'rewards': [], 'acute_events': [], 'count': 0},\n",
    "            'high_risk': {'rewards': [], 'acute_events': [], 'count': 0}\n",
    "        }\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset(episode % len(env.sequences))\n",
    "            risk_score = state.features.get('riskScore', 0.5)\n",
    "\n",
    "            # Determine risk category\n",
    "            if risk_score < 0.3:\n",
    "                risk_category = 'low_risk'\n",
    "            elif risk_score < 0.7:\n",
    "                risk_category = 'med_risk'\n",
    "            else:\n",
    "                risk_category = 'high_risk'\n",
    "\n",
    "            results[risk_category]['count'] += 1\n",
    "            episode_reward = 0\n",
    "            episode_acute = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                state_tensor = state.to_tensor(env.state_cache)\n",
    "                action_mask = env.generate_action_mask(state)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    action, _ = self.select_action(state_tensor, action_mask, training=False)\n",
    "\n",
    "                next_state, reward, done, info = env.step(state, action.item())\n",
    "\n",
    "                episode_reward += reward\n",
    "                episode_acute += int(info.get('is_acute', False))\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            # Store results for this episode\n",
    "            results[risk_category]['rewards'].append(episode_reward)\n",
    "            results[risk_category]['acute_events'].append(episode_acute)\n",
    "\n",
    "        # Calculate metrics\n",
    "        summary = {}\n",
    "        for category, data in results.items():\n",
    "            if data['count'] > 0:\n",
    "                summary[category] = {\n",
    "                    'mean_reward': float(np.mean(data['rewards'])) if data['rewards'] else 0,\n",
    "                    'mean_acute_events': float(np.mean(data['acute_events'])) if data['acute_events'] else 0,\n",
    "                    'count': data['count']\n",
    "                }\n",
    "            else:\n",
    "                summary[category] = {'mean_reward': 0, 'mean_acute_events': 0, 'count': 0}\n",
    "\n",
    "        print(\"\\n=== Performance by Risk Strata ===\")\n",
    "        for category, metrics in summary.items():\n",
    "            print(f\"{category.replace('_', ' ').title()} (n={metrics['count']}): \" +\n",
    "                f\"Acute events = {metrics['mean_acute_events']:.4f}, \" +\n",
    "                f\"Reward = {metrics['mean_reward']:.2f}\")\n",
    "\n",
    "        return summary\n",
    "\n",
    "    def select_action(self, state: torch.Tensor, action_mask: torch.Tensor, training: bool = True) -> Tuple[torch.Tensor, float]:\n",
    "        \"\"\"Select action using risk-stratified exploration with diversity encouragement.\"\"\"\n",
    "        # Ensure state has correct format\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.from_numpy(state).float().to(DEVICE)\n",
    "\n",
    "        # Extract risk score from state\n",
    "        risk_score = state[3].item()  # Assuming risk score is at index 3\n",
    "\n",
    "        with torch.set_grad_enabled(training):\n",
    "            # Generate Q-values\n",
    "            q_values = self.q_network(state)\n",
    "\n",
    "            # Apply action mask\n",
    "            masked_q = q_values.clone()\n",
    "            masked_q[~action_mask] = float('-inf')\n",
    "\n",
    "            # Risk-stratified policy approach\n",
    "            if risk_score < 0.7:  # Low or medium risk patients - perform poorly here\n",
    "                if training:\n",
    "                    # Use status quo approach more frequently during training for low/medium risk\n",
    "                    if random.random() < 0.6:  # 60% of the time\n",
    "                        status_quo_action = self._get_status_quo_action(state, action_mask)\n",
    "                        return status_quo_action, 0.0\n",
    "                else:\n",
    "                    # For evaluation, still use status quo sometimes for low/medium risk\n",
    "                    if random.random() < 0.3:  # 30% of the time during evaluation\n",
    "                        status_quo_action = self._get_status_quo_action(state, action_mask)\n",
    "                        return status_quo_action, 0.0\n",
    "\n",
    "            # Apply action diversity correction - penalize overused actions\n",
    "            if training and hasattr(self, 'action_counts') and sum(self.action_counts) > 100:\n",
    "                action_probs = self.action_counts / sum(self.action_counts)\n",
    "\n",
    "                # Penalize overused actions in Q-values\n",
    "                for i, prob in enumerate(action_probs):\n",
    "                    if prob > 0.25:  # If any action used >25% of time\n",
    "                        masked_q[i] -= (prob - 0.25) * 10.0  # Progressive penalty\n",
    "\n",
    "                # Boost rarely used interventions\n",
    "                for i, prob in enumerate(action_probs):\n",
    "                    if prob < 0.05 and action_mask[i]:  # Rarely used but valid action\n",
    "                        masked_q[i] += 0.05 * 5.0  # Small boost\n",
    "\n",
    "            # Adjust epsilon based on risk level\n",
    "            if training:\n",
    "                if risk_score > 0.7:  # High risk - be more conservative\n",
    "                    effective_epsilon = min(0.1, self.epsilon * 0.5)\n",
    "                elif risk_score > 0.3:  # Medium risk - more exploration\n",
    "                    effective_epsilon = min(0.8, self.epsilon * 2.0)\n",
    "                else:  # Low risk\n",
    "                    effective_epsilon = self.epsilon * 0.8\n",
    "            else:\n",
    "                effective_epsilon = 0.0  # No exploration during evaluation\n",
    "\n",
    "            # Epsilon-greedy selection\n",
    "            if training and torch.rand(1) < effective_epsilon:\n",
    "                # Random action from valid actions\n",
    "                valid_actions = torch.nonzero(action_mask).squeeze()\n",
    "                if valid_actions.dim() == 0:\n",
    "                    action = valid_actions.unsqueeze(0)\n",
    "                else:\n",
    "                    action = valid_actions[torch.randint(0, len(valid_actions), (1,))]\n",
    "            else:\n",
    "                # Greedy action\n",
    "                action = torch.argmax(masked_q)\n",
    "\n",
    "            # Track Q-values during training\n",
    "            if training:\n",
    "                self.q_values.append(q_values[action].item())\n",
    "\n",
    "                # Track action counts if not already tracking\n",
    "                if not hasattr(self, 'action_counts'):\n",
    "                    self.action_counts = np.zeros(len(INTERVENTIONS))\n",
    "                self.action_counts[action.item()] += 1\n",
    "\n",
    "        return action, effective_epsilon\n",
    "\n",
    "    def _get_status_quo_action(self, state, action_mask):\n",
    "        \"\"\"Rule-based action selection that mimics status quo approach.\"\"\"\n",
    "        # Extract features for decision making\n",
    "        if isinstance(state, torch.Tensor):\n",
    "            # Get risk score and other relevant features\n",
    "            risk_score = state[3].item()\n",
    "            # You might need to extract other features based on your state representation\n",
    "        else:\n",
    "            # Handle non-tensor state\n",
    "            risk_score = state[3] if hasattr(state, '__getitem__') else 0.5\n",
    "\n",
    "        # Get valid actions\n",
    "        if isinstance(action_mask, torch.Tensor):\n",
    "            valid_indices = torch.nonzero(action_mask).squeeze()\n",
    "        else:\n",
    "            valid_indices = np.where(action_mask)[0]\n",
    "            valid_indices = torch.tensor(valid_indices, device=DEVICE)\n",
    "\n",
    "        # Ensure valid_indices is iterable\n",
    "        if valid_indices.dim() == 0:\n",
    "            valid_indices = valid_indices.unsqueeze(0)\n",
    "\n",
    "        # Define prioritized interventions based on risk level\n",
    "        if risk_score > 0.7:  # High risk\n",
    "            priorities = ['CHRONIC_CONDITION_MANAGEMENT', 'MENTAL_HEALTH_SUPPORT', 'SUBSTANCE_USE_SUPPORT']\n",
    "        elif risk_score > 0.3:  # Medium risk\n",
    "            priorities = ['CHRONIC_CONDITION_MANAGEMENT', 'WATCHFUL_WAITING', 'HOUSING_ASSISTANCE']\n",
    "        else:  # Low risk\n",
    "            priorities = ['WATCHFUL_WAITING', 'FOOD_ASSISTANCE', 'TRANSPORTATION_ASSISTANCE']\n",
    "\n",
    "        # Try each priority in order\n",
    "        for priority in priorities:\n",
    "            priority_idx = INTERVENTIONS.get(priority, -1)\n",
    "            if priority_idx >= 0 and priority_idx in valid_indices:\n",
    "                return torch.tensor(priority_idx, device=DEVICE)\n",
    "\n",
    "        # If no priority matches, return a random valid action\n",
    "        rand_idx = random.randint(0, len(valid_indices) - 1)\n",
    "        return valid_indices[rand_idx]\n",
    "\n",
    "    # Add this helper method in the SARSAAgent class\n",
    "    def _get_status_quo_action(self, state, action_mask):\n",
    "        \"\"\"Rule-based action selection for medium-risk patients.\"\"\"\n",
    "        # Simple rule-based approach that mimics standard clinical practice\n",
    "        # Extract features for decision making\n",
    "        if isinstance(state, torch.Tensor):\n",
    "            features = state.cpu().numpy()\n",
    "        else:\n",
    "            features = state\n",
    "\n",
    "        # Get action mask as numpy array\n",
    "        if isinstance(action_mask, torch.Tensor):\n",
    "            mask = action_mask.cpu().numpy()\n",
    "        else:\n",
    "            mask = action_mask\n",
    "\n",
    "        # Get indices of valid actions\n",
    "        valid_indices = np.where(mask)[0]\n",
    "\n",
    "        # Simple rule-based logic\n",
    "        # This is simplified - you should implement more sophisticated rules\n",
    "        # based on your domain knowledge\n",
    "        if len(valid_indices) > 0:\n",
    "            # Prioritize chronic condition management for medium risk\n",
    "            if INTERVENTIONS['CHRONIC_CONDITION_MANAGEMENT'] in valid_indices:\n",
    "                return torch.tensor(INTERVENTIONS['CHRONIC_CONDITION_MANAGEMENT'], device=DEVICE)\n",
    "            else:\n",
    "                # Otherwise, randomly select from valid actions\n",
    "                action_idx = valid_indices[np.random.randint(0, len(valid_indices))]\n",
    "                return torch.tensor(action_idx, device=DEVICE)\n",
    "        else:\n",
    "            # Fallback if no valid actions\n",
    "            return torch.tensor(0, device=DEVICE)\n",
    "    def update(self, state: torch.Tensor, action: torch.Tensor,\n",
    "            reward: float, next_state: torch.Tensor,\n",
    "            next_action: torch.Tensor, done: bool) -> float:\n",
    "        \"\"\"Update Q-network using SARSA update rule.\"\"\"\n",
    "        # Debug reward values\n",
    "        print(f\"Original reward: {reward}\")\n",
    "\n",
    "        # Normalize reward for stability\n",
    "        norm_reward = float(self.reward_rms.normalize(np.array([reward])))[0]\n",
    "        print(f\"Normalized reward: {norm_reward}\")\n",
    "\n",
    "        # Get current Q-value\n",
    "        current_q = self.q_network(state)[0, action]\n",
    "\n",
    "        # Get next Q-value\n",
    "        with torch.no_grad():\n",
    "            next_q = 0.0 if done else self.q_network(next_state)[0, next_action]\n",
    "            next_q = torch.clamp(next_q, self.min_value, self.max_value)\n",
    "\n",
    "        # Compute TD target\n",
    "        target = norm_reward + (1 - float(done)) * self.gamma * next_q\n",
    "\n",
    "        # Debug Q-values and target\n",
    "        print(f\"Current Q: {current_q.item():.4f}, Target: {target.item():.4f}\")\n",
    "\n",
    "        # Compute loss (Huber loss for stability)\n",
    "        loss = F.smooth_l1_loss(current_q, target)\n",
    "        print(f\"Loss before update: {loss.item():.6f}\")\n",
    "\n",
    "        # Optimize - check if gradients are flowing\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Check gradients\n",
    "        has_grad = False\n",
    "        max_grad = 0.0\n",
    "        for param in self.q_network.parameters():\n",
    "            if param.grad is not None:\n",
    "                max_grad_val = torch.max(torch.abs(param.grad)).item()\n",
    "                max_grad = max(max_grad, max_grad_val)\n",
    "                if max_grad_val > 0:\n",
    "                    has_grad = True\n",
    "        print(f\"Gradients flowing: {has_grad}, Max gradient: {max_grad:.6f}\")\n",
    "\n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update statistics\n",
    "        self.training_steps += 1\n",
    "        self.losses.append(loss.item())\n",
    "\n",
    "        # FORCE epsilon decay - this is critical\n",
    "        self.epsilon = max(\n",
    "            self.epsilon_end,\n",
    "            self.epsilon * self.epsilon_decay\n",
    "        )\n",
    "        print(f\"Epsilon updated to: {self.epsilon:.4f}\")\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def add_experience(self, state, action, reward, next_state, next_action, done):\n",
    "        # Calculate priority based on reward magnitude and acute events\n",
    "        priority = abs(reward) + 1.0\n",
    "\n",
    "        # Store transition with priority\n",
    "        if isinstance(state, torch.Tensor):\n",
    "            state = state.cpu()\n",
    "        if isinstance(next_state, torch.Tensor):\n",
    "            next_state = next_state.cpu()\n",
    "\n",
    "        # Store transition with priority\n",
    "        self.replay_buffer.append((state, action, reward, next_state, next_action, done, priority))\n",
    "\n",
    "        # Maintain buffer size\n",
    "        if len(self.replay_buffer) > self.max_buffer_size:\n",
    "            self.replay_buffer.pop(0)\n",
    "\n",
    "\n",
    "    def train_on_batch(self, batch_size=64):\n",
    "        \"\"\"Train with risk-stratified sampling.\"\"\"\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return 0.0\n",
    "\n",
    "        # Separate experiences by risk level\n",
    "        low_risk_experiences = []\n",
    "        med_high_risk_experiences = []\n",
    "\n",
    "        for exp in self.replay_buffer:\n",
    "            state = exp[0]\n",
    "            if isinstance(state, torch.Tensor):\n",
    "                risk_score = state[3].item()\n",
    "            else:\n",
    "                risk_score = state[3]\n",
    "\n",
    "            if risk_score < 0.3:\n",
    "                low_risk_experiences.append(exp)\n",
    "            else:\n",
    "                med_high_risk_experiences.append(exp)\n",
    "\n",
    "        # Calculate sample sizes with focus on medium/high risk\n",
    "        # Since SARSA is doing well on low-risk, we'll focus training on medium/high risk\n",
    "        low_risk_sample = min(int(batch_size * 0.3), len(low_risk_experiences))\n",
    "        med_high_risk_sample = min(batch_size - low_risk_sample, len(med_high_risk_experiences))\n",
    "\n",
    "        # If insufficient samples in either category, compensate with the other\n",
    "        if low_risk_sample < int(batch_size * 0.3) and med_high_risk_sample < batch_size - low_risk_sample:\n",
    "            if len(low_risk_experiences) > 0:\n",
    "                low_risk_sample = min(batch_size, len(low_risk_experiences))\n",
    "            else:\n",
    "                med_high_risk_sample = min(batch_size, len(med_high_risk_experiences))\n",
    "        elif low_risk_sample < int(batch_size * 0.3):\n",
    "            med_high_risk_sample = min(batch_size, len(med_high_risk_experiences))\n",
    "        elif med_high_risk_sample < batch_size - low_risk_sample:\n",
    "            low_risk_sample = min(batch_size, len(low_risk_experiences))\n",
    "\n",
    "        # Sample experiences\n",
    "        sampled_low_risk = random.sample(low_risk_experiences, low_risk_sample) if low_risk_sample > 0 else []\n",
    "        sampled_med_high_risk = random.sample(med_high_risk_experiences, med_high_risk_sample) if med_high_risk_sample > 0 else []\n",
    "\n",
    "        batch = sampled_low_risk + sampled_med_high_risk\n",
    "\n",
    "        if len(batch) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # Unpack batch\n",
    "        states, actions, rewards, next_states, next_actions, dones, _ = zip(*batch)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = torch.stack([torch.tensor(s, device=DEVICE) for s in states])\n",
    "        action_batch = torch.tensor(actions, device=DEVICE)\n",
    "        reward_batch = torch.tensor(rewards, dtype=torch.float32, device=DEVICE)\n",
    "        next_state_batch = torch.stack([torch.tensor(s, device=DEVICE) for s in next_states])\n",
    "        next_action_batch = torch.tensor(next_actions, device=DEVICE)\n",
    "        done_batch = torch.tensor(dones, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "        # Adjust rewards for medium/high risk samples\n",
    "        adjusted_rewards = []\n",
    "        for i, state in enumerate(states):\n",
    "            if isinstance(state, torch.Tensor):\n",
    "                risk_score = state[3].item()\n",
    "            else:\n",
    "                risk_score = state[3]\n",
    "\n",
    "            # Apply reward scaling to medium/high risk samples\n",
    "            # This increases the importance of medium/high risk outcomes\n",
    "            if risk_score >= 0.3:\n",
    "                adjusted_rewards.append(rewards[i] * 1.2)  # 20% boost\n",
    "            else:\n",
    "                adjusted_rewards.append(rewards[i])\n",
    "\n",
    "        adjusted_reward_batch = torch.tensor(adjusted_rewards, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "        # Get current Q values\n",
    "        current_q_values = self.q_network(state_batch).gather(1, action_batch.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Get next Q values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.q_network(next_state_batch).gather(1, next_action_batch.unsqueeze(1)).squeeze(1)\n",
    "            next_q_values = torch.clamp(next_q_values, self.min_value, self.max_value)\n",
    "\n",
    "        # Compute target Q values\n",
    "        target_q_values = adjusted_reward_batch + (1 - done_batch) * self.gamma * next_q_values\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
    "\n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update epsilon\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Save model and training state.\"\"\"\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        torch.save({\n",
    "            'network_state': self.q_network.state_dict(),\n",
    "            'optimizer_state': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon,\n",
    "            'training_steps': self.training_steps,\n",
    "            'state_rms': vars(self.state_rms),\n",
    "            'reward_rms': vars(self.reward_rms),\n",
    "            'losses': self.losses,\n",
    "            'q_values': self.q_values\n",
    "        }, path)\n",
    "        logging.info(f\"Model saved to {path}\")\n",
    "\n",
    "    def load(self, path: str) -> None:\n",
    "        \"\"\"Load model and training state.\"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            logging.error(f\"Model path {path} does not exist\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            checkpoint = torch.load(path, map_location=DEVICE)\n",
    "\n",
    "            # Load model and optimizer states\n",
    "            self.q_network.load_state_dict(checkpoint['network_state'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "\n",
    "            # Load training state\n",
    "            self.epsilon = checkpoint['epsilon']\n",
    "            self.training_steps = checkpoint['training_steps']\n",
    "\n",
    "            # Load statistics\n",
    "            self.state_rms = RunningStats()\n",
    "            for key, value in checkpoint['state_rms'].items():\n",
    "                setattr(self.state_rms, key, value)\n",
    "\n",
    "            self.reward_rms = RunningStats()\n",
    "            for key, value in checkpoint['reward_rms'].items():\n",
    "                setattr(self.reward_rms, key, value)\n",
    "\n",
    "            # Load metrics\n",
    "            if 'losses' in checkpoint:\n",
    "                self.losses = checkpoint['losses']\n",
    "            if 'q_values' in checkpoint:\n",
    "                self.q_values = checkpoint['q_values']\n",
    "\n",
    "            logging.info(f\"Model loaded from {path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading model: {str(e)}\")\n",
    "\n",
    "    def reset_network_if_needed(self):\n",
    "        \"\"\"Reset Q-network if it appears to be stuck.\"\"\"\n",
    "        # Check if Q-values are all very similar\n",
    "        if len(self.q_values) > 100:\n",
    "            recent_q = self.q_values[-100:]\n",
    "            q_std = np.std(recent_q)\n",
    "\n",
    "            if q_std < 0.01:  # Very little variation in Q-values\n",
    "                logging.warning(\"Q-network appears stuck. Resetting weights...\")\n",
    "\n",
    "                # Re-initialize network with different seed\n",
    "                for m in self.q_network.modules():\n",
    "                    if isinstance(m, nn.Linear):\n",
    "                        nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                        if m.bias is not None:\n",
    "                            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "                # Adjust learning rate\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] *= 2.0  # Increase learning rate temporarily\n",
    "\n",
    "                # Reset epsilon for more exploration\n",
    "                self.epsilon = min(1.0, self.epsilon * 2)\n",
    "\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def evaluate(self, env: ClinicalEnvironment, num_episodes: int = 10) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate agent performance.\"\"\"\n",
    "        self.q_network.eval()\n",
    "        rewards = []\n",
    "        acute_events = []\n",
    "        safety_violations = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_acute = 0\n",
    "            episode_violations = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                state_tensor = state.to_tensor(env.state_cache)\n",
    "                action_mask = env.generate_action_mask(state)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    action, _ = self.select_action(state_tensor, action_mask, training=False)\n",
    "\n",
    "                next_state, reward, done, info = env.step(state, action.item())\n",
    "\n",
    "                episode_reward += reward\n",
    "                episode_acute += int(info.get('is_acute', False))\n",
    "                episode_violations += int(info.get('safety_violation', False))\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            rewards.append(episode_reward)\n",
    "            acute_events.append(episode_acute)\n",
    "            safety_violations.append(episode_violations)\n",
    "\n",
    "        return {\n",
    "            'mean_reward': float(np.mean(rewards)),\n",
    "            'mean_acute_events': float(np.mean(acute_events)),\n",
    "            'mean_safety_violations': float(np.mean(safety_violations))\n",
    "        }\n",
    "\n",
    "    def init_model(self):\n",
    "        \"\"\"Initialize the Q-network with improved architecture.\"\"\"\n",
    "        # Use a more powerful network architecture\n",
    "        self.q_network = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.hidden_dim),\n",
    "            nn.LayerNorm(self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),  # Increased dropout for better generalization\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.LayerNorm(self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim // 2),\n",
    "            nn.LayerNorm(self.hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim // 2, self.n_actions)\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Use better initialization\n",
    "        for m in self.q_network.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # Orthogonal initialization with gain\n",
    "                nn.init.orthogonal_(m.weight, gain=1.414)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "        # Initialize optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskAwareQNetwork(nn.Module):\n",
    "    \"\"\"Q-network with explicit risk level handling.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, n_actions, hidden_dim=256):\n",
    "        super(RiskAwareQNetwork, self).__init__()\n",
    "\n",
    "        # Main network\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Risk-specific pathways\n",
    "        self.low_risk_path = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, n_actions)\n",
    "        )\n",
    "\n",
    "        self.med_risk_path = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, n_actions)\n",
    "        )\n",
    "\n",
    "        self.high_risk_path = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, n_actions)\n",
    "        )\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize network weights.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=1.414)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with risk-based pathway selection.\"\"\"\n",
    "        # Extract risk score (assuming it's at index 3)\n",
    "        risk_scores = x[:, 3]\n",
    "\n",
    "        # Process shared features\n",
    "        shared_features = self.shared_layers(x)\n",
    "\n",
    "        # Initialize output tensor\n",
    "        batch_size = x.size(0)\n",
    "        output = torch.zeros(batch_size, 9, device=x.device)\n",
    "\n",
    "        # Process each sample based on risk level\n",
    "        for i in range(batch_size):\n",
    "            risk = risk_scores[i].item()\n",
    "\n",
    "            if risk < 0.3:  # Low risk\n",
    "                output[i] = self.low_risk_path(shared_features[i:i+1]).squeeze(0)\n",
    "            elif risk < 0.7:  # Medium risk\n",
    "                output[i] = self.med_risk_path(shared_features[i:i+1]).squeeze(0)\n",
    "            else:  # High risk\n",
    "                output[i] = self.high_risk_path(shared_features[i:i+1]).squeeze(0)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSATrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        agent: SARSAAgent,\n",
    "        env: ClinicalEnvironment,\n",
    "        train_sequences: List,\n",
    "        val_sequences: List,\n",
    "        log_dir: str = \"sarsa_logs\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the SARSA trainer with improved diagnostics and monitoring.\n",
    "\n",
    "        Args:\n",
    "            agent: SARSAAgent instance for RL training\n",
    "            env: ClinicalEnvironment for simulating patient trajectories\n",
    "            train_sequences: List of patient sequence data for training\n",
    "            val_sequences: List of patient sequence data for validation\n",
    "            log_dir: Directory for storing logs and checkpoints\n",
    "        \"\"\"\n",
    "        self.agent = agent\n",
    "        self.env = env\n",
    "        self.train_sequences = train_sequences\n",
    "        self.val_sequences = val_sequences\n",
    "        self.log_dir = log_dir\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "        # Enhanced metrics tracking\n",
    "        self.train_metrics = defaultdict(list)\n",
    "        self.val_metrics = defaultdict(list)\n",
    "        self.clinical_outcomes = {\n",
    "            'acute_events_prevented': 0,\n",
    "            'acute_events_induced': 0,\n",
    "            'total_patients': len(train_sequences),\n",
    "            'nnt': float('inf'),\n",
    "            'nnh': float('inf'),\n",
    "            'sarsa_acute_rate': 0.0,\n",
    "            'status_quo_acute_rate': 0.0,\n",
    "            'acute_reduction': 0.0,\n",
    "            'low_risk_nnt': float('inf'),\n",
    "            'medium_risk_nnt': float('inf'),\n",
    "            'high_risk_nnt': float('inf')\n",
    "        }\n",
    "\n",
    "        # Detailed action statistics\n",
    "        self.action_stats = {action: {'count': 0, 'rewards': []} for action in INTERVENTIONS.keys()}\n",
    "\n",
    "        # Learning rate scheduler for adaptive learning\n",
    "        self.lr_patience = 5\n",
    "        self.stagnant_epochs = 0\n",
    "        self.best_reward = -float('inf')\n",
    "\n",
    "        # Configure logging with more details\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(log_dir, 'training.log'),\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "\n",
    "        # Also log to console\n",
    "        console = logging.StreamHandler()\n",
    "        console.setLevel(logging.INFO)\n",
    "        logging.getLogger('').addHandler(console)\n",
    "\n",
    "        # Record initialization parameters for reproducibility\n",
    "        self.config = {\n",
    "            'agent_params': {\n",
    "                'state_dim': agent.state_dim,\n",
    "                'n_actions': agent.n_actions,\n",
    "                'hidden_dim': agent.hidden_dim,\n",
    "                'learning_rate': agent.optimizer.param_groups[0]['lr'],\n",
    "                'gamma': agent.gamma,\n",
    "                'epsilon_start': agent.epsilon,\n",
    "                'epsilon_end': agent.epsilon_end,\n",
    "                'epsilon_decay': agent.epsilon_decay\n",
    "            },\n",
    "            'train_data_size': len(train_sequences),\n",
    "            'val_data_size': len(val_sequences),\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "\n",
    "        # Save configuration\n",
    "        with open(os.path.join(log_dir, 'config.json'), 'w') as f:\n",
    "            json.dump(self.config, f, indent=2)\n",
    "        \n",
    "    def train(self, n_epochs: int, batch_size: int = 64, \n",
    "             eval_freq: int = 10, checkpoint_freq: int = 50,\n",
    "             updates_per_epoch: int = 100, early_stopping_patience: int = 15):\n",
    "        \"\"\"\n",
    "        Train SARSA agent with enhanced monitoring and diagnostics.\n",
    "\n",
    "        Args:\n",
    "            n_epochs: Number of training epochs\n",
    "            batch_size: Batch size for replay buffer sampling\n",
    "            eval_freq: Frequency of evaluation (in epochs)\n",
    "            checkpoint_freq: Frequency of model checkpointing (in epochs)\n",
    "            updates_per_epoch: Number of model updates per epoch\n",
    "            early_stopping_patience: Number of epochs with no improvement before stopping\n",
    "\n",
    "        Returns:\n",
    "            Tuple of final evaluation metrics and clinical impact metrics\n",
    "        \"\"\"\n",
    "        import time  # Import time module here to avoid NameError\n",
    "\n",
    "        logging.info(f\"Starting SARSA training for {n_epochs} epochs\")\n",
    "        logging.info(f\"Training on {len(self.train_sequences)} sequences\")\n",
    "        logging.info(f\"Agent parameters: LR={self.config['agent_params']['learning_rate']}, \" +\n",
    "                     f\"={self.agent.gamma}, ={self.agent.epsilon}\")\n",
    "\n",
    "        best_metrics = None\n",
    "        best_epoch = 0\n",
    "        no_improvement_epochs = 0\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "\n",
    "            # Record epsilon before collection\n",
    "            pre_collection_epsilon = self.agent.epsilon\n",
    "                        # Collect experience with detailed monitoring\n",
    "            collection_stats = self._collect_experience()\n",
    "\n",
    "            # Enhanced training with more diagnostic information\n",
    "            print(f\"\\n--- Epoch {epoch+1}/{n_epochs} Model Updates ---\")\n",
    "            epoch_losses = []\n",
    "\n",
    "            # Update model with batch training\n",
    "            for update_idx in range(updates_per_epoch):\n",
    "                loss = self.agent.train_on_batch(batch_size)\n",
    "                epoch_losses.append(loss)\n",
    "\n",
    "                # Print detailed diagnostics for first few updates and last update\n",
    "                if update_idx < 3 or update_idx == updates_per_epoch-1:\n",
    "                    print(f\"  Update {update_idx+1}/{updates_per_epoch}, Loss: {loss:.6f}\")\n",
    "\n",
    "                # Check if learning is happening\n",
    "                if update_idx > 0 and update_idx % 25 == 0:\n",
    "                    recent_losses = epoch_losses[-25:]\n",
    "                    loss_change = abs(recent_losses[0] - recent_losses[-1])\n",
    "                    print(f\"  Recent loss change: {loss_change:.6f}\")\n",
    "\n",
    "                    # If losses aren't changing, try a larger learning rate temporarily\n",
    "                    if loss_change < 1e-6 and self.agent.training_steps > 100:\n",
    "                        print(\"  Warning: Loss not changing. Applying learning rate boost.\")\n",
    "                        for param_group in self.agent.optimizer.param_groups:\n",
    "                            param_group['lr'] *= 1.5  # Temporary boost\n",
    "\n",
    "            # Debug statistics every 10 epochs\n",
    "            if epoch % 10 == 0:\n",
    "                # Summarize Q-values\n",
    "                if hasattr(self.agent, 'q_values') and len(self.agent.q_values) > 0:\n",
    "                    recent_q = self.agent.q_values[-100:]\n",
    "                    print(f\"Q-value stats: min={min(recent_q):.4f}, max={max(recent_q):.4f}, \"\n",
    "                          f\"mean={np.mean(recent_q):.4f}, std={np.std(recent_q):.4f}\")\n",
    "\n",
    "                # Check action distribution in replay buffer\n",
    "                if len(self.agent.replay_buffer) > 0:\n",
    "                    actions = [transition[1] for transition in self.agent.replay_buffer[-200:]]\n",
    "                    action_counts = {}\n",
    "                    for idx, name in enumerate(INTERVENTIONS.keys()):\n",
    "                        action_counts[name] = actions.count(idx)\n",
    "                    print(f\"Recent action distribution: {action_counts}\")\n",
    "\n",
    "            # Calculate epoch metrics\n",
    "            mean_loss = np.mean(epoch_losses) if epoch_losses else 0.0\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            epsilon_change = pre_collection_epsilon - self.agent.epsilon\n",
    "\n",
    "            # Check if model is stuck\n",
    "            if epoch > 20 and epoch % 20 == 0:\n",
    "                if hasattr(self.agent, 'reset_network_if_needed') and self.agent.reset_network_if_needed():\n",
    "                    logging.info(\"Reset Q-network weights due to stagnation\")\n",
    "\n",
    "            # Log detailed training metrics\n",
    "            self.train_metrics['loss'].append(mean_loss)\n",
    "            self.train_metrics['epsilon'].append(self.agent.epsilon)\n",
    "            self.train_metrics['update_time'].append(epoch_time)\n",
    "            self.train_metrics['epsilon_change'].append(epsilon_change)\n",
    "\n",
    "            # Add collection statistics to metrics\n",
    "            for key, value in collection_stats.items():\n",
    "                self.train_metrics[f'collection_{key}'].append(value)\n",
    "\n",
    "            # Enhanced logging\n",
    "            logging.info(f\"Epoch {epoch+1}/{n_epochs}, \" +\n",
    "                         f\"Loss: {mean_loss:.6f}, \" +\n",
    "                         f\"Epsilon: {self.agent.epsilon:.4f} (={epsilon_change:.4f}), \" +\n",
    "                         f\"Avg Reward: {collection_stats['mean_reward']:.4f}, \" +\n",
    "                         f\"Time: {epoch_time:.2f}s\")\n",
    "\n",
    "            # Periodic evaluation with more detailed metrics\n",
    "            if (epoch + 1) % eval_freq == 0:\n",
    "                eval_metrics = self._evaluate()\n",
    "\n",
    "                # Check for improvement\n",
    "                current_reward = eval_metrics['mean_reward']\n",
    "\n",
    "                logging.info(f\"Evaluation: \" +\n",
    "                           f\"Reward: {eval_metrics['mean_reward']:.2f}, \" +\n",
    "                           f\"Acute Events: {eval_metrics['mean_acute_events']:.2f}, \" +\n",
    "                           f\"Action Entropy: {eval_metrics.get('action_entropy', 0.0):.2f}\")\n",
    "\n",
    "                # Store detailed metrics\n",
    "                for k, v in eval_metrics.items():\n",
    "                    self.val_metrics[k].append(v)\n",
    "\n",
    "                # Early stopping and learning rate adjustment logic\n",
    "                if current_reward > self.best_reward:\n",
    "                    self.best_reward = current_reward\n",
    "                    self.stagnant_epochs = 0\n",
    "\n",
    "                    # Save best model\n",
    "                    best_metrics = eval_metrics\n",
    "                    best_epoch = epoch\n",
    "                    self.agent.save(os.path.join(self.log_dir, \"sarsa_best.pt\"))\n",
    "                    logging.info(f\"New best model saved (reward: {current_reward:.4f})\")\n",
    "                else:\n",
    "                    self.stagnant_epochs += 1\n",
    "\n",
    "                    # Adjust learning rate if stagnating\n",
    "                    if self.stagnant_epochs >= self.lr_patience:\n",
    "                        for param_group in self.agent.optimizer.param_groups:\n",
    "                            param_group['lr'] *= 0.5\n",
    "                            new_lr = param_group['lr']\n",
    "\n",
    "                        logging.info(f\"Learning rate reduced to {new_lr:.6f} after {self.stagnant_epochs} epochs without improvement\")\n",
    "                        self.stagnant_epochs = 0\n",
    "\n",
    "                    # Early stopping\n",
    "                    if no_improvement_epochs >= early_stopping_patience:\n",
    "                        logging.info(f\"Early stopping triggered after {no_improvement_epochs} epochs without improvement\")\n",
    "                        break\n",
    "\n",
    "            # Checkpoint saving with metadata\n",
    "            if (epoch + 1) % checkpoint_freq == 0:\n",
    "                checkpoint_path = os.path.join(self.log_dir, f\"sarsa_checkpoint_{epoch+1}.pt\")\n",
    "                self.agent.save(checkpoint_path)\n",
    "                logging.info(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "                # Save intermediate metrics\n",
    "                self.save_results(os.path.join(self.log_dir, f\"metrics_epoch_{epoch+1}.json\"))\n",
    "\n",
    "        # Final evaluation and metrics with detailed analysis\n",
    "        logging.info(\"Performing final comprehensive evaluation...\")\n",
    "        final_metrics = self._evaluate(n_episodes=min(100, len(self.val_sequences)))\n",
    "        clinical_impact = self._calculate_clinical_impact()\n",
    "\n",
    "        # Log completion with detailed statistics\n",
    "        logging.info(\"Training completed\")\n",
    "        logging.info(f\"Final metrics: {json.dumps(final_metrics, indent=2)}\")\n",
    "        logging.info(f\"Clinical impact: NNT={clinical_impact['nnt']:.2f}, \" +\n",
    "                   f\"Acute reduction: {clinical_impact['acute_reduction']*100:.2f}%\")\n",
    "\n",
    "        # Generate action distribution plot\n",
    "        try:\n",
    "            self._plot_action_distribution()\n",
    "            logging.info(\"Action distribution plot saved\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Could not generate plot: {str(e)}\")\n",
    "\n",
    "        # Save final model and best model\n",
    "        self.agent.save(os.path.join(self.log_dir, \"sarsa_final.pt\"))\n",
    "\n",
    "        # If best model is better than final, load it for final metrics\n",
    "        if best_metrics and best_metrics['mean_reward'] > final_metrics['mean_reward']:\n",
    "            logging.info(f\"Loading best model from epoch {best_epoch+1} for final metrics\")\n",
    "            self.agent.load(os.path.join(self.log_dir, \"sarsa_best.pt\"))\n",
    "            final_metrics = self._evaluate(n_episodes=min(100, len(self.val_sequences)))\n",
    "\n",
    "        return final_metrics, clinical_impact\n",
    "\n",
    "    def _collect_experience(self, n_episodes: int = 50) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Collect experience from environment with enhanced monitoring.\n",
    "\n",
    "        Args:\n",
    "            n_episodes: Number of episodes to collect experience from\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of collection statistics\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Starting Experience Collection ---\")\n",
    "\n",
    "        # Choose random subset of training sequences\n",
    "        episode_indices = np.random.choice(\n",
    "            len(self.train_sequences),\n",
    "            min(n_episodes, len(self.train_sequences)),\n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        # Track statistics\n",
    "        action_distribution = defaultdict(int)\n",
    "        reward_stats = []\n",
    "        acute_events = 0\n",
    "        transitions_added = 0\n",
    "\n",
    "        for episode_idx, seq_idx in enumerate(episode_indices):\n",
    "            self.env.current_sequence = seq_idx\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            episode_actions = []\n",
    "            episode_rewards = []\n",
    "\n",
    "            # Only print detailed info for first few and last episodes\n",
    "            verbose = (episode_idx < 2) or (episode_idx == len(episode_indices) - 1)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\nStarting episode {episode_idx+1}/{len(episode_indices)}, \" +\n",
    "                      f\"patient ID: {state.patient_id}\")\n",
    "\n",
    "            step_count = 0\n",
    "            episode_acute = 0\n",
    "\n",
    "            while not done:\n",
    "                step_count += 1\n",
    "\n",
    "                # Convert state\n",
    "                state_tensor = state.to_tensor(self.env.state_cache)\n",
    "\n",
    "                # Get action mask\n",
    "                action_mask = self.env.generate_action_mask(state)\n",
    "\n",
    "                # Select action\n",
    "                action, eps = self.agent.select_action(state_tensor, action_mask)\n",
    "                action_name = list(INTERVENTIONS.keys())[action.item()]\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Step {step_count}: Selected action: {action_name} (={eps:.2f})\")\n",
    "\n",
    "                # Take step\n",
    "                next_state, reward, done, info = self.env.step(state, action.item())\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"  Reward: {reward:.4f}, Done: {done}, Acute: {info.get('is_acute', False)}\")\n",
    "\n",
    "                # Track if acute event occurred\n",
    "                if info.get('is_acute', False):\n",
    "                    episode_acute += 1\n",
    "\n",
    "                # Convert next state\n",
    "                next_state_tensor = next_state.to_tensor(self.env.state_cache)\n",
    "\n",
    "                # Select next action for SARSA update\n",
    "                next_action_mask = self.env.generate_action_mask(next_state)\n",
    "                next_action, _ = self.agent.select_action(next_state_tensor, next_action_mask)\n",
    "\n",
    "                # Add to replay buffer\n",
    "                self.agent.add_experience(\n",
    "                    state_tensor,\n",
    "                    action.item(),\n",
    "                    reward,\n",
    "                    next_state_tensor,\n",
    "                    next_action.item(),\n",
    "                    done\n",
    "                )\n",
    "                transitions_added += 1\n",
    "\n",
    "                # Track detailed statistics\n",
    "                action_distribution[action_name] += 1\n",
    "                episode_actions.append(action_name)\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                # Track action-specific rewards for analysis\n",
    "                self.action_stats[action_name]['count'] += 1\n",
    "                self.action_stats[action_name]['rewards'].append(reward)\n",
    "\n",
    "                # Update state\n",
    "                state = next_state\n",
    "\n",
    "            # Episode summary\n",
    "            if verbose:\n",
    "                print(f\"Episode {episode_idx+1} complete:\")\n",
    "                print(f\"  Actions taken: {episode_actions}\")\n",
    "                print(f\"  Total reward: {sum(episode_rewards):.4f}\")\n",
    "                print(f\"  Acute events: {episode_acute}\")\n",
    "\n",
    "            reward_stats.append(sum(episode_rewards))\n",
    "            acute_events += episode_acute\n",
    "\n",
    "        # Calculate action entropy (measure of diversity)\n",
    "        action_counts = np.array(list(action_distribution.values()))\n",
    "        if len(action_counts) > 0 and np.sum(action_counts) > 0:\n",
    "            action_probs = action_counts / np.sum(action_counts)\n",
    "            action_entropy = -np.sum(action_probs * np.log2(action_probs + 1e-10))\n",
    "        else:\n",
    "            action_entropy = 0.0\n",
    "\n",
    "        # Calculate mean rewards by action type for analysis\n",
    "        action_mean_rewards = {}\n",
    "        for action, stats in self.action_stats.items():\n",
    "            if stats['count'] > 0:\n",
    "                action_mean_rewards[action] = np.mean(stats['rewards'][-n_episodes:])\n",
    "            else:\n",
    "                action_mean_rewards[action] = 0.0\n",
    "\n",
    "        # Overall statistics\n",
    "        print(\"\\n--- Experience Collection Summary ---\")\n",
    "        print(f\"Episodes: {len(episode_indices)}, Transitions: {transitions_added}\")\n",
    "        print(f\"Action distribution: {dict(action_distribution)}\")\n",
    "        print(f\"Action entropy: {action_entropy:.4f} bits\")\n",
    "        print(f\"Average episode reward: {np.mean(reward_stats):.4f}\")\n",
    "        print(f\"Acute events: {acute_events} ({acute_events/len(episode_indices):.4f} per episode)\")\n",
    "        print(f\"Mean rewards by action: {action_mean_rewards}\")\n",
    "        print(\"----------------------------------\\n\")\n",
    "\n",
    "        # Return collection statistics for metrics tracking\n",
    "        return {\n",
    "            'mean_reward': float(np.mean(reward_stats)),\n",
    "            'action_entropy': float(action_entropy),\n",
    "            'acute_rate': float(acute_events/max(1, len(episode_indices))),\n",
    "            'transitions': transitions_added,\n",
    "            'unique_actions': len(action_distribution)\n",
    "        }\n",
    "\n",
    "    def _evaluate(self, n_episodes: int = 20) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate SARSA performance with comprehensive metrics.\n",
    "\n",
    "        Args:\n",
    "            n_episodes: Number of episodes to evaluate on\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        self.agent.q_network.eval()\n",
    "        rewards = []\n",
    "        acute_events = []\n",
    "        safety_violations = []\n",
    "        intervention_counts = defaultdict(int)\n",
    "        risk_changes = []\n",
    "        q_values = []\n",
    "\n",
    "        # Choose random subset of validation sequences\n",
    "        episode_indices = np.random.choice(\n",
    "            len(self.val_sequences),\n",
    "            min(n_episodes, len(self.val_sequences)),\n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        # Track paths through diagnosis-treatment decision trees\n",
    "        clinical_pathway_counts = defaultdict(int)\n",
    "\n",
    "        print(f\"\\n--- Evaluating on {len(episode_indices)} episodes ---\")\n",
    "\n",
    "        for idx_num, idx in enumerate(episode_indices):\n",
    "            self.env.current_sequence = idx\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_acute = 0\n",
    "            episode_violations = 0\n",
    "            pre_risk = state.features.get('riskScore', 0.5)\n",
    "            pathway = []\n",
    "            done = False\n",
    "\n",
    "            verbose = idx_num < 2 or idx_num == len(episode_indices) - 1\n",
    "            if verbose:\n",
    "                print(f\"\\nEvaluation episode {idx_num+1}, patient {state.patient_id}\")\n",
    "\n",
    "            while not done:\n",
    "                # Get action with exploration disabled\n",
    "                state_tensor = state.to_tensor(self.env.state_cache)\n",
    "                action_mask = self.env.generate_action_mask(state)  # <-- FIXED LINE\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    action, _ = self.agent.select_action(\n",
    "                        state_tensor, action_mask, training=False\n",
    "                    )\n",
    "                    # Get Q-values for analysis\n",
    "                    q_values_tensor = self.agent.q_network(state_tensor)[0]\n",
    "                    q_values.append(q_values_tensor.cpu().numpy())\n",
    "\n",
    "                # Take step\n",
    "                next_state, reward, done, info = self.env.step(state, action.item())\n",
    "\n",
    "\n",
    "                # Track metrics\n",
    "                episode_reward += reward\n",
    "                episode_acute += int(info.get('is_acute', False))\n",
    "                episode_violations += int(info.get('safety_violation', False))\n",
    "\n",
    "                # Track intervention\n",
    "                intervention = info.get('intervention', 'UNKNOWN')\n",
    "                intervention_counts[intervention] += 1\n",
    "\n",
    "                # Track clinical pathway\n",
    "                pathway.append(intervention)\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"  Action: {intervention}, Reward: {reward:.2f}, Acute: {info.get('is_acute', False)}\")\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            # Record outcomes\n",
    "            rewards.append(episode_reward)\n",
    "            acute_events.append(episode_acute)\n",
    "            safety_violations.append(episode_violations)\n",
    "\n",
    "            # Track risk change\n",
    "            post_risk = state.features.get('riskScore', 0.5)\n",
    "            risk_change = pre_risk - post_risk\n",
    "            risk_changes.append(risk_change)\n",
    "\n",
    "            # Track pathway pattern (simplified)\n",
    "            pathway_key = '->'.join(pathway[:3]) + '...' if len(pathway) > 3 else '->'.join(pathway)\n",
    "            clinical_pathway_counts[pathway_key] += 1\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Episode result: Reward={episode_reward:.2f}, Acute events={episode_acute}\")\n",
    "                print(f\"Risk change: {pre_risk:.2f} -> {post_risk:.2f} (={risk_change:.2f})\")\n",
    "\n",
    "        # Calculate metrics\n",
    "\n",
    "        # Basic statistics\n",
    "        eval_metrics = {\n",
    "            'mean_reward': float(np.mean(rewards)),\n",
    "            'mean_acute_events': float(np.mean(acute_events)),\n",
    "            'mean_safety_violations': float(np.mean(safety_violations)),\n",
    "            'mean_risk_reduction': float(np.mean(risk_changes))\n",
    "        }\n",
    "\n",
    "        # Add intervention distribution\n",
    "        total_interventions = sum(intervention_counts.values())\n",
    "        if total_interventions > 0:\n",
    "            for intervention, count in intervention_counts.items():\n",
    "                eval_metrics[f'pct_{intervention}'] = count / total_interventions * 100\n",
    "\n",
    "        # Calculate action entropy (measure of diversity)\n",
    "        action_counts = np.array(list(intervention_counts.values()))\n",
    "        if len(action_counts) > 0 and np.sum(action_counts) > 0:\n",
    "            action_probs = action_counts / np.sum(action_counts)\n",
    "            eval_metrics['action_entropy'] = float(-np.sum(action_probs * np.log2(action_probs + 1e-10)))\n",
    "        else:\n",
    "            eval_metrics['action_entropy'] = 0.0\n",
    "\n",
    "        # Q-value analysis\n",
    "        if q_values:\n",
    "            q_values_array = np.vstack(q_values)\n",
    "            eval_metrics['mean_q_value'] = float(np.mean(q_values_array))\n",
    "            eval_metrics['max_q_value'] = float(np.max(q_values_array))\n",
    "            eval_metrics['q_value_std'] = float(np.std(q_values_array))\n",
    "\n",
    "            # Q-value action gap (difference between highest and second highest)\n",
    "            q_sorted = np.sort(q_values_array, axis=1)\n",
    "            if q_sorted.shape[1] >= 2:\n",
    "                action_gaps = q_sorted[:, -1] - q_sorted[:, -2]\n",
    "                eval_metrics['mean_action_gap'] = float(np.mean(action_gaps))\n",
    "\n",
    "        # Top clinical pathways\n",
    "        top_pathways = sorted(clinical_pathway_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        eval_metrics['top_pathways'] = {k: v for k, v in top_pathways}\n",
    "\n",
    "        # Print summary\n",
    "        print(\"\\n--- Evaluation Summary ---\")\n",
    "        print(f\"Mean reward: {eval_metrics['mean_reward']:.4f}\")\n",
    "        print(f\"Acute events: {eval_metrics['mean_acute_events']:.4f} per episode\")\n",
    "        print(f\"Risk reduction: {eval_metrics['mean_risk_reduction']:.4f}\")\n",
    "        print(f\"Action entropy: {eval_metrics['action_entropy']:.4f} bits\")\n",
    "        print(f\"Action distribution: {dict(intervention_counts)}\")\n",
    "        print(\"---------------------------\\n\")\n",
    "\n",
    "        return eval_metrics\n",
    "\n",
    "    def _calculate_clinical_impact(self, n_sequences=None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate detailed clinical impact metrics comparing SARSA to status quo.\n",
    "\n",
    "        Args:\n",
    "            n_sequences: Optional number of sequences for evaluation. \n",
    "                        If None, uses min(100, available sequences).\n",
    "                        \n",
    "        Returns:\n",
    "            Dictionary of clinical impact metrics including NNT, NNH, and statistical significance.\n",
    "        \"\"\"\n",
    "        # Collection metrics\n",
    "        sarsa_outcomes = defaultdict(list)\n",
    "        status_quo_outcomes = defaultdict(list)\n",
    "\n",
    "        print(\"\\n--- Calculating Clinical Impact ---\")\n",
    "        print(\"Comparing SARSA-guided vs. status quo care management\")\n",
    "\n",
    "        # Process validation sequences with parameter handling\n",
    "        if n_sequences is None:\n",
    "            n_sequences = min(100, len(self.val_sequences))\n",
    "        else:\n",
    "            n_sequences = min(n_sequences, len(self.val_sequences))\n",
    "\n",
    "        print(f\"Evaluating on {n_sequences} validation sequences\")\n",
    "\n",
    "            \n",
    "        sequence_indices = np.random.choice(len(self.val_sequences), n_sequences, replace=False)\n",
    "\n",
    "\n",
    "        for seq_idx, sequence_idx in enumerate(sequence_indices):\n",
    "            # Run SARSA trajectory\n",
    "            sarsa_trajectory = self._simulate_trajectory(sequence_idx, use_sarsa=True)\n",
    "\n",
    "            # Run status quo trajectory\n",
    "            status_trajectory = self._simulate_trajectory(sequence_idx, use_sarsa=False)\n",
    "\n",
    "            # Count acute events\n",
    "            sarsa_acute = sum(1 for step in sarsa_trajectory if step['is_acute'])\n",
    "            status_acute = sum(1 for step in status_trajectory if step['is_acute'])\n",
    "\n",
    "            # Track counts for NNT/NNH calculation\n",
    "            if sarsa_acute < status_acute:\n",
    "                self.clinical_outcomes['acute_events_prevented'] += (status_acute - sarsa_acute)\n",
    "            elif sarsa_acute > status_acute:\n",
    "                self.clinical_outcomes['acute_events_induced'] += (sarsa_acute - status_acute)\n",
    "\n",
    "            # Get initial risk score for stratification\n",
    "            if seq_idx < 5:  # Print details for a few examples\n",
    "                print(f\"\\nPatient {seq_idx+1}/{n_sequences}:\")\n",
    "                print(f\"  SARSA: {sarsa_acute} acute events, Status quo: {status_acute} acute events\")\n",
    "                print(f\"  Difference: {status_acute - sarsa_acute} events\")\n",
    "\n",
    "            # Store outcomes for further analysis\n",
    "            sarsa_outcomes['acute_events'].append(sarsa_acute)\n",
    "            status_quo_outcomes['acute_events'].append(status_acute)\n",
    "\n",
    "            # Track risk scores\n",
    "            sarsa_outcomes['final_risk'].append(sarsa_trajectory[-1]['risk'] if sarsa_trajectory else 0.5)\n",
    "            status_quo_outcomes['final_risk'].append(status_trajectory[-1]['risk'] if status_trajectory else 0.5)\n",
    "\n",
    "            # Store additional metrics for detailed analysis\n",
    "            self._store_trajectory_metrics(sarsa_trajectory, status_trajectory)\n",
    "\n",
    "        # Calculate absolute risk reduction\n",
    "        sarsa_rate = np.mean(sarsa_outcomes['acute_events'])\n",
    "        status_quo_rate = np.mean(status_quo_outcomes['acute_events'])\n",
    "        acute_reduction = status_quo_rate - sarsa_rate\n",
    "\n",
    "        print(f\"\\nOverall comparison:\")\n",
    "        print(f\"SARSA acute event rate: {sarsa_rate:.4f} per trajectory\")\n",
    "        print(f\"Status quo acute event rate: {status_quo_rate:.4f} per trajectory\")\n",
    "        print(f\"Absolute reduction: {acute_reduction:.4f} events per trajectory\")\n",
    "        print(f\"Relative reduction: {(acute_reduction/max(0.001, status_quo_rate))*100:.2f}%\")\n",
    "\n",
    "        # Calculate NNT and NNH\n",
    "        if acute_reduction > 0:\n",
    "            nnt = 1 / acute_reduction\n",
    "            nnh = float('inf')  # No harm observed\n",
    "            print(f\"Number needed to treat (NNT): {nnt:.2f}\")\n",
    "        else:\n",
    "            nnt = float('inf')  # No benefit observed\n",
    "            nnh = 1 / abs(acute_reduction) if acute_reduction < 0 else float('inf')\n",
    "            print(f\"No reduction observed. Number needed to harm (NNH): {nnh:.2f}\")\n",
    "\n",
    "        # Risk-stratified analysis\n",
    "        low_risk_nnt = self._calculate_stratified_nnt('low')\n",
    "        medium_risk_nnt = self._calculate_stratified_nnt('medium')\n",
    "        high_risk_nnt = self._calculate_stratified_nnt('high')\n",
    "\n",
    "        print(\"\\nRisk-stratified analysis:\")\n",
    "        print(f\"Low-risk patients: NNT = {low_risk_nnt:.2f}\")\n",
    "        print(f\"Medium-risk patients: NNT = {medium_risk_nnt:.2f}\")\n",
    "        print(f\"High-risk patients: NNT = {high_risk_nnt:.2f}\")\n",
    "\n",
    "        # Update clinical outcomes\n",
    "        self.clinical_outcomes.update({\n",
    "            'nnt': float(nnt),\n",
    "            'nnh': float(nnh),\n",
    "            'acute_reduction': float(acute_reduction),\n",
    "            'sarsa_acute_rate': float(sarsa_rate),\n",
    "            'status_quo_acute_rate': float(status_quo_rate),\n",
    "            'low_risk_nnt': float(low_risk_nnt),\n",
    "            'medium_risk_nnt': float(medium_risk_nnt),\n",
    "            'high_risk_nnt': float(high_risk_nnt)\n",
    "        })\n",
    "\n",
    "        # Calculate statistical significance\n",
    "        t_stat, p_value = stats.ttest_ind(\n",
    "            sarsa_outcomes['acute_events'],\n",
    "            status_quo_outcomes['acute_events']\n",
    "        )\n",
    "\n",
    "        self.clinical_outcomes['p_value'] = float(p_value)\n",
    "        self.clinical_outcomes['t_statistic'] = float(t_stat)\n",
    "        print(f\"Statistical significance: p-value = {p_value:.4f}\")\n",
    "\n",
    "        # Calculate confidence intervals\n",
    "        n = len(sarsa_outcomes['acute_events'])\n",
    "        std_diff = np.std(np.array(sarsa_outcomes['acute_events']) - np.array(status_quo_outcomes['acute_events']))\n",
    "        ci_width = 1.96 * std_diff / np.sqrt(n)\n",
    "\n",
    "        self.clinical_outcomes['reduction_ci_low'] = float(acute_reduction - ci_width)\n",
    "        self.clinical_outcomes['reduction_ci_high'] = float(acute_reduction + ci_width)\n",
    "\n",
    "        print(f\"95% CI for reduction: [{acute_reduction - ci_width:.4f}, {acute_reduction + ci_width:.4f}]\")\n",
    "        print(\"------------------------------------------------\\n\")\n",
    "\n",
    "        return self.clinical_outcomes\n",
    "\n",
    "    def _simulate_trajectory(self, sequence_idx: int, use_sarsa: bool = True) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Simulate intervention trajectory using either SARSA or status quo.\n",
    "\n",
    "        Args:\n",
    "            sequence_idx: Index of patient sequence to simulate\n",
    "            use_sarsa: Whether to use SARSA policy (True) or status quo (False)\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries containing trajectory information\n",
    "        \"\"\"\n",
    "        self.env.current_sequence = sequence_idx\n",
    "        state = self.env.reset()\n",
    "        trajectory = []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Get action mask\n",
    "            action_mask = self.env.generate_action_mask(state)\n",
    "\n",
    "            # Select action based on policy\n",
    "            if use_sarsa:\n",
    "                state_tensor = state.to_tensor(self.env.state_cache)\n",
    "                action, _ = self.agent.select_action(\n",
    "                    state_tensor, action_mask, training=False\n",
    "                )\n",
    "            else:\n",
    "                # Status quo uses rule-based decision making\n",
    "                action = self._get_status_quo_action(state, action_mask)\n",
    "\n",
    "            # Take step\n",
    "            next_state, reward, done, info = self.env.step(state, action.item())\n",
    "\n",
    "            # Store step details\n",
    "            trajectory.append({\n",
    "                'action': action.item(),\n",
    "                'intervention': info['intervention'],\n",
    "                'reward': reward,\n",
    "                'is_acute': info.get('is_acute', False),\n",
    "                'risk': info.get('post_risk', 0.5),\n",
    "                'risk_reduction': info.get('risk_reduction', 0),\n",
    "                'safety_violation': info.get('safety_violation', False)\n",
    "            })\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        return trajectory\n",
    "\n",
    "    def _get_status_quo_action(self, state: ClinicalState, action_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Implement rule-based status quo decision making with improved clinical logic.\n",
    "\n",
    "        Args:\n",
    "            state: Current clinical state\n",
    "            action_mask: Binary mask of allowed actions\n",
    "\n",
    "        Returns:\n",
    "            Tensor containing selected action index\n",
    "        \"\"\"\n",
    "        # Get risk assessments with safeguards against missing data\n",
    "        medical_risk = state.risk_summary.get('medical_risk_mentions', 0)\n",
    "        behavioral_risk = state.risk_summary.get('behavioral_risk_mentions', 0)\n",
    "        social_risk = state.risk_summary.get('social_risk_mentions', 0)\n",
    "        risk_score = state.features.get('riskScore', 0.5)\n",
    "\n",
    "        # Rule-based priority hierarchy based on clinical guidelines and risk level\n",
    "        priority = None\n",
    "\n",
    "        # Check recent history for patterns\n",
    "        recent_notes = \"\"\n",
    "        if hasattr(state, 'history') and len(state.history) > 0:\n",
    "            recent_notes = ' '.join([str(h.get('encounter_note', '')) for h in state.history[-3:]])\n",
    "\n",
    "        # High-risk patients (prioritize effective interventions)\n",
    "        if risk_score > 0.7:\n",
    "            # For very high risk, prioritize the domain with the highest risk\n",
    "            if medical_risk >= behavioral_risk and medical_risk >= social_risk:\n",
    "                priority = 'CHRONIC_CONDITION_MANAGEMENT'\n",
    "            elif behavioral_risk >= medical_risk and behavioral_risk >= social_risk:\n",
    "                # Choose between mental health and substance use based on notes\n",
    "                if 'substance' in recent_notes or 'alcohol' in recent_notes or 'drug' in recent_notes:\n",
    "                    priority = 'SUBSTANCE_USE_SUPPORT'\n",
    "                else:\n",
    "                    priority = 'MENTAL_HEALTH_SUPPORT'\n",
    "            else:\n",
    "                # Choose most appropriate social intervention\n",
    "                if 'housing' in recent_notes or 'homeless' in recent_notes:\n",
    "                    priority = 'HOUSING_ASSISTANCE'\n",
    "                elif 'food' in recent_notes or 'hunger' in recent_notes:\n",
    "                    priority = 'FOOD_ASSISTANCE'\n",
    "                else:\n",
    "                    priority = 'HOUSING_ASSISTANCE'  # Default to housing for high social need\n",
    "\n",
    "        # Medium-risk patients (balanced approach)\n",
    "        elif risk_score > 0.3:\n",
    "            # Check for domain with highest risk but with more balanced approach\n",
    "            domain_risks = [\n",
    "                ('medical', medical_risk, 'CHRONIC_CONDITION_MANAGEMENT'),\n",
    "                ('behavioral', behavioral_risk, None),  # Will determine specific intervention below\n",
    "                ('social', social_risk, None)  # Will determine specific intervention below\n",
    "            ]\n",
    "\n",
    "            # Sort by risk level (highest first)\n",
    "            domain_risks.sort(key=lambda x: x[1], reverse=True)\n",
    "            highest_domain, highest_risk, highest_intervention = domain_risks[0]\n",
    "\n",
    "            if highest_domain == 'medical':\n",
    "                priority = highest_intervention\n",
    "            elif highest_domain == 'behavioral':\n",
    "                # Determine specific behavioral intervention\n",
    "                if 'substance' in recent_notes or 'alcohol' in recent_notes:\n",
    "                    priority = 'SUBSTANCE_USE_SUPPORT'\n",
    "                else:\n",
    "                    priority = 'MENTAL_HEALTH_SUPPORT'\n",
    "            else:  # social domain\n",
    "                # Choose appropriate social intervention based on notes\n",
    "                if 'housing' in recent_notes:\n",
    "                    priority = 'HOUSING_ASSISTANCE'\n",
    "                elif 'food' in recent_notes:\n",
    "                    priority = 'FOOD_ASSISTANCE'\n",
    "                elif 'transport' in recent_notes:\n",
    "                    priority = 'TRANSPORTATION_ASSISTANCE'\n",
    "                elif 'utility' in recent_notes or 'electric' in recent_notes:\n",
    "                    priority = 'UTILITY_ASSISTANCE'\n",
    "                elif 'child' in recent_notes:\n",
    "                    priority = 'CHILDCARE_ASSISTANCE'\n",
    "                else:\n",
    "                    # Default social intervention based on program statistics\n",
    "                    priority = random.choices(\n",
    "                        ['HOUSING_ASSISTANCE', 'FOOD_ASSISTANCE', 'TRANSPORTATION_ASSISTANCE',\n",
    "                        'UTILITY_ASSISTANCE', 'CHILDCARE_ASSISTANCE'],\n",
    "                        weights=[0.3, 0.3, 0.2, 0.1, 0.1]\n",
    "                    )[0]\n",
    "\n",
    "        # Low-risk patients (less intensive interventions)\n",
    "        else:\n",
    "            # For low risk, more frequently use watchful waiting\n",
    "            if random.random() < 0.4:\n",
    "                priority = 'WATCHFUL_WAITING'\n",
    "            else:\n",
    "                # Address any noticeable domain risks\n",
    "                if medical_risk > 1.0:\n",
    "                    priority = 'CHRONIC_CONDITION_MANAGEMENT'\n",
    "                elif behavioral_risk > 1.0:\n",
    "                    priority = 'MENTAL_HEALTH_SUPPORT'\n",
    "                elif social_risk > 1.0:\n",
    "                    priority = random.choices(\n",
    "                        ['FOOD_ASSISTANCE', 'TRANSPORTATION_ASSISTANCE', 'UTILITY_ASSISTANCE'],\n",
    "                        weights=[0.4, 0.3, 0.3]\n",
    "                    )[0]\n",
    "                else:\n",
    "                    # No significant risks - use watchful waiting\n",
    "                    priority = 'WATCHFUL_                    WAITING'\n",
    "\n",
    "        # If no priority set, use reasonable default based on risk\n",
    "        if priority is None:\n",
    "            if risk_score > 0.5:\n",
    "                priority = 'CHRONIC_CONDITION_MANAGEMENT'\n",
    "            else:\n",
    "                priority = 'WATCHFUL_WAITING'\n",
    "\n",
    "        # Convert to action index with safeguards\n",
    "        try:\n",
    "            action_idx = list(INTERVENTIONS.keys()).index(priority)\n",
    "        except ValueError:\n",
    "            # Fallback if priority is invalid\n",
    "            action_idx = list(INTERVENTIONS.keys()).index('WATCHFUL_WAITING')\n",
    "\n",
    "        # Ensure action is valid\n",
    "        if action_idx < len(action_mask) and not action_mask[action_idx]:\n",
    "            # Find the highest priority valid action\n",
    "            for backup_priority in ['CHRONIC_CONDITION_MANAGEMENT', 'MENTAL_HEALTH_SUPPORT',\n",
    "                                  'HOUSING_ASSISTANCE', 'FOOD_ASSISTANCE', 'WATCHFUL_WAITING']:\n",
    "                backup_idx = list(INTERVENTIONS.keys()).index(backup_priority)\n",
    "                if backup_idx < len(action_mask) and action_mask[backup_idx]:\n",
    "                    action_idx = backup_idx\n",
    "                    break\n",
    "            # Final fallback - take first valid action\n",
    "            if not action_mask[action_idx]:\n",
    "                valid_indices = torch.nonzero(action_mask).squeeze()\n",
    "                if valid_indices.dim() == 0:\n",
    "                    action_idx = valid_indices.item()\n",
    "                else:\n",
    "                    action_idx = valid_indices[0].item()\n",
    "\n",
    "        return torch.tensor(action_idx, device=DEVICE)\n",
    "\n",
    "\n",
    "    def _calculate_stratified_nnt(self, risk_stratum: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate NNT for specific risk stratum.\n",
    "\n",
    "        Args:\n",
    "            risk_stratum: Risk level to calculate NNT for ('low', 'medium', or 'high')\n",
    "\n",
    "        Returns:\n",
    "            Number needed to treat for the specified risk stratum\n",
    "        \"\"\"\n",
    "        sarsa_events = []\n",
    "        status_events = []\n",
    "\n",
    "        for idx, val_item in enumerate(self.val_sequences):\n",
    "            # Handle different data formats\n",
    "            if isinstance(val_item, dict):\n",
    "                # Extract risk score from dictionary\n",
    "                risk_score = val_item.get('riskScore',\n",
    "                            val_item.get('features', {}).get('riskScore', 0.0))\n",
    "            elif isinstance(val_item, tuple) and len(val_item) > 0 and hasattr(val_item[0], 'features'):\n",
    "                # Handle tuple of (state, encounters)\n",
    "                risk_score = val_item[0].features.get('riskScore', 0.0)\n",
    "            else:\n",
    "                # Default risk score for other formats\n",
    "                risk_score = 0.5\n",
    "\n",
    "            # Filter by risk stratum\n",
    "            if risk_stratum == 'low' and risk_score <= 0.3:\n",
    "                pass  # Include in low risk\n",
    "            elif risk_stratum == 'medium' and 0.3 < risk_score <= 0.7:\n",
    "                pass  # Include in medium risk\n",
    "            elif risk_stratum == 'high' and risk_score > 0.7:\n",
    "                pass  # Include in high risk\n",
    "            else:\n",
    "                continue  # Skip if not in target stratum\n",
    "\n",
    "            # Run trajectories\n",
    "            sarsa_trajectory = self._simulate_trajectory(idx, use_sarsa=True)\n",
    "            status_trajectory = self._simulate_trajectory(idx, use_sarsa=False)\n",
    "\n",
    "            # Count acute events\n",
    "            sarsa_acute = sum(1 for step in sarsa_trajectory if step['is_acute'])\n",
    "            status_acute = sum(1 for step in status_trajectory if step['is_acute'])\n",
    "\n",
    "            sarsa_events.append(sarsa_acute)\n",
    "            status_events.append(status_acute)\n",
    "\n",
    "        # Calculate risk reduction\n",
    "        if not sarsa_events:  # No patients in this stratum\n",
    "            return float('inf')\n",
    "\n",
    "        sarsa_rate = np.mean(sarsa_events)\n",
    "        status_rate = np.mean(status_events)\n",
    "        reduction = status_rate - sarsa_rate\n",
    "\n",
    "        # Calculate NNT\n",
    "        if reduction > 0:\n",
    "            return 1 / reduction\n",
    "        else:\n",
    "            return float('inf')  # No benefit in this stratum\n",
    "\n",
    "    def _store_trajectory_metrics(self, sarsa_trajectory: List[Dict],\n",
    "                                status_trajectory: List[Dict]) -> None:\n",
    "        \"\"\"\n",
    "        Store additional trajectory metrics for detailed analysis.\n",
    "\n",
    "        Args:\n",
    "            sarsa_trajectory: Trajectory using SARSA policy\n",
    "            status_trajectory: Trajectory using status quo policy\n",
    "        \"\"\"\n",
    "        # Analyze intervention patterns\n",
    "        if not hasattr(self, 'intervention_patterns'):\n",
    "            self.intervention_patterns = {\n",
    "                'sarsa': defaultdict(int),\n",
    "                'status_quo': defaultdict(int)\n",
    "            }\n",
    "\n",
    "        # Store sequential patterns (bigrams)\n",
    "        for trajectory, policy in [(sarsa_trajectory, 'sarsa'), (status_trajectory, 'status_quo')]:\n",
    "            for i in range(len(trajectory) - 1):\n",
    "                current = trajectory[i]['intervention']\n",
    "                next_int = trajectory[i+1]['intervention']\n",
    "                bigram = f\"{current}->{next_int}\"\n",
    "                self.intervention_patterns[policy][bigram] += 1\n",
    "\n",
    "    def _plot_action_distribution(self):\n",
    "        \"\"\"Generate and save action distribution plot.\"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure(figsize=(12, 6))\n",
    "\n",
    "            # Get action counts from validation metrics\n",
    "            action_keys = [k for k in self.val_metrics.keys() if k.startswith('pct_')]\n",
    "            if not action_keys or len(self.val_metrics[action_keys[0]]) == 0:\n",
    "                return  # No data to plot\n",
    "\n",
    "            # Most recent evaluation\n",
    "            final_counts = {k.replace('pct_', ''): self.val_metrics[k][-1] for k in action_keys}\n",
    "\n",
    "            # Sort by value\n",
    "            sorted_actions = sorted(final_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            labels, values = zip(*sorted_actions)\n",
    "\n",
    "            # Create bar chart\n",
    "            plt.bar(labels, values, color='skyblue')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.title('Action Distribution in Final Evaluation')\n",
    "            plt.ylabel('Percentage (%)')\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save figure\n",
    "            plt.savefig(os.path.join(self.log_dir, 'action_distribution.png'), dpi=300)\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Plotting error: {str(e)}\")\n",
    "\n",
    "    def save_results(self, output_path: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Save training results and metrics.\n",
    "\n",
    "        Args:\n",
    "            output_path: Path to save results to (default: {log_dir}/results.json)\n",
    "        \"\"\"\n",
    "        if output_path is None:\n",
    "            output_path = os.path.join(self.log_dir, \"results.json\")\n",
    "\n",
    "        # Process metrics for JSON serialization\n",
    "        results = {\n",
    "            'clinical_outcomes': self.clinical_outcomes,\n",
    "            'train_metrics': {k: list(map(float, v)) for k, v in self.train_metrics.items()},\n",
    "            'val_metrics': {k: list(map(float, v)) for k, v in self.val_metrics.items()\n",
    "                         if not isinstance(v[0], dict)},  # Skip nested dicts\n",
    "            'action_stats': {action: {'count': stats['count'],\n",
    "                                   'mean_reward': float(np.mean(stats['rewards'])) if stats['rewards'] else 0.0}\n",
    "                         for action, stats in self.action_stats.items()},\n",
    "            'config': self.config,\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "\n",
    "        # Save top intervention patterns if available\n",
    "        if hasattr(self, 'intervention_patterns'):\n",
    "            # Get top 10 patterns for each policy\n",
    "            sarsa_patterns = sorted(self.intervention_patterns['sarsa'].items(),\n",
    "                                 key=lambda x: x[1], reverse=True)[:10]\n",
    "            status_patterns = sorted(self.intervention_patterns['status_quo'].items(),\n",
    "                                  key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "            results['intervention_patterns'] = {\n",
    "                'sarsa': {k: v for k, v in sarsa_patterns},\n",
    "                'status_quo': {k: v for k, v in status_patterns}\n",
    "            }\n",
    "\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        logging.info(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessedDataLoader:\n",
    "    \"\"\"Load processed clinical data from chunked files.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: str):\n",
    "        \"\"\"\n",
    "        Initialize the data loader with specified directory.\n",
    "\n",
    "        Args:\n",
    "            data_dir: Directory containing data splits\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    def load_split(self, split: str):\n",
    "        \"\"\"\n",
    "        Load data split with metadata by combining chunks.\n",
    "\n",
    "        Args:\n",
    "            split: Split name ('train', 'val', 'test')\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (sequences, metadata)\n",
    "        \"\"\"\n",
    "        # Path to split directory and metadata\n",
    "        split_dir = os.path.join(self.data_dir, split)\n",
    "\n",
    "        # Check if directory exists\n",
    "        if not os.path.isdir(split_dir):\n",
    "            print(f\"Directory not found: {split_dir}\")\n",
    "            print(f\"Available directories: {os.listdir(self.data_dir)}\")\n",
    "            raise FileNotFoundError(f\"Split directory not found: {split_dir}\")\n",
    "\n",
    "        # Get all chunk files\n",
    "        chunk_files = sorted([f for f in os.listdir(split_dir) if f.startswith('chunk_') and f.endswith('.pkl')])\n",
    "\n",
    "        if not chunk_files:\n",
    "            # Look for other data formats\n",
    "            sequences_file = os.path.join(split_dir, f\"{split}_sequences.npy\")\n",
    "            labels_file = os.path.join(split_dir, f\"{split}_labels.npy\")\n",
    "\n",
    "            if os.path.exists(sequences_file) and os.path.exists(labels_file):\n",
    "                print(f\"Found numpy data files instead of chunks for {split}\")\n",
    "                sequences = np.load(sequences_file, allow_pickle=True)\n",
    "                labels = np.load(labels_file, allow_pickle=True)\n",
    "\n",
    "                # Convert to list of dictionaries for consistency\n",
    "                data = []\n",
    "                for i in range(len(sequences)):\n",
    "                    data.append({\n",
    "                        'patient_id': f\"{split}_{i}\",\n",
    "                        'features': {'riskScore': 0.5},  # Default risk score\n",
    "                        'encounters': [{'daysSinceLastEncounter': 7}],  # Default encounter\n",
    "                        'history': [],\n",
    "                        'sequence': sequences[i],\n",
    "                        'label': labels[i]\n",
    "                    })\n",
    "\n",
    "                return data, {'n_sequences': len(data)}\n",
    "\n",
    "            print(f\"No data files found in {split_dir}\")\n",
    "            return [], {}\n",
    "\n",
    "        # Load chunks\n",
    "        sequences = []\n",
    "        for chunk_file in chunk_files:\n",
    "            chunk_path = os.path.join(split_dir, chunk_file)\n",
    "            try:\n",
    "                with open(chunk_path, 'rb') as f:\n",
    "                    chunk_data = pickle.load(f)\n",
    "                    print(f\"Loaded {chunk_file}: {len(chunk_data)} sequences\")\n",
    "\n",
    "                    # If it's a list, extend sequences\n",
    "                    if isinstance(chunk_data, list):\n",
    "                        sequences.extend(chunk_data)\n",
    "                    else:\n",
    "                        # If not a list, add as a single item\n",
    "                        sequences.append(chunk_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {chunk_file}: {str(e)}\")\n",
    "\n",
    "        # Try to load metadata\n",
    "        meta_path = os.path.join(split_dir, \"metadata.json\")\n",
    "        metadata = {}\n",
    "        try:\n",
    "            if os.path.exists(meta_path):\n",
    "                with open(meta_path, 'r') as f:\n",
    "                    metadata = json.load(f)\n",
    "            else:\n",
    "                # Generate basic metadata\n",
    "                metadata = {\n",
    "                    'n_sequences': len(sequences),\n",
    "                    'creation_date': datetime.now().strftime('%Y-%m-%d')\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading metadata: {str(e)}\")\n",
    "\n",
    "        # Handle empty data\n",
    "        if not sequences and os.path.exists(os.path.join(split_dir, f\"{split}.pkl\")):\n",
    "            try:\n",
    "                with open(os.path.join(split_dir, f\"{split}.pkl\"), 'rb') as f:\n",
    "                    sequences = pickle.load(f)\n",
    "                print(f\"Loaded {split}.pkl as fallback: {len(sequences) if isinstance(sequences, list) else 'N/A'}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {split}.pkl: {str(e)}\")\n",
    "\n",
    "        # Convert numpy arrays to lists if needed\n",
    "        if isinstance(sequences, np.ndarray):\n",
    "            sequences = sequences.tolist()\n",
    "\n",
    "        # Ensure each sequence has a patient ID and features\n",
    "        for i, seq in enumerate(sequences):\n",
    "            if isinstance(seq, dict):\n",
    "                if 'patient_id' not in seq:\n",
    "                    seq['patient_id'] = f\"{split}_{i}\"\n",
    "                if 'features' not in seq:\n",
    "                    seq['features'] = {'riskScore': 0.5}  # Default risk score\n",
    "            else:\n",
    "                # Handle non-dictionary sequences by wrapping them\n",
    "                sequences[i] = {\n",
    "                    'patient_id': f\"{split}_{i}\",\n",
    "                    'features': {'riskScore': 0.5},  # Default risk score\n",
    "                    'sequence_data': seq  # Store original data\n",
    "                }\n",
    "\n",
    "        print(f\"Total {split} sequences loaded: {len(sequences)}\")\n",
    "        return sequences, metadata\n",
    "\n",
    "    def create_synthetic_data(self, n_sequences=1000, output_dir=None):\n",
    "        \"\"\"\n",
    "        Create synthetic data for development and testing.\n",
    "\n",
    "        Args:\n",
    "            n_sequences: Number of sequences to generate\n",
    "            output_dir: Directory to save synthetic data (optional)\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (train_sequences, val_sequences, test_sequences)\n",
    "        \"\"\"\n",
    "        if output_dir is None:\n",
    "            output_dir = self.data_dir\n",
    "\n",
    "        # Create directories\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_dir, 'train'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_dir, 'val'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_dir, 'test'), exist_ok=True)\n",
    "\n",
    "        # Helper to create a synthetic patient\n",
    "        def create_synthetic_patient(patient_id):\n",
    "            # Determine patient risk profile\n",
    "            risk_type = np.random.choice(['low', 'medium', 'high'],\n",
    "                                         p=[0.3, 0.5, 0.2])\n",
    "\n",
    "            if risk_type == 'low':\n",
    "                base_risk = np.random.uniform(0.1, 0.3)\n",
    "                n_encounters = np.random.randint(3, 7)\n",
    "                medical_risk = np.random.uniform(0, 1.5)\n",
    "                behavioral_risk = np.random.uniform(0, 1.5)\n",
    "                social_risk = np.random.uniform(0, 1.5)\n",
    "            elif risk_type == 'medium':\n",
    "                base_risk = np.random.uniform(0.3, 0.7)\n",
    "                n_encounters = np.random.randint(5, 10)\n",
    "                medical_risk = np.random.uniform(1.0, 2.5)\n",
    "                behavioral_risk = np.random.uniform(1.0, 2.5)\n",
    "                social_risk = np.random.uniform(1.0, 2.5)\n",
    "            else:  # high\n",
    "                base_risk = np.random.uniform(0.7, 0.9)\n",
    "                n_encounters = np.random.randint(7, 15)\n",
    "                medical_risk = np.random.uniform(2.0, 4.0)\n",
    "                behavioral_risk = np.random.uniform(2.0, 4.0)\n",
    "                social_risk = np.random.uniform(2.0, 4.0)\n",
    "\n",
    "            # Create demographics\n",
    "            gender = np.random.choice(['Male', 'Female'])\n",
    "            age = np.random.randint(18, 85)\n",
    "            race = np.random.choice(['White', 'Black', 'Hispanic', 'Asian', 'Other'])\n",
    "            region = np.random.choice(['Virginia', 'Washington'])\n",
    "\n",
    "            # Create encounters\n",
    "            encounters = []\n",
    "            for i in range(n_encounters):\n",
    "                # Risk change over time (can increase or decrease)\n",
    "                risk_change = np.random.normal(-0.03, 0.1)  # Slight bias toward improvement\n",
    "                current_risk = min(0.95, max(0.05, base_risk + risk_change))\n",
    "\n",
    "                # Acute event probability based on risk\n",
    "                is_acute = np.random.random() < current_risk * 0.2\n",
    "\n",
    "                # Create encounter\n",
    "                encounters.append({\n",
    "                    'daysSinceLastEncounter': np.random.randint(1, 14),\n",
    "                    'riskScore': current_risk,\n",
    "                    'isAcuteEvent': is_acute,\n",
    "                    'encounter_note': self._generate_synthetic_note(medical_risk, behavioral_risk, social_risk)\n",
    "                })\n",
    "\n",
    "            # Create patient data\n",
    "            patient = {\n",
    "                'patient_id': f\"patient_{patient_id}\",\n",
    "                'features': {\n",
    "                    'age': age,\n",
    "                    'gender': gender,\n",
    "                    'race': race,\n",
    "                    'region': region,\n",
    "                    'riskScore': base_risk\n",
    "                },\n",
    "                'risk_summary': {\n",
    "                    'medical_risk_mentions': medical_risk,\n",
    "                    'behavioral_risk_mentions': behavioral_risk,\n",
    "                    'social_risk_mentions': social_risk\n",
    "                },\n",
    "                'encounters': encounters,\n",
    "                'history': []\n",
    "            }\n",
    "\n",
    "            return patient\n",
    "\n",
    "        # Generate sequences\n",
    "        all_sequences = [create_synthetic_patient(i) for i in range(n_sequences)]\n",
    "\n",
    "        # Split into train/val/test (70/15/15)\n",
    "        train_split = int(n_sequences * 0.7)\n",
    "        val_split = int(n_sequences * 0.85)\n",
    "\n",
    "        train_sequences = all_sequences[:train_split]\n",
    "        val_sequences = all_sequences[train_split:val_split]\n",
    "        test_sequences = all_sequences[val_split:]\n",
    "\n",
    "        # Save data if output directory is specified\n",
    "        self._save_synthetic_data(train_sequences, os.path.join(output_dir, 'train'))\n",
    "        self._save_synthetic_data(val_sequences, os.path.join(output_dir, 'val'))\n",
    "        self._save_synthetic_data(test_sequences, os.path.join(output_dir, 'test'))\n",
    "\n",
    "        print(f\"Created {len(train_sequences)} train, {len(val_sequences)} val, {len(test_sequences)} test sequences\")\n",
    "        return train_sequences, val_sequences, test_sequences\n",
    "\n",
    "    def _generate_synthetic_note(self, medical_risk, behavioral_risk, social_risk):\n",
    "        \"\"\"\n",
    "        Generate synthetic encounter note based on risk factors.\n",
    "\n",
    "        Args:\n",
    "            medical_risk: Medical risk factor (0-5)\n",
    "            behavioral_risk: Behavioral risk factor (0-5)\n",
    "            social_risk: Social risk factor (0-5)\n",
    "\n",
    "        Returns:\n",
    "            String containing synthetic encounter note\n",
    "        \"\"\"\n",
    "        notes = []\n",
    "\n",
    "        # Medical notes\n",
    "        if medical_risk > 2.5:\n",
    "            notes.append(np.random.choice([\n",
    "                \"Patient reports difficulty breathing and increased coughing.\",\n",
    "                \"Blood pressure remains elevated at 160/95.\",\n",
    "                \"Blood glucose is poorly controlled with readings >300.\",\n",
    "                \"Patient reports severe pain requiring increased medication.\",\n",
    "                \"Multiple chronic conditions showing poor control.\"\n",
    "            ]))\n",
    "        elif medical_risk > 1.0:\n",
    "            notes.append(np.random.choice([\n",
    "                \"Patient has moderate asthma symptoms.\",\n",
    "                \"Blood pressure slightly elevated at 145/85.\",\n",
    "                \"Blood glucose occasionally elevated.\",\n",
    "                \"Patient reports moderate pain levels.\",\n",
    "                \"Chronic conditions stable but requiring monitoring.\"\n",
    "            ]))\n",
    "\n",
    "        # Behavioral notes\n",
    "        if behavioral_risk > 2.5:\n",
    "            notes.append(np.random.choice([\n",
    "                \"Patient shows signs of severe depression with suicidal ideation.\",\n",
    "                \"Reports heavy alcohol consumption daily.\",\n",
    "                \"Anxiety symptoms significantly impacting daily functioning.\",\n",
    "                \"Reports using substances to cope with stress.\",\n",
    "                \"Missed multiple psychiatric appointments.\"\n",
    "            ]))\n",
    "        elif behavioral_risk > 1.0:\n",
    "            notes.append(np.random.choice([\n",
    "                \"Patient reports mild depressive symptoms.\",\n",
    "                \"Occasional alcohol use, sometimes excessive.\",\n",
    "                \"Moderate anxiety symptoms.\",\n",
    "                \"Past history of substance use, currently stable.\",\n",
    "                \"Engaged in behavioral health treatment with some compliance issues.\"\n",
    "            ]))\n",
    "\n",
    "        # Social notes\n",
    "        if social_risk > 2.5:\n",
    "            notes.append(np.random.choice([\n",
    "                \"Patient recently became homeless and is staying in shelter.\",\n",
    "                \"Reports having no food for past 3 days.\",\n",
    "                \"Electricity was shut off due to unpaid bills.\",\n",
    "                \"Lost transportation access, unable to attend appointments.\",\n",
    "                \"Cannot afford childcare, missing work and appointments.\"\n",
    "            ]))\n",
    "        elif social_risk > 1.0:\n",
    "            notes.append(np.random.choice([\n",
    "                \"Housing situation unstable but currently housed.\",\n",
    "                \"Limited food access, relying on food banks.\",\n",
    "                \"Struggling to pay utility bills but services maintained.\",\n",
    "                \"Transportation barriers make appointment attendance difficult.\",\n",
    "                \"Childcare challenges affecting appointment adherence.\"\n",
    "            ]))\n",
    "\n",
    "        # Add generic note if none generated\n",
    "        if not notes:\n",
    "            notes.append(\"Routine follow-up visit. Patient generally stable.\")\n",
    "\n",
    "        return \" \".join(notes)\n",
    "\n",
    "    def _save_synthetic_data(self, sequences, output_dir):\n",
    "        \"\"\"\n",
    "        Save synthetic data to disk.\n",
    "\n",
    "        Args:\n",
    "            sequences: List of sequence data\n",
    "            output_dir: Directory to save data\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Save in chunks of 500 to avoid memory issues\n",
    "        chunk_size = 500\n",
    "        for i in range(0, len(sequences), chunk_size):\n",
    "            chunk = sequences[i:i+chunk_size]\n",
    "            chunk_path = os.path.join(output_dir, f\"chunk_{i//chunk_size}.pkl\")\n",
    "\n",
    "            with open(chunk_path, 'wb') as f:\n",
    "                pickle.dump(chunk, f)\n",
    "\n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'n_sequences': len(sequences),\n",
    "            'creation_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "            'description': 'Synthetic patient data for SARSA training'\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(output_dir, \"metadata.json\"), 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSATrainer:\n",
    "    \"\"\"Trainer for SARSA reinforcement learning with comprehensive diagnostics and monitoring.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        agent: SARSAAgent,\n",
    "        env: ClinicalEnvironment,\n",
    "        train_sequences: List,\n",
    "        val_sequences: List,\n",
    "        log_dir: str = \"sarsa_logs\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the SARSA trainer with improved diagnostics and monitoring.\n",
    "\n",
    "        Args:\n",
    "            agent: SARSAAgent instance for RL training\n",
    "            env: ClinicalEnvironment for simulating patient trajectories\n",
    "            train_sequences: List of patient sequence data for training\n",
    "            val_sequences: List of patient sequence data for validation\n",
    "            log_dir: Directory for storing logs and checkpoints\n",
    "        \"\"\"\n",
    "        self.agent = agent\n",
    "        self.env = env\n",
    "        self.train_sequences = train_sequences\n",
    "        self.val_sequences = val_sequences\n",
    "        self.log_dir = log_dir\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "        # Enhanced metrics tracking\n",
    "        self.train_metrics = defaultdict(list)\n",
    "        self.val_metrics = defaultdict(list)\n",
    "        self.clinical_outcomes = {\n",
    "            'acute_events_prevented': 0,\n",
    "            'acute_events_induced': 0,\n",
    "            'total_patients': len(train_sequences),\n",
    "            'nnt': float('inf'),\n",
    "            'nnh': float('inf'),\n",
    "            'sarsa_acute_rate': 0.0,\n",
    "            'status_quo_acute_rate': 0.0,\n",
    "            'acute_reduction': 0.0,\n",
    "            'low_risk_nnt': float('inf'),\n",
    "            'medium_risk_nnt': float('inf'),\n",
    "            'high_risk_nnt': float('inf')\n",
    "        }\n",
    "\n",
    "        # Detailed action statistics\n",
    "        self.action_stats = {action: {'count': 0, 'rewards': []} for action in INTERVENTIONS.keys()}\n",
    "\n",
    "        # Learning rate scheduler for adaptive learning\n",
    "        self.lr_patience = 5\n",
    "        self.stagnant_epochs = 0\n",
    "        self.best_reward = -float('inf')\n",
    "\n",
    "        # Configure logging with more details\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(log_dir, 'training.log'),\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "\n",
    "        # Also log to console\n",
    "        console = logging.StreamHandler()\n",
    "        console.setLevel(logging.INFO)\n",
    "        logging.getLogger('').addHandler(console)\n",
    "\n",
    "        # Record initialization parameters for reproducibility\n",
    "        self.config = {\n",
    "            'agent_params': {\n",
    "                'state_dim': agent.state_dim,\n",
    "                'n_actions': agent.n_actions,\n",
    "                'hidden_dim': agent.hidden_dim,\n",
    "                'learning_rate': agent.optimizer.param_groups[0]['lr'],\n",
    "                'gamma': agent.gamma,\n",
    "                'epsilon_start': agent.epsilon,\n",
    "                'epsilon_end': agent.epsilon_end,\n",
    "                'epsilon_decay': agent.epsilon_decay\n",
    "            },\n",
    "            'train_data_size': len(train_sequences),\n",
    "            'val_data_size': len(val_sequences),\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "\n",
    "        # Save configuration\n",
    "        with open(os.path.join(log_dir, 'config.json'), 'w') as f:\n",
    "            json.dump(self.config, f, indent=2)\n",
    "\n",
    "    def adjust_learning_rate(self):\n",
    "        \"\"\"\n",
    "        Dynamically adjust learning rate based on recent performance.\n",
    "        Should be called at the end of each epoch.\n",
    "        \"\"\"\n",
    "        # Check if we have enough history\n",
    "        if len(self.train_metrics.get('collection_mean_reward', [])) < 5:\n",
    "            return\n",
    "\n",
    "        # Get recent rewards\n",
    "        recent_rewards = self.train_metrics['collection_mean_reward'][-5:]\n",
    "\n",
    "        # Calculate trend\n",
    "        reward_change = recent_rewards[-1] - recent_rewards[0]\n",
    "\n",
    "        # If performance declining or plateauing\n",
    "        if reward_change <= 0:\n",
    "            # Reduce learning rate\n",
    "            for param_group in self.agent.optimizer.param_groups:\n",
    "                param_group['lr'] *= 0.7\n",
    "                current_lr = param_group['lr']\n",
    "\n",
    "            logging.info(f\"Performance plateauing/declining. Reducing learning rate to {current_lr:.6f}\")\n",
    "\n",
    "            # If learning rate becomes too small, reset with a larger learning rate \n",
    "            # to escape potential local minima\n",
    "            if current_lr < 1e-6:\n",
    "                for param_group in self.agent.optimizer.param_groups:\n",
    "                    param_group['lr'] = self.config['agent_params']['learning_rate'] * 2\n",
    "                logging.info(f\"Learning rate too small. Resetting to {param_group['lr']:.6f}\")\n",
    "\n",
    "        # If performance improving significantly, consider increasing exploration\n",
    "        elif reward_change > 0.5:\n",
    "            # Temporarily increase epsilon for more exploration\n",
    "            if hasattr(self.agent, 'epsilon') and self.agent.epsilon < 0.3:\n",
    "                self.agent.epsilon = min(0.3, self.agent.epsilon * 1.5)\n",
    "                logging.info(f\"Performance improving. Increasing exploration epsilon to {self.agent.epsilon:.4f}\")\n",
    "\n",
    "    def train(self, n_epochs: int, batch_size: int = 64,\n",
    "             eval_freq: int = 10, checkpoint_freq: int = 50,\n",
    "             updates_per_epoch: int = 100, early_stopping_patience: int = 15):\n",
    "        \"\"\"\n",
    "        Train SARSA agent with enhanced monitoring and diagnostics.\n",
    "\n",
    "        Args:\n",
    "            n_epochs: Number of training epochs\n",
    "            batch_size: Batch size for replay buffer sampling\n",
    "            eval_freq: Frequency of evaluation (in epochs)\n",
    "            checkpoint_freq: Frequency of model checkpointing (in epochs)\n",
    "            updates_per_epoch: Number of model updates per epoch\n",
    "            early_stopping_patience: Number of epochs with no improvement before stopping\n",
    "\n",
    "        Returns:\n",
    "            Tuple of final evaluation metrics and clinical impact metrics\n",
    "        \"\"\"\n",
    "        logging.info(f\"Starting SARSA training for {n_epochs} epochs\")\n",
    "        logging.info(f\"Training on {len(self.train_sequences)} sequences\")\n",
    "        logging.info(f\"Agent parameters: LR={self.config['agent_params']['learning_rate']}, \" +\n",
    "                     f\"={self.agent.gamma}, ={self.agent.epsilon}\")\n",
    "\n",
    "        best_metrics = None\n",
    "        best_epoch = 0\n",
    "        no_improvement_epochs = 0\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "\n",
    "            # Record epsilon before collection\n",
    "            pre_collection_epsilon = self.agent.epsilon\n",
    "\n",
    "            # Collect experience with detailed monitoring\n",
    "            collection_stats = self._collect_experience()\n",
    "\n",
    "            # Enhanced training with more diagnostic information\n",
    "            print(f\"\\n--- Epoch {epoch+1}/{n_epochs} Model Updates ---\")\n",
    "            epoch_losses = []\n",
    "\n",
    "            # Update model with batch training\n",
    "            for update_idx in range(updates_per_epoch):\n",
    "                loss = self.agent.train_on_batch(batch_size)\n",
    "                epoch_losses.append(loss)\n",
    "\n",
    "                # Print detailed diagnostics for first few updates and last update\n",
    "                if update_idx < 3 or update_idx == updates_per_epoch-1:\n",
    "                    print(f\"  Update {update_idx+1}/{updates_per_epoch}, Loss: {loss:.6f}\")\n",
    "\n",
    "                # Check if learning is happening\n",
    "                if update_idx > 0 and update_idx % 25 == 0:\n",
    "                    recent_losses = epoch_losses[-25:]\n",
    "                    loss_change = abs(recent_losses[0] - recent_losses[-1])\n",
    "                    print(f\"  Recent loss change: {loss_change:.6f}\")\n",
    "\n",
    "                    # If losses aren't changing, try a larger learning rate temporarily\n",
    "                    if loss_change < 1e-6 and self.agent.training_steps > 100:\n",
    "                        print(\"  Warning: Loss not changing. Applying learning rate boost.\")\n",
    "                        for param_group in self.agent.optimizer.param_groups:\n",
    "                            param_group['lr'] *= 1.5  # Temporary boost\n",
    "\n",
    "            # Debug statistics every 10 epochs\n",
    "            if epoch % 10 == 0:\n",
    "                # Summarize Q-values\n",
    "                if hasattr(self.agent, 'q_values') and len(self.agent.q_values) > 0:\n",
    "                    recent_q = self.agent.q_values[-100:]\n",
    "                    print(f\"Q-value stats: min={min(recent_q):.4f}, max={max(recent_q):.4f}, \"\n",
    "                          f\"mean={np.mean(recent_q):.4f}, std={np.std(recent_q):.4f}\")\n",
    "\n",
    "                # Check action distribution in replay buffer\n",
    "                if len(self.agent.replay_buffer) > 0:\n",
    "                    actions = [transition[1] for transition in self.agent.replay_buffer[-200:]]\n",
    "                    action_counts = {}\n",
    "                    for idx, name in enumerate(INTERVENTIONS.keys()):\n",
    "                        action_counts[name] = actions.count(idx)\n",
    "                    print(f\"Recent action distribution: {action_counts}\")\n",
    "\n",
    "            # Calculate epoch metrics\n",
    "            mean_loss = np.mean(epoch_losses) if epoch_losses else 0.0\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            epsilon_change = pre_collection_epsilon - self.agent.epsilon\n",
    "\n",
    "            # Check if model is stuck\n",
    "            if epoch > 20 and epoch % 20 == 0:\n",
    "                if hasattr(self.agent, 'reset_network_if_needed') and self.agent.reset_network_if_needed():\n",
    "                    logging.info(\"Reset Q-network weights due to stagnation\")\n",
    "\n",
    "            # Log detailed training metrics\n",
    "            self.train_metrics['loss'].append(mean_loss)\n",
    "            self.train_metrics['epsilon'].append(self.agent.epsilon)\n",
    "            self.train_metrics['update_time'].append(epoch_time)\n",
    "            self.train_metrics['epsilon_change'].append(epsilon_change)\n",
    "\n",
    "            # Add collection statistics to metrics\n",
    "            for key, value in collection_stats.items():\n",
    "                self.train_metrics[f'collection_{key}'].append(value)\n",
    "\n",
    "            # Enhanced logging\n",
    "            logging.info(f\"Epoch {epoch+1}/{n_epochs}, \" +\n",
    "                         f\"Loss: {mean_loss:.6f}, \" +\n",
    "                         f\"Epsilon: {self.agent.epsilon:.4f} (={epsilon_change:.4f}), \" +\n",
    "                         f\"Avg Reward: {collection_stats['mean_reward']:.4f}, \" +\n",
    "                         f\"Time: {epoch_time:.2f}s\")\n",
    "\n",
    "            # Periodic evaluation with more detailed metrics\n",
    "            if (epoch + 1) % eval_freq == 0:\n",
    "                eval_metrics = self._evaluate()\n",
    "\n",
    "                # Check for improvement\n",
    "                current_reward = eval_metrics['mean_reward']\n",
    "\n",
    "\n",
    "                # Store detailed metrics\n",
    "                for k, v in eval_metrics.items():\n",
    "                    self.val_metrics[k].append(v)\n",
    "\n",
    "                # Early stopping and learning rate adjustment logic\n",
    "                if current_reward > self.best_reward:\n",
    "                    self.best_reward = current_reward\n",
    "                    self.stagnant_epochs = 0\n",
    "\n",
    "                    # Save best model\n",
    "                    best_metrics = eval_metrics\n",
    "                    best_epoch = epoch\n",
    "                    self.agent.save(os.path.join(self.log_dir, \"sarsa_best.pt\"))\n",
    "                    logging.info(f\"New best model saved (reward: {current_reward:.4f})\")\n",
    "                else:\n",
    "                    self.stagnant_epochs += 1\n",
    "\n",
    "                    # Adjust learning rate if stagnating\n",
    "                    if self.stagnant_epochs >= self.lr_patience:\n",
    "                        for param_group in self.agent.optimizer.param_groups:\n",
    "                            param_group['lr'] *= 0.5\n",
    "                            new_lr = param_group['lr']\n",
    "\n",
    "                        logging.info(f\"Learning rate reduced to {new_lr:.6f} after {self.stagnant_epochs} epochs without improvement\")\n",
    "                        self.stagnant_epochs = 0\n",
    "\n",
    "                    # Early stopping\n",
    "                    if no_improvement_epochs >= early_stopping_patience:\n",
    "                        logging.info(f\"Early stopping triggered after {no_improvement_epochs} epochs without improvement\")\n",
    "                        break\n",
    "\n",
    "            # Checkpoint saving with metadata\n",
    "            if (epoch + 1) % checkpoint_freq == 0:\n",
    "                checkpoint_path = os.path.join(self.log_dir, f\"sarsa_checkpoint_{epoch+1}.pt\")\n",
    "                self.agent.save(checkpoint_path)\n",
    "                logging.info(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "                # Save intermediate metrics\n",
    "                self.save_results(os.path.join(self.log_dir, f\"metrics_epoch_{epoch+1}.json\"))\n",
    "\n",
    "        # Final evaluation and metrics with detailed analysis\n",
    "        logging.info(\"Performing final comprehensive evaluation...\")\n",
    "        final_metrics = self._evaluate(n_episodes=min(100, len(self.val_sequences)))\n",
    "        clinical_impact = self._calculate_clinical_impact()\n",
    "\n",
    "        # Log completion with detailed statistics\n",
    "        logging.info(\"Training completed\")\n",
    "        logging.info(f\"Final metrics: {json.dumps(final_metrics, indent=2)}\")\n",
    "        logging.info(f\"Clinical impact: NNT={clinical_impact['nnt']:.2f}, \" +\n",
    "                   f\"Acute reduction: {clinical_impact['acute_reduction']*100:.2f}%\")\n",
    "\n",
    "        # Generate action distribution plot\n",
    "        try:\n",
    "            self._plot_action_distribution()\n",
    "            logging.info(\"Action distribution plot saved\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Could not generate plot: {str(e)}\")\n",
    "\n",
    "        # Save final model and best model\n",
    "        self.agent.save(os.path.join(self.log_dir, \"sarsa_final.pt\"))\n",
    "\n",
    "        # If best model is better than final, load it for final metrics\n",
    "        if best_metrics and best_metrics['mean_reward'] > final_metrics['mean_reward']:\n",
    "            logging.info(f\"Loading best model from epoch {best_epoch+1} for final metrics\")\n",
    "            self.agent.load(os.path.join(self.log_dir, \"sarsa_best.pt\"))\n",
    "            final_metrics = self._evaluate(n_episodes=min(100, len(self.val_sequences)))\n",
    "\n",
    "        return final_metrics, clinical_impact\n",
    "    \n",
    "    def _collect_experience(self, n_episodes: int = 50) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Collect experience from environment with enhanced monitoring.\n",
    "\n",
    "        Args:\n",
    "            n_episodes: Number of episodes to collect experience from\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of collection statistics\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Starting Experience Collection ---\")\n",
    "\n",
    "        # Choose random subset of training sequences\n",
    "        episode_indices = np.random.choice(\n",
    "            len(self.train_sequences),\n",
    "            min(n_episodes, len(self.train_sequences)),\n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        # Track statistics\n",
    "        action_distribution = defaultdict(int)\n",
    "        reward_stats = []\n",
    "        acute_events = 0\n",
    "        transitions_added = 0\n",
    "\n",
    "        for episode_idx, seq_idx in enumerate(episode_indices):\n",
    "            self.env.current_sequence = seq_idx\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            episode_actions = []\n",
    "            episode_rewards = []\n",
    "\n",
    "            # Only print detailed info for first few and last episodes\n",
    "            verbose = (episode_idx < 2) or (episode_idx == len(episode_indices) - 1)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\nStarting episode {episode_idx+1}/{len(episode_indices)}, \" +\n",
    "                      f\"patient ID: {state.patient_id}\")\n",
    "\n",
    "            step_count = 0\n",
    "            episode_acute = 0\n",
    "\n",
    "            while not done:\n",
    "                step_count += 1\n",
    "\n",
    "                # Convert state\n",
    "                state_tensor = state.to_tensor(self.env.state_cache)\n",
    "\n",
    "                # Get action mask\n",
    "                action_mask = self.env.generate_action_mask(state)\n",
    "\n",
    "                # Select action\n",
    "                action, eps = self.agent.select_action(state_tensor, action_mask)\n",
    "                action_name = list(INTERVENTIONS.keys())[action.item()]\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Step {step_count}: Selected action: {action_name} (={eps:.2f})\")\n",
    "\n",
    "                # Take step\n",
    "                next_state, reward, done, info = self.env.step(state, action.item())\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"  Reward: {reward:.4f}, Done: {done}, Acute: {info.get('is_acute', False)}\")\n",
    "\n",
    "                # Track if acute event occurred\n",
    "                if info.get('is_acute', False):\n",
    "                    episode_acute += 1\n",
    "\n",
    "                # Convert next state\n",
    "                next_state_tensor = next_state.to_tensor(self.env.state_cache)\n",
    "\n",
    "                # Select next action for SARSA update\n",
    "                next_action_mask = self.env.generate_action_mask(next_state)\n",
    "                next_action, _ = self.agent.select_action(next_state_tensor, next_action_mask)\n",
    "\n",
    "                # Add to replay buffer\n",
    "                self.agent.add_experience(\n",
    "                    state_tensor,\n",
    "                    action.item(),\n",
    "                    reward,\n",
    "                    next_state_tensor,\n",
    "                    next_action.item(),\n",
    "                    done\n",
    "                )\n",
    "                transitions_added += 1\n",
    "\n",
    "                # Track detailed statistics\n",
    "                action_distribution[action_name] += 1\n",
    "                episode_actions.append(action_name)\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                # Track action-specific rewards for analysis\n",
    "                self.action_stats[action_name]['count'] += 1\n",
    "                self.action_stats[action_name]['rewards'].append(reward)\n",
    "\n",
    "                # Update state\n",
    "                state = next_state\n",
    "\n",
    "            # Episode summary\n",
    "            if verbose:\n",
    "                print(f\"Episode {episode_idx+1} complete:\")\n",
    "                print(f\"  Actions taken: {episode_actions}\")\n",
    "                print(f\"  Total reward: {sum(episode_rewards):.4f}\")\n",
    "                print(f\"  Acute events: {episode_acute}\")\n",
    "\n",
    "            reward_stats.append(sum(episode_rewards))\n",
    "            acute_events += episode_acute\n",
    "\n",
    "        # Calculate action entropy (measure of diversity)\n",
    "        action_counts = np.array(list(action_distribution.values()))\n",
    "        if len(action_counts) > 0 and np.sum(action_counts) > 0:\n",
    "            action_probs = action_counts / np.sum(action_counts)\n",
    "            action_entropy = -np.sum(action_probs * np.log2(action_probs + 1e-10))\n",
    "        else:\n",
    "            action_entropy = 0.0\n",
    "\n",
    "        # Calculate mean rewards by action type for analysis\n",
    "        action_mean_rewards = {}\n",
    "        for action, stats in self.action_stats.items():\n",
    "            if stats['count'] > 0:\n",
    "                action_mean_rewards[action] = np.mean(stats['rewards'][-n_episodes:])\n",
    "            else:\n",
    "                action_mean_rewards[action] = 0.0\n",
    "\n",
    "        # Overall statistics\n",
    "        print(\"\\n--- Experience Collection Summary ---\")\n",
    "        print(f\"Episodes: {len(episode_indices)}, Transitions: {transitions_added}\")\n",
    "        print(f\"Action distribution: {dict(action_distribution)}\")\n",
    "        print(f\"Action entropy: {action_entropy:.4f} bits\")\n",
    "        print(f\"Average episode reward: {np.mean(reward_stats):.4f}\")\n",
    "        print(f\"Acute events: {acute_events} ({acute_events/len(episode_indices):.4f} per episode)\")\n",
    "        print(f\"Mean rewards by action: {action_mean_rewards}\")\n",
    "        print(\"----------------------------------\\n\")\n",
    "\n",
    "        # Return collection statistics for metrics tracking\n",
    "        return {\n",
    "            'mean_reward': float(np.mean(reward_stats)),\n",
    "            'action_entropy': float(action_entropy),\n",
    "            'acute_rate': float(acute_events/max(1, len(episode_indices))),\n",
    "            'transitions': transitions_added,\n",
    "            'unique_actions': len(action_distribution)\n",
    "        }\n",
    "                    \n",
    "    def _evaluate(self, n_episodes: int = 20) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate SARSA performance with comprehensive metrics.\n",
    "\n",
    "        Args:\n",
    "            n_episodes: Number of episodes to evaluate on\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        self.agent.q_network.eval()\n",
    "        rewards = []\n",
    "        acute_events = []\n",
    "        safety_violations = []\n",
    "        intervention_counts = defaultdict(int)\n",
    "        risk_changes = []\n",
    "        q_values = []\n",
    "\n",
    "        # Choose random subset of validation sequences\n",
    "        episode_indices = np.random.choice(\n",
    "            len(self.val_sequences),\n",
    "            min(n_episodes, len(self.val_sequences)),\n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        # Track paths through diagnosis-treatment decision trees\n",
    "        clinical_pathway_counts = defaultdict(int)\n",
    "\n",
    "        print(f\"\\n--- Evaluating on {len(episode_indices)} episodes ---\")\n",
    "\n",
    "        for idx_num, idx in enumerate(episode_indices):\n",
    "            self.env.current_sequence = idx\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_acute = 0\n",
    "            episode_violations = 0\n",
    "            pre_risk = state.features.get('riskScore', 0.5)\n",
    "            pathway = []\n",
    "            done = False\n",
    "\n",
    "            verbose = idx_num < 2 or idx_num == len(episode_indices) - 1\n",
    "            if verbose:\n",
    "                print(f\"\\nEvaluation episode {idx_num+1}, patient {state.patient_id}\")\n",
    "\n",
    "            while not done:\n",
    "                # Get action with exploration disabled\n",
    "                state_tensor = state.to_tensor(self.env.state_cache)\n",
    "                action_mask = self.env.generate_action_mask(state)  # <-- FIXED LINE\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    action, _ = self.agent.select_action(\n",
    "                        state_tensor, action_mask, training=False\n",
    "                    )\n",
    "                    # Get Q-values for analysis\n",
    "                    q_values_tensor = self.agent.q_network(state_tensor)[0]\n",
    "                    q_values.append(q_values_tensor.cpu().numpy())\n",
    "\n",
    "                # Take step\n",
    "                next_state, reward, done, info = self.env.step(state, action.item())\n",
    "\n",
    "\n",
    "                # Track metrics\n",
    "                episode_reward += reward\n",
    "                episode_acute += int(info.get('is_acute', False))\n",
    "                episode_violations += int(info.get('safety_violation', False))\n",
    "\n",
    "                # Track intervention\n",
    "                intervention = info.get('intervention', 'UNKNOWN')\n",
    "                intervention_counts[intervention] += 1\n",
    "\n",
    "                # Track clinical pathway\n",
    "                pathway.append(intervention)\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"  Action: {intervention}, Reward: {reward:.2f}, Acute: {info.get('is_acute', False)}\")\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            # Record outcomes\n",
    "            rewards.append(episode_reward)\n",
    "            acute_events.append(episode_acute)\n",
    "            safety_violations.append(episode_violations)\n",
    "\n",
    "            # Track risk change\n",
    "            post_risk = state.features.get('riskScore', 0.5)\n",
    "            risk_change = pre_risk - post_risk\n",
    "            risk_changes.append(risk_change)\n",
    "\n",
    "            # Track pathway pattern (simplified)\n",
    "            pathway_key = '->'.join(pathway[:3]) + '...' if len(pathway) > 3 else '->'.join(pathway)\n",
    "            clinical_pathway_counts[pathway_key] += 1\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Episode result: Reward={episode_reward:.2f}, Acute events={episode_acute}\")\n",
    "                print(f\"Risk change: {pre_risk:.2f} -> {post_risk:.2f} (={risk_change:.2f})\")\n",
    "\n",
    "        # Calculate metrics\n",
    "\n",
    "        # Basic statistics\n",
    "        eval_metrics = {\n",
    "            'mean_reward': float(np.mean(rewards)),\n",
    "            'mean_acute_events': float(np.mean(acute_events)),\n",
    "            'mean_safety_violations': float(np.mean(safety_violations)),\n",
    "            'mean_risk_reduction': float(np.mean(risk_changes))\n",
    "        }\n",
    "\n",
    "        # Add intervention distribution\n",
    "        total_interventions = sum(intervention_counts.values())\n",
    "        if total_interventions > 0:\n",
    "            for intervention, count in intervention_counts.items():\n",
    "                eval_metrics[f'pct_{intervention}'] = count / total_interventions * 100\n",
    "\n",
    "        # Calculate action entropy (measure of diversity)\n",
    "        action_counts = np.array(list(intervention_counts.values()))\n",
    "        if len(action_counts) > 0 and np.sum(action_counts) > 0:\n",
    "            action_probs = action_counts / np.sum(action_counts)\n",
    "            eval_metrics['action_entropy'] = float(-np.sum(action_probs * np.log2(action_probs + 1e-10)))\n",
    "        else:\n",
    "            eval_metrics['action_entropy'] = 0.0\n",
    "\n",
    "        # Q-value analysis\n",
    "        if q_values:\n",
    "            q_values_array = np.vstack(q_values)\n",
    "            eval_metrics['mean_q_value'] = float(np.mean(q_values_array))\n",
    "            eval_metrics['max_q_value'] = float(np.max(q_values_array))\n",
    "            eval_metrics['q_value_std'] = float(np.std(q_values_array))\n",
    "\n",
    "            # Q-value action gap (difference between highest and second highest)\n",
    "            q_sorted = np.sort(q_values_array, axis=1)\n",
    "            if q_sorted.shape[1] >= 2:\n",
    "                action_gaps = q_sorted[:, -1] - q_sorted[:, -2]\n",
    "                eval_metrics['mean_action_gap'] = float(np.mean(action_gaps))\n",
    "\n",
    "        # Top clinical pathways\n",
    "        top_pathways = sorted(clinical_pathway_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        eval_metrics['top_pathways'] = {k: v for k, v in top_pathways}\n",
    "\n",
    "        # Print summary\n",
    "        print(\"\\n--- Evaluation Summary ---\")\n",
    "        print(f\"Mean reward: {eval_metrics['mean_reward']:.4f}\")\n",
    "        print(f\"Acute events: {eval_metrics['mean_acute_events']:.4f} per episode\")\n",
    "        print(f\"Risk reduction: {eval_metrics['mean_risk_reduction']:.4f}\")\n",
    "        print(f\"Action entropy: {eval_metrics['action_entropy']:.4f} bits\")\n",
    "        print(f\"Action distribution: {dict(intervention_counts)}\")\n",
    "        print(\"---------------------------\\n\")\n",
    "\n",
    "        return eval_metrics\n",
    "\n",
    "    def _calculate_clinical_impact(self, n_sequences=None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate detailed clinical impact metrics comparing SARSA to status quo.\n",
    "\n",
    "        Args:\n",
    "            n_sequences: Optional number of sequences for evaluation. \n",
    "                        If None, uses min(100, available sequences).\n",
    "                        \n",
    "        Returns:\n",
    "            Dictionary of clinical impact metrics including NNT, NNH, and statistical significance.\n",
    "        \"\"\"\n",
    "        # Collection metrics\n",
    "        sarsa_outcomes = defaultdict(list)\n",
    "        status_quo_outcomes = defaultdict(list)\n",
    "\n",
    "        print(\"\\n--- Calculating Clinical Impact ---\")\n",
    "        print(\"Comparing SARSA-guided vs. status quo care management\")\n",
    "\n",
    "        # Process validation sequences with parameter handling\n",
    "        if n_sequences is None:\n",
    "            n_sequences = min(100, len(self.val_sequences))\n",
    "        else:\n",
    "            n_sequences = min(n_sequences, len(self.val_sequences))\n",
    "\n",
    "        print(f\"Evaluating on {n_sequences} validation sequences\")\n",
    "\n",
    "            \n",
    "        sequence_indices = np.random.choice(len(self.val_sequences), n_sequences, replace=False)\n",
    "\n",
    "\n",
    "        for seq_idx, sequence_idx in enumerate(sequence_indices):\n",
    "            # Run SARSA trajectory\n",
    "            sarsa_trajectory = self._simulate_trajectory(sequence_idx, use_sarsa=True)\n",
    "\n",
    "            # Run status quo trajectory\n",
    "            status_trajectory = self._simulate_trajectory(sequence_idx, use_sarsa=False)\n",
    "\n",
    "            # Count acute events\n",
    "            sarsa_acute = sum(1 for step in sarsa_trajectory if step['is_acute'])\n",
    "            status_acute = sum(1 for step in status_trajectory if step['is_acute'])\n",
    "\n",
    "            # Track counts for NNT/NNH calculation\n",
    "            if sarsa_acute < status_acute:\n",
    "                self.clinical_outcomes['acute_events_prevented'] += (status_acute - sarsa_acute)\n",
    "            elif sarsa_acute > status_acute:\n",
    "                self.clinical_outcomes['acute_events_induced'] += (sarsa_acute - status_acute)\n",
    "\n",
    "            # Get initial risk score for stratification\n",
    "            if seq_idx < 5:  # Print details for a few examples\n",
    "                print(f\"\\nPatient {seq_idx+1}/{n_sequences}:\")\n",
    "                print(f\"  SARSA: {sarsa_acute} acute events, Status quo: {status_acute} acute events\")\n",
    "                print(f\"  Difference: {status_acute - sarsa_acute} events\")\n",
    "\n",
    "            # Store outcomes for further analysis\n",
    "            sarsa_outcomes['acute_events'].append(sarsa_acute)\n",
    "            status_quo_outcomes['acute_events'].append(status_acute)\n",
    "\n",
    "            # Track risk scores\n",
    "            sarsa_outcomes['final_risk'].append(sarsa_trajectory[-1]['risk'] if sarsa_trajectory else 0.5)\n",
    "            status_quo_outcomes['final_risk'].append(status_trajectory[-1]['risk'] if status_trajectory else 0.5)\n",
    "\n",
    "            # Store additional metrics for detailed analysis\n",
    "            self._store_trajectory_metrics(sarsa_trajectory, status_trajectory)\n",
    "\n",
    "        # Calculate absolute risk reduction\n",
    "        sarsa_rate = np.mean(sarsa_outcomes['acute_events'])\n",
    "        status_quo_rate = np.mean(status_quo_outcomes['acute_events'])\n",
    "        acute_reduction = status_quo_rate - sarsa_rate\n",
    "\n",
    "        print(f\"\\nOverall comparison:\")\n",
    "        print(f\"SARSA acute event rate: {sarsa_rate:.4f} per trajectory\")\n",
    "        print(f\"Status quo acute event rate: {status_quo_rate:.4f} per trajectory\")\n",
    "        print(f\"Absolute reduction: {acute_reduction:.4f} events per trajectory\")\n",
    "        print(f\"Relative reduction: {(acute_reduction/max(0.001, status_quo_rate))*100:.2f}%\")\n",
    "\n",
    "        # Calculate NNT and NNH\n",
    "        if acute_reduction > 0:\n",
    "            nnt = 1 / acute_reduction\n",
    "            nnh = float('inf')  # No harm observed\n",
    "            print(f\"Number needed to treat (NNT): {nnt:.2f}\")\n",
    "        else:\n",
    "            nnt = float('inf')  # No benefit observed\n",
    "            nnh = 1 / abs(acute_reduction) if acute_reduction < 0 else float('inf')\n",
    "            print(f\"No reduction observed. Number needed to harm (NNH): {nnh:.2f}\")\n",
    "\n",
    "        # Risk-stratified analysis\n",
    "        low_risk_nnt = self._calculate_stratified_nnt('low')\n",
    "        medium_risk_nnt = self._calculate_stratified_nnt('medium')\n",
    "        high_risk_nnt = self._calculate_stratified_nnt('high')\n",
    "\n",
    "        print(\"\\nRisk-stratified analysis:\")\n",
    "        print(f\"Low-risk patients: NNT = {low_risk_nnt:.2f}\")\n",
    "        print(f\"Medium-risk patients: NNT = {medium_risk_nnt:.2f}\")\n",
    "        print(f\"High-risk patients: NNT = {high_risk_nnt:.2f}\")\n",
    "\n",
    "        # Update clinical outcomes\n",
    "        self.clinical_outcomes.update({\n",
    "            'nnt': float(nnt),\n",
    "            'nnh': float(nnh),\n",
    "            'acute_reduction': float(acute_reduction),\n",
    "            'sarsa_acute_rate': float(sarsa_rate),\n",
    "            'status_quo_acute_rate': float(status_quo_rate),\n",
    "            'low_risk_nnt': float(low_risk_nnt),\n",
    "            'medium_risk_nnt': float(medium_risk_nnt),\n",
    "            'high_risk_nnt': float(high_risk_nnt)\n",
    "        })\n",
    "\n",
    "        # Calculate statistical significance\n",
    "        t_stat, p_value = stats.ttest_ind(\n",
    "            sarsa_outcomes['acute_events'],\n",
    "            status_quo_outcomes['acute_events']\n",
    "        )\n",
    "\n",
    "        self.clinical_outcomes['p_value'] = float(p_value)\n",
    "        self.clinical_outcomes['t_statistic'] = float(t_stat)\n",
    "        print(f\"Statistical significance: p-value = {p_value:.4f}\")\n",
    "\n",
    "        # Calculate confidence intervals\n",
    "        n = len(sarsa_outcomes['acute_events'])\n",
    "        std_diff = np.std(np.array(sarsa_outcomes['acute_events']) - np.array(status_quo_outcomes['acute_events']))\n",
    "        ci_width = 1.96 * std_diff / np.sqrt(n)\n",
    "\n",
    "        self.clinical_outcomes['reduction_ci_low'] = float(acute_reduction - ci_width)\n",
    "        self.clinical_outcomes['reduction_ci_high'] = float(acute_reduction + ci_width)\n",
    "\n",
    "        print(f\"95% CI for reduction: [{acute_reduction - ci_width:.4f}, {acute_reduction + ci_width:.4f}]\")\n",
    "        print(\"------------------------------------------------\\n\")\n",
    "\n",
    "        return self.clinical_outcomes\n",
    "\n",
    "    def _simulate_trajectory(self, sequence_idx: int, use_sarsa: bool = True) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Simulate intervention trajectory using either SARSA or status quo.\n",
    "\n",
    "        Args:\n",
    "            sequence_idx: Index of patient sequence to simulate\n",
    "            use_sarsa: Whether to use SARSA policy (True) or status quo (False)\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries containing trajectory information\n",
    "        \"\"\"\n",
    "        self.env.current_sequence = sequence_idx\n",
    "        state = self.env.reset()\n",
    "        trajectory = []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Get action mask\n",
    "            action_mask = self.env.generate_action_mask(state)\n",
    "\n",
    "            # Select action based on policy\n",
    "            if use_sarsa:\n",
    "                state_tensor = state.to_tensor(self.env.state_cache)\n",
    "                action, _ = self.agent.select_action(\n",
    "                    state_tensor, action_mask, training=False\n",
    "                )\n",
    "            else:\n",
    "                # Status quo uses rule-based decision making\n",
    "                action = self._get_status_quo_action(state, action_mask)\n",
    "\n",
    "            # Take step\n",
    "            next_state, reward, done, info = self.env.step(state, action.item())\n",
    "\n",
    "            # Store step details\n",
    "            trajectory.append({\n",
    "                'action': action.item(),\n",
    "                'intervention': info['intervention'],\n",
    "                'reward': reward,\n",
    "                'is_acute': info.get('is_acute', False),\n",
    "                'risk': info.get('post_risk', 0.5),\n",
    "                'risk_reduction': info.get('risk_reduction', 0),\n",
    "                'safety_violation': info.get('safety_violation', False)\n",
    "            })\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        return trajectory\n",
    "\n",
    "    def _get_status_quo_action(self, state: ClinicalState, action_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Implement rule-based status quo decision making with improved clinical logic.\n",
    "\n",
    "        Args:\n",
    "            state: Current clinical state\n",
    "            action_mask: Binary mask of allowed actions\n",
    "\n",
    "        Returns:\n",
    "            Tensor containing selected action index\n",
    "        \"\"\"\n",
    "        # Get risk assessments with safeguards against missing data\n",
    "        medical_risk = state.risk_summary.get('medical_risk_mentions', 0)\n",
    "        behavioral_risk = state.risk_summary.get('behavioral_risk_mentions', 0)\n",
    "        social_risk = state.risk_summary.get('social_risk_mentions', 0)\n",
    "        risk_score = state.features.get('riskScore', 0.5)\n",
    "\n",
    "        # Rule-based priority hierarchy based on clinical guidelines and risk level\n",
    "        priority = None\n",
    "\n",
    "        # Check recent history for patterns\n",
    "        recent_notes = \"\"\n",
    "        if hasattr(state, 'history') and len(state.history) > 0:\n",
    "            recent_notes = ' '.join([str(h.get('encounter_note', '')) for h in state.history[-3:]])\n",
    "\n",
    "        # High-risk patients (prioritize effective interventions)\n",
    "        if risk_score > 0.7:\n",
    "            # For very high risk, prioritize the domain with the highest risk\n",
    "            if medical_risk >= behavioral_risk and medical_risk >= social_risk:\n",
    "                priority = 'CHRONIC_CONDITION_MANAGEMENT'\n",
    "            elif behavioral_risk >= medical_risk and behavioral_risk >= social_risk:\n",
    "                # Choose between mental health and substance use based on notes\n",
    "                if 'substance' in recent_notes or 'alcohol' in recent_notes or 'drug' in recent_notes:\n",
    "                    priority = 'SUBSTANCE_USE_SUPPORT'\n",
    "                else:\n",
    "                    priority = 'MENTAL_HEALTH_SUPPORT'\n",
    "            else:\n",
    "                # Choose most appropriate social intervention\n",
    "                if 'housing' in recent_notes or 'homeless' in recent_notes:\n",
    "                    priority = 'HOUSING_ASSISTANCE'\n",
    "                elif 'food' in recent_notes or 'hunger' in recent_notes:\n",
    "                    priority = 'FOOD_ASSISTANCE'\n",
    "                else:\n",
    "                    priority = 'HOUSING_ASSISTANCE'  # Default to housing for high social need\n",
    "\n",
    "        # Medium-risk patients (balanced approach)\n",
    "        elif risk_score > 0.3:\n",
    "            # Check for domain with highest risk but with more balanced approach\n",
    "            domain_risks = [\n",
    "                ('medical', medical_risk, 'CHRONIC_CONDITION_MANAGEMENT'),\n",
    "                ('behavioral', behavioral_risk, None),  # Will determine specific intervention below\n",
    "                ('social', social_risk, None)  # Will determine specific intervention below\n",
    "            ]\n",
    "\n",
    "            # Sort by risk level (highest first)\n",
    "            domain_risks.sort(key=lambda x: x[1], reverse=True)\n",
    "            highest_domain, highest_risk, highest_intervention = domain_risks[0]\n",
    "\n",
    "            if highest_domain == 'medical':\n",
    "                priority = highest_intervention\n",
    "            elif highest_domain == 'behavioral':\n",
    "                # Determine specific behavioral intervention\n",
    "                if 'substance' in recent_notes or 'alcohol' in recent_notes:\n",
    "                    priority = 'SUBSTANCE_USE_SUPPORT'\n",
    "                else:\n",
    "                    priority = 'MENTAL_HEALTH_SUPPORT'\n",
    "            else:  # social domain\n",
    "                # Choose appropriate social intervention based on notes\n",
    "                if 'housing' in recent_notes:\n",
    "                    priority = 'HOUSING_ASSISTANCE'\n",
    "                elif 'food' in recent_notes:\n",
    "                    priority = 'FOOD_ASSISTANCE'\n",
    "                elif 'transport' in recent_notes:\n",
    "                    priority = 'TRANSPORTATION_ASSISTANCE'\n",
    "                elif 'utility' in recent_notes or 'electric' in recent_notes:\n",
    "                    priority = 'UTILITY_ASSISTANCE'\n",
    "                elif 'child' in recent_notes:\n",
    "                    priority = 'CHILDCARE_ASSISTANCE'\n",
    "                else:\n",
    "                    # Default social intervention based on program statistics\n",
    "                    priority = random.choices(\n",
    "                        ['HOUSING_ASSISTANCE', 'FOOD_ASSISTANCE', 'TRANSPORTATION_ASSISTANCE',\n",
    "                        'UTILITY_ASSISTANCE', 'CHILDCARE_ASSISTANCE'],\n",
    "                        weights=[0.3, 0.3, 0.2, 0.1, 0.1]\n",
    "                    )[0]\n",
    "\n",
    "        # Low-risk patients (less intensive interventions)\n",
    "        else:\n",
    "            # For low risk, more frequently use watchful waiting\n",
    "            if random.random() < 0.4:\n",
    "                priority = 'WATCHFUL_WAITING'\n",
    "            else:\n",
    "                # Address any noticeable domain risks\n",
    "                if medical_risk > 1.0:\n",
    "                    priority = 'CHRONIC_CONDITION_MANAGEMENT'\n",
    "                elif behavioral_risk > 1.0:\n",
    "                    priority = 'MENTAL_HEALTH_SUPPORT'\n",
    "                elif social_risk > 1.0:\n",
    "                    priority = random.choices(\n",
    "                        ['FOOD_ASSISTANCE', 'TRANSPORTATION_ASSISTANCE', 'UTILITY_ASSISTANCE'],\n",
    "                        weights=[0.4, 0.3, 0.3]\n",
    "                    )[0]\n",
    "                else:\n",
    "                    # No significant risks - use watchful waiting\n",
    "                    priority = 'WATCHFUL_WAITING'\n",
    "\n",
    "        # If no priority set, use reasonable default based on risk\n",
    "        if priority is None:\n",
    "            if risk_score > 0.5:\n",
    "                priority = 'CHRONIC_CONDITION_MANAGEMENT'\n",
    "            else:\n",
    "                priority = 'WATCHFUL_WAITING'\n",
    "\n",
    "        # Convert to action index with safeguards\n",
    "        try:\n",
    "            action_idx = list(INTERVENTIONS.keys()).index(priority)\n",
    "        except ValueError:\n",
    "            # Fallback if priority is invalid\n",
    "            action_idx = list(INTERVENTIONS.keys()).index('WATCHFUL_WAITING')\n",
    "\n",
    "        # Ensure action is valid\n",
    "        if action_idx < len(action_mask) and not action_mask[action_idx]:\n",
    "            # Find the highest priority valid action\n",
    "            for backup_priority in ['CHRONIC_CONDITION_MANAGEMENT', 'MENTAL_HEALTH_SUPPORT',\n",
    "                                  'HOUSING_ASSISTANCE', 'FOOD_ASSISTANCE', 'WATCHFUL_WAITING']:\n",
    "                backup_idx = list(INTERVENTIONS.keys()).index(backup_priority)\n",
    "                if backup_idx < len(action_mask) and action_mask[backup_idx]:\n",
    "                    action_idx = backup_idx\n",
    "                    break\n",
    "            # Final fallback - take first valid action\n",
    "            if not action_mask[action_idx]:\n",
    "                valid_indices = torch.nonzero(action_mask).squeeze()\n",
    "                if valid_indices.dim() == 0:\n",
    "                    action_idx = valid_indices.item()\n",
    "                else:\n",
    "                    action_idx = valid_indices[0].item()\n",
    "\n",
    "        return torch.tensor(action_idx, device=DEVICE)\n",
    "\n",
    "\n",
    "    def _calculate_stratified_nnt(self, risk_stratum: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate NNT for specific risk stratum.\n",
    "\n",
    "        Args:\n",
    "            risk_stratum: Risk level to calculate NNT for ('low', 'medium', or 'high')\n",
    "\n",
    "        Returns:\n",
    "            Number needed to treat for the specified risk stratum\n",
    "        \"\"\"\n",
    "        sarsa_events = []\n",
    "        status_events = []\n",
    "\n",
    "        for idx, val_item in enumerate(self.val_sequences):\n",
    "            # Handle different data formats\n",
    "            if isinstance(val_item, dict):\n",
    "                # Extract risk score from dictionary\n",
    "                risk_score = val_item.get('riskScore',\n",
    "                            val_item.get('features', {}).get('riskScore', 0.0))\n",
    "            elif isinstance(val_item, tuple) and len(val_item) > 0 and hasattr(val_item[0], 'features'):\n",
    "                # Handle tuple of (state, encounters)\n",
    "                risk_score = val_item[0].features.get('riskScore', 0.0)\n",
    "            else:\n",
    "                # Default risk score for other formats\n",
    "                risk_score = 0.5\n",
    "\n",
    "            # Filter by risk stratum\n",
    "            if risk_stratum == 'low' and risk_score <= 0.3:\n",
    "                pass  # Include in low risk\n",
    "            elif risk_stratum == 'medium' and 0.3 < risk_score <= 0.7:\n",
    "                pass  # Include in medium risk\n",
    "            elif risk_stratum == 'high' and risk_score > 0.7:\n",
    "                pass  # Include in high risk\n",
    "            else:\n",
    "                continue  # Skip if not in target stratum\n",
    "\n",
    "            # Run trajectories\n",
    "            sarsa_trajectory = self._simulate_trajectory(idx, use_sarsa=True)\n",
    "            status_trajectory = self._simulate_trajectory(idx, use_sarsa=False)\n",
    "\n",
    "            # Count acute events\n",
    "            sarsa_acute = sum(1 for step in sarsa_trajectory if step['is_acute'])\n",
    "            status_acute = sum(1 for step in status_trajectory if step['is_acute'])\n",
    "\n",
    "            sarsa_events.append(sarsa_acute)\n",
    "            status_events.append(status_acute)\n",
    "\n",
    "        # Calculate risk reduction\n",
    "        if not sarsa_events:  # No patients in this stratum\n",
    "            return float('inf')\n",
    "\n",
    "        sarsa_rate = np.mean(sarsa_events)\n",
    "        status_rate = np.mean(status_events)\n",
    "        reduction = status_rate - sarsa_rate\n",
    "\n",
    "        # Calculate NNT\n",
    "        if reduction > 0:\n",
    "            return 1 / reduction\n",
    "        else:\n",
    "            return float('inf')  # No benefit in this stratum\n",
    "\n",
    "    def _store_trajectory_metrics(self, sarsa_trajectory: List[Dict],\n",
    "                                                                status_trajectory: List[Dict]) -> None:\n",
    "        \"\"\"\n",
    "        Store additional trajectory metrics for detailed analysis.\n",
    "\n",
    "        Args:\n",
    "            sarsa_trajectory: Trajectory using SARSA policy\n",
    "            status_trajectory: Trajectory using status quo policy\n",
    "        \"\"\"\n",
    "        # Analyze intervention patterns\n",
    "        if not hasattr(self, 'intervention_patterns'):\n",
    "            self.intervention_patterns = {\n",
    "                'sarsa': defaultdict(int),\n",
    "                'status_quo': defaultdict(int)\n",
    "            }\n",
    "\n",
    "        # Store sequential patterns (bigrams)\n",
    "        for trajectory, policy in [(sarsa_trajectory, 'sarsa'), (status_trajectory, 'status_quo')]:\n",
    "            for i in range(len(trajectory) - 1):\n",
    "                current = trajectory[i]['intervention']\n",
    "                next_int = trajectory[i+1]['intervention']\n",
    "                bigram = f\"{current}->{next_int}\"\n",
    "                self.intervention_patterns[policy][bigram] += 1\n",
    "\n",
    "    def _plot_action_distribution(self):\n",
    "        \"\"\"Generate and save action distribution plot.\"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure(figsize=(12, 6))\n",
    "\n",
    "            # Get action counts from validation metrics\n",
    "            action_keys = [k for k in self.val_metrics.keys() if k.startswith('pct_')]\n",
    "            if not action_keys or len(self.val_metrics[action_keys[0]]) == 0:\n",
    "                return  # No data to plot\n",
    "\n",
    "            # Most recent evaluation\n",
    "            final_counts = {k.replace('pct_', ''): self.val_metrics[k][-1] for k in action_keys}\n",
    "\n",
    "            # Sort by value\n",
    "            sorted_actions = sorted(final_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            labels, values = zip(*sorted_actions)\n",
    "\n",
    "            # Create bar chart\n",
    "            plt.bar(labels, values, color='skyblue')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.title('Action Distribution in Final Evaluation')\n",
    "            plt.ylabel('Percentage (%)')\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save figure\n",
    "            plt.savefig(os.path.join(self.log_dir, 'action_distribution.png'), dpi=300)\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Plotting error: {str(e)}\")\n",
    "\n",
    "    def save_results(self, output_path: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Save training results and metrics.\n",
    "\n",
    "        Args:\n",
    "            output_path: Path to save results to (default: {log_dir}/results.json)\n",
    "        \"\"\"\n",
    "        if output_path is None:\n",
    "            output_path = os.path.join(self.log_dir, \"results.json\")\n",
    "\n",
    "        # Process metrics for JSON serialization\n",
    "        results = {\n",
    "            'clinical_outcomes': self.clinical_outcomes,\n",
    "            'train_metrics': {k: list(map(float, v)) for k, v in self.train_metrics.items()},\n",
    "            'val_metrics': {k: list(map(float, v)) for k, v in self.val_metrics.items()\n",
    "                         if not isinstance(v[0], dict)},  # Skip nested dicts\n",
    "            'action_stats': {action: {'count': stats['count'],\n",
    "                                   'mean_reward': float(np.mean(stats['rewards'])) if stats['rewards'] else 0.0}\n",
    "                         for action, stats in self.action_stats.items()},\n",
    "            'config': self.config,\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "\n",
    "        # Save top intervention patterns if available\n",
    "        if hasattr(self, 'intervention_patterns'):\n",
    "            # Get top 10 patterns for each policy\n",
    "            sarsa_patterns = sorted(self.intervention_patterns['sarsa'].items(),\n",
    "                                 key=lambda x: x[1], reverse=True)[:10]\n",
    "            status_patterns = sorted(self.intervention_patterns['status_quo'].items(),\n",
    "                                  key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "            results['intervention_patterns'] = {\n",
    "                'sarsa': {k: v for k, v in sarsa_patterns},\n",
    "                'status_quo': {k: v for k, v in status_patterns}\n",
    "            }\n",
    "\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        logging.info(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StratifiedSARSAAgent:\n",
    "    \"\"\"SARSA agent with separate networks for different risk strata.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        n_actions: int,\n",
    "        hidden_dim: int = 256,\n",
    "        learning_rate: float = 3e-4,\n",
    "        gamma: float = 0.99,\n",
    "        epsilon_start: float = 1.0,\n",
    "        epsilon_end: float = 0.1,\n",
    "        epsilon_decay: float = 0.995,\n",
    "        max_buffer_size: int = 100000\n",
    "    ):\n",
    "        self.state_dim = state_dim\n",
    "        self.n_actions = n_actions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # Create separate agents for different risk strata\n",
    "        self.low_risk_agent = EnhancedSARSAAgent(\n",
    "            state_dim, n_actions, hidden_dim, learning_rate,\n",
    "            gamma, epsilon_start, epsilon_end, epsilon_decay, max_buffer_size\n",
    "        )\n",
    "\n",
    "        self.med_risk_agent = EnhancedSARSAAgent(\n",
    "            state_dim, n_actions, hidden_dim, learning_rate,\n",
    "            gamma, epsilon_start, epsilon_end, epsilon_decay, max_buffer_size\n",
    "        )\n",
    "\n",
    "        self.high_risk_agent = EnhancedSARSAAgent(\n",
    "            state_dim, n_actions, hidden_dim, learning_rate,\n",
    "            gamma, epsilon_start, epsilon_end * 0.5, epsilon_decay, max_buffer_size  # More conservative exploration\n",
    "        )\n",
    "\n",
    "        # Tracking variables\n",
    "        self.training_steps = 0\n",
    "        self.q_values = []\n",
    "        # Initialize replay buffer here\n",
    "        self.replay_buffer = []\n",
    "        self.max_buffer_size = max_buffer_size\n",
    "\n",
    "\n",
    "    def _get_agent_for_state(self, state):\n",
    "        \"\"\"Get appropriate agent based on risk score.\"\"\"\n",
    "        if isinstance(state, torch.Tensor):\n",
    "            risk_score = state[3].item()  # Assuming risk score at index 3\n",
    "        else:\n",
    "            risk_score = state[3]\n",
    "\n",
    "        if risk_score < 0.3:\n",
    "            return self.low_risk_agent\n",
    "        elif risk_score < 0.7:\n",
    "            return self.med_risk_agent\n",
    "        else:\n",
    "            return self.high_risk_agent\n",
    "\n",
    "    def select_action(self, state, action_mask, training=True):\n",
    "        \"\"\"Route action selection to appropriate agent based on risk.\"\"\"\n",
    "        agent = self._get_agent_for_state(state)\n",
    "        return agent.select_action(state, action_mask, training)\n",
    "\n",
    "    def update(self, state, action, reward, next_state, next_action, done):\n",
    "        \"\"\"Update appropriate agent based on state risk.\"\"\"\n",
    "        agent = self._get_agent_for_state(state)\n",
    "        loss = agent.update(state, action, reward, next_state, next_action, done)\n",
    "        self.training_steps += 1\n",
    "        return loss\n",
    "\n",
    "    def add_experience(self, state, action, reward, next_state, next_action, done):\n",
    "        \"\"\"Add experience to replay buffer with prioritization.\"\"\"\n",
    "        # Extract risk score\n",
    "        if isinstance(state, torch.Tensor):\n",
    "            risk_score = state[3].item()\n",
    "            state = state.cpu()\n",
    "        else:\n",
    "            risk_score = state[3]\n",
    "\n",
    "        if isinstance(next_state, torch.Tensor):\n",
    "            next_state = next_state.cpu()\n",
    "\n",
    "        # Calculate priority based on reward magnitude, acute events, and risk\n",
    "        priority = abs(reward) * (1.0 + risk_score)\n",
    "\n",
    "        # Boost priority for experiences with acute events\n",
    "        if abs(reward) > 20:  # Likely an acute event\n",
    "            priority *= 2.0\n",
    "\n",
    "        # Boost priority for medium-risk patients (our problem area)\n",
    "        if 0.3 < risk_score < 0.7:\n",
    "            priority *= 3.0\n",
    "\n",
    "        # Store transition with priority\n",
    "        self.replay_buffer.append((state, action, reward, next_state, next_action, done, priority))\n",
    "\n",
    "        # Maintain buffer size\n",
    "        if len(self.replay_buffer) > self.max_buffer_size:\n",
    "            # When at capacity, sometimes remove lowest priority item instead of oldest\n",
    "            if random.random() < 0.3:  # 30% chance to use priority-based removal\n",
    "                min_priority_idx = np.argmin([exp[6] for exp in self.replay_buffer])\n",
    "                self.replay_buffer.pop(min_priority_idx)\n",
    "            else:\n",
    "                # Otherwise remove oldest (FIFO)\n",
    "                self.replay_buffer.pop(0)\n",
    "\n",
    "\n",
    "    def train_on_batch(self, batch_size=64):\n",
    "      \"\"\"Train with prioritized sampling from replay buffer.\"\"\"\n",
    "      if len(self.replay_buffer) < batch_size:\n",
    "          return 0.0\n",
    "\n",
    "      # Sample based on priority\n",
    "      priorities = np.array([exp[6] for exp in self.replay_buffer])\n",
    "      probs = priorities / priorities.sum()\n",
    "\n",
    "      # Sample indices based on priority\n",
    "      indices = np.random.choice(len(self.replay_buffer), min(batch_size, len(self.replay_buffer)),\n",
    "                                p=probs, replace=False)\n",
    "      batch = [self.replay_buffer[i] for i in indices]\n",
    "\n",
    "      # Unpack batch and convert to tensors (rest of the method remains the same)\n",
    "      states, actions, rewards, next_states, next_actions, dones, _ = zip(*batch)\n",
    "      state_batch = torch.stack([torch.tensor(s, device=DEVICE) for s in states])\n",
    "      action_batch = torch.tensor(actions, device=DEVICE).long()  # Ensure correct dtype\n",
    "      reward_batch = torch.tensor(rewards, dtype=torch.float32, device=DEVICE)\n",
    "      next_state_batch = torch.stack([torch.tensor(s, device=DEVICE) for s in next_states])\n",
    "      next_action_batch = torch.tensor(next_actions, device=DEVICE).long() # Ensure correct dtype\n",
    "      done_batch = torch.tensor(dones, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "      # Now, we need to route the training to the correct agent based on the state's risk level.\n",
    "      # For simplicity, we'll process each sample individually.  A more efficient approach\n",
    "      # would batch by agent, but this is clearer for demonstration.\n",
    "      total_loss = 0\n",
    "      for i in range(len(batch)):\n",
    "          agent = self._get_agent_for_state(state_batch[i])\n",
    "          current_q = agent.q_network(state_batch[i].unsqueeze(0)).gather(1, action_batch[i].unsqueeze(0).unsqueeze(0)).squeeze()\n",
    "\n",
    "          with torch.no_grad():\n",
    "              next_q = agent.q_network(next_state_batch[i].unsqueeze(0)).gather(1, next_action_batch[i].unsqueeze(0).unsqueeze(0)).squeeze()\n",
    "              next_q = torch.clamp(next_q, agent.min_value, agent.max_value)\n",
    "          target_q = reward_batch[i] + (1 - done_batch[i]) * agent.gamma * next_q\n",
    "          loss = F.smooth_l1_loss(current_q, target_q)\n",
    "          total_loss += loss.item()\n",
    "\n",
    "          agent.optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(agent.q_network.parameters(), 1.0)\n",
    "          agent.optimizer.step()\n",
    "          agent.epsilon = max(agent.epsilon_end, agent.epsilon * agent.epsilon_decay)\n",
    "\n",
    "      return total_loss / len(batch) # Return average loss\n",
    "\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"Save all agents to separate files.\"\"\"\n",
    "        base_path = path.replace('.pt', '')\n",
    "        self.low_risk_agent.save(f\"{base_path}_low_risk.pt\")\n",
    "        self.med_risk_agent.save(f\"{base_path}_med_risk.pt\")\n",
    "        self.high_risk_agent.save(f\"{base_path}_high_risk.pt\")\n",
    "\n",
    "        # Save metadata\n",
    "        torch.save({\n",
    "            'epsilon': self.epsilon,\n",
    "            'training_steps': self.training_steps,\n",
    "        }, f\"{base_path}_meta.pt\")\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"Load all agents from separate files.\"\"\"\n",
    "        base_path = path.replace('.pt', '')\n",
    "        self.low_risk_agent.load(f\"{base_path}_low_risk.pt\")\n",
    "        self.med_risk_agent.load(f\"{base_path}_med_risk.pt\")\n",
    "        self.high_risk_agent.load(f\"{base_path}_high_risk.pt\")\n",
    "\n",
    "        # Load metadata\n",
    "        meta = torch.load(f\"{base_path}_meta.pt\", map_location=DEVICE)\n",
    "        self.epsilon = meta['epsilon']\n",
    "        self.training_steps = meta['training_steps']\n",
    "\n",
    "    def evaluate(self, env, num_episodes=10):\n",
    "        \"\"\"Evaluate performance using all agents.\"\"\"\n",
    "        results = []\n",
    "        results.append(self.low_risk_agent.evaluate(env, num_episodes//3))\n",
    "        results.append(self.med_risk_agent.evaluate(env, num_episodes//3))\n",
    "        results.append(self.high_risk_agent.evaluate(env, num_episodes//3 + num_episodes % 3))\n",
    "\n",
    "        # Aggregate results\n",
    "        combined_results = {\n",
    "            'mean_reward': np.mean([r['mean_reward'] for r in results]),\n",
    "            'mean_acute_events': np.mean([r['mean_acute_events'] for r in results]),\n",
    "            'mean_safety_violations': np.mean([r['mean_safety_violations'] for r in results])\n",
    "        }\n",
    "\n",
    "        return combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeploymentManager:\n",
    "    \"\"\"Manages model deployment with risk-stratified decision making.\"\"\"\n",
    "\n",
    "    def __init__(self, sarsa_agent, status_quo_function):\n",
    "        \"\"\"\n",
    "        Initialize the deployment manager.\n",
    "\n",
    "        Args:\n",
    "            sarsa_agent: Trained SARSA agent\n",
    "            status_quo_function: Function implementing status quo recommendations\n",
    "        \"\"\"\n",
    "        self.sarsa_agent = sarsa_agent\n",
    "        self.status_quo_function = status_quo_function\n",
    "        self.stats = {\n",
    "            'sarsa_used': 0,\n",
    "            'status_quo_used': 0,\n",
    "            'sarsa_acute_events': 0,\n",
    "            'status_quo_acute_events': 0,\n",
    "            'total_patients': 0\n",
    "        }\n",
    "\n",
    "    def get_recommendation(self, patient):\n",
    "        \"\"\"Get optimal recommendation using risk-stratified approach.\"\"\"\n",
    "        # Extract risk score\n",
    "        risk_score = patient.features.get('riskScore', 0.5)\n",
    "\n",
    "        # Create state representation\n",
    "        state = patient.to_tensor()\n",
    "\n",
    "        # Generate action mask\n",
    "        action_mask = self._get_action_mask(patient)\n",
    "\n",
    "        # Use specific model based on risk level\n",
    "        # NOTE: Adjust these thresholds based on your best-performing run\n",
    "        if risk_score > 0.5:  # Medium to high risk\n",
    "            self.stats['sarsa_used'] += 1\n",
    "            action, _ = self.sarsa_agent.select_action(state, action_mask, training=False)\n",
    "            intervention = list(INTERVENTIONS.keys())[action.item()]\n",
    "            return intervention\n",
    "        else:\n",
    "            # Use status quo for other risk levels\n",
    "            self.stats['status_quo_used'] += 1\n",
    "            action = self.status_quo_function(patient, action_mask)\n",
    "            intervention = list(INTERVENTIONS.keys())[action.item()]\n",
    "            return intervention\n",
    "\n",
    "    def record_outcome(self, patient_id, recommendation_source, had_acute_event):\n",
    "        \"\"\"Record outcome for analysis.\"\"\"\n",
    "        if recommendation_source == 'sarsa':\n",
    "            if had_acute_event:\n",
    "                self.stats['sarsa_acute_events'] += 1\n",
    "        else:\n",
    "            if had_acute_event:\n",
    "                self.stats['status_quo_acute_events'] += 1\n",
    "\n",
    "        self.stats['total_patients'] += 1\n",
    "\n",
    "    def _get_action_mask(self, patient):\n",
    "        \"\"\"Generate appropriate action mask.\"\"\"\n",
    "        if hasattr(patient, 'generate_action_mask'):\n",
    "            return patient.generate_action_mask()\n",
    "        else:\n",
    "            return torch.ones(len(INTERVENTIONS), dtype=torch.bool, device=DEVICE)\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"Return usage and effectiveness statistics.\"\"\"\n",
    "        total = self.stats['sarsa_used'] + self.stats['status_quo_used']\n",
    "        if total == 0:\n",
    "            return {'sarsa_percentage': 0, 'status_quo_percentage': 0}\n",
    "\n",
    "        # Calculate acute event rates\n",
    "        sarsa_rate = self.stats['sarsa_acute_events'] / max(1, self.stats['sarsa_used'])\n",
    "        status_quo_rate = self.stats['status_quo_acute_events'] / max(1, self.stats['status_quo_used'])\n",
    "\n",
    "        return {\n",
    "            'sarsa_percentage': self.stats['sarsa_used'] / total * 100,\n",
    "            'status_quo_percentage': self.stats['status_quo_used'] / total * 100,\n",
    "            'sarsa_acute_rate': sarsa_rate,\n",
    "            'status_quo_acute_rate': status_quo_rate,\n",
    "            'relative_reduction': (status_quo_rate - sarsa_rate) / status_quo_rate * 100 if status_quo_rate > 0 else 0,\n",
    "            'total_recommendations': total\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedSARSAAgent(SARSAAgent):\n",
    "    \"\"\"Enhanced SARSA agent with improved training and policy implementation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        n_actions: int,\n",
    "        hidden_dim: int = 256,\n",
    "        learning_rate: float = 3e-4,\n",
    "        gamma: float = 0.99,\n",
    "        epsilon_start: float = 1.0,\n",
    "        epsilon_end: float = 0.1,\n",
    "        epsilon_decay: float = 0.995,\n",
    "        max_buffer_size: int = 100000\n",
    "    ):\n",
    "        super().__init__(\n",
    "            state_dim, n_actions, hidden_dim, learning_rate,\n",
    "            gamma, epsilon_start, epsilon_end, epsilon_decay, max_buffer_size\n",
    "        )\n",
    "\n",
    "        # Replace default network with more advanced architecture\n",
    "        self.q_network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, n_actions)\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Better initialization\n",
    "        for m in self.q_network.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=1.414)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "        # Action tracking for encouraging diversity\n",
    "        self.action_counts = np.ones(n_actions) * 0.1  # Initialize with small counts\n",
    "\n",
    "        # Optimizer with weight decay\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.q_network.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=1e-4  # Add regularization\n",
    "        )\n",
    "\n",
    "    def train_on_batch(self, batch_size=64):\n",
    "        \"\"\"Train with basic sampling from replay buffer.\"\"\"\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return 0.0\n",
    "\n",
    "        # Simple random sampling from replay buffer\n",
    "        batch = random.sample(self.replay_buffer, batch_size)\n",
    "\n",
    "        # Unpack batch\n",
    "        states, actions, rewards, next_states, next_actions, dones, _ = zip(*batch)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = torch.stack([torch.tensor(s, device=DEVICE) for s in states])\n",
    "        action_batch = torch.tensor(actions, device=DEVICE)\n",
    "        reward_batch = torch.tensor(rewards, dtype=torch.float32, device=DEVICE)\n",
    "        next_state_batch = torch.stack([torch.tensor(s, device=DEVICE) for s in next_states])\n",
    "        next_action_batch = torch.tensor(next_actions, device=DEVICE)\n",
    "        done_batch = torch.tensor(dones, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "        # Get current Q values\n",
    "        current_q_values = self.q_network(state_batch).gather(1, action_batch.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Get next Q values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.q_network(next_state_batch).gather(1, next_action_batch.unsqueeze(1)).squeeze(1)\n",
    "            next_q_values = torch.clamp(next_q_values, self.min_value, self.max_value)\n",
    "\n",
    "        # Compute target Q values\n",
    "        target_q_values = reward_batch + (1 - done_batch) * self.gamma * next_q_values\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
    "\n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update epsilon\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function for SARSA training and evaluation.\n",
    "    \"\"\"\n",
    "    # Set up logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "\n",
    "    # Parse command line arguments\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description='Train and evaluate SARSA for clinical decision support')\n",
    "    parser.add_argument('--data_dir', type=str, default='data', help='Directory containing data splits')\n",
    "    parser.add_argument('--log_dir', type=str, default='sarsa_results', help='Directory to save results')\n",
    "    parser.add_argument('--synthetic', action='store_true', help='Use synthetic data if true')\n",
    "    parser.add_argument('--n_synthetic', type=int, default=1000, help='Number of synthetic sequences if using synthetic data')\n",
    "    parser.add_argument('--n_epochs', type=int, default=100, help='Number of training epochs')\n",
    "    parser.add_argument('--batch_size', type=int, default=64, help='Batch size for training')\n",
    "    parser.add_argument('--eval_freq', type=int, default=10, help='Evaluation frequency in epochs')\n",
    "    parser.add_argument('--checkpoint_freq', type=int, default=20, help='Checkpoint frequency in epochs')\n",
    "    parser.add_argument('--load_model', type=str, default=None, help='Path to model to load (optional)')\n",
    "    parser.add_argument('--evaluate_only', action='store_true', help='Only evaluate, no training')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(args.log_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize data loader\n",
    "    data_loader = ProcessedDataLoader(data_dir=args.data_dir)\n",
    "\n",
    "    # Load or generate data\n",
    "    if args.synthetic:\n",
    "        logging.info(f\"Generating synthetic data with {args.n_synthetic} sequences\")\n",
    "        train_data, val_data, test_data = data_loader.create_synthetic_data(\n",
    "            n_sequences=args.n_synthetic,\n",
    "            output_dir=args.data_dir\n",
    "        )\n",
    "    else:\n",
    "        # Load real data\n",
    "        logging.info(\"Loading training data...\")\n",
    "        train_data, train_meta = data_loader.load_split('train')\n",
    "\n",
    "        logging.info(\"Loading validation data...\")\n",
    "        val_data, val_meta = data_loader.load_split('val')\n",
    "\n",
    "        # Optional test data loading\n",
    "        try:\n",
    "            test_data, test_meta = data_loader.load_split('test')\n",
    "            logging.info(f\"Loaded {len(test_data)} test sequences\")\n",
    "        except:\n",
    "            test_data = []\n",
    "            logging.info(\"No test data found\")\n",
    "\n",
    "    logging.info(f\"Loaded {len(train_data)} training sequences and {len(val_data)} validation sequences\")\n",
    "\n",
    "    # Check if we have enough data\n",
    "    if len(train_data) < 10 or len(val_data) < 5:\n",
    "        logging.error(\"Insufficient data for training. Please provide more data or use synthetic data.\")\n",
    "        if len(train_data) == 0:\n",
    "            logging.info(\"Generating minimal synthetic data for demonstration\")\n",
    "            train_data, val_data, _ = data_loader.create_synthetic_data(\n",
    "                n_sequences=100,\n",
    "                output_dir=args.data_dir\n",
    "            )\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    # Initialize environment\n",
    "    env = ClinicalEnvironment(max_sequence_length=50)\n",
    "    env.set_sequences(train_data)\n",
    "\n",
    "    # Get state dimension from first state\n",
    "    sample_state = env.reset()\n",
    "    state_tensor = sample_state.to_tensor(env.state_cache)\n",
    "    state_dim = state_tensor.shape[0]\n",
    "\n",
    "    logging.info(f\"State dimension: {state_dim}\")\n",
    "\n",
    "    # Initialize SARSA agent\n",
    "    agent = EnhancedSARSAAgent(\n",
    "        state_dim=state_dim,\n",
    "        n_actions=len(INTERVENTIONS),\n",
    "        learning_rate=3e-4,\n",
    "        hidden_dim=256,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.1,\n",
    "        epsilon_decay=0.975,  # Make sure this is < 1.0 and not too close to 1.0\n",
    "        max_buffer_size=100000\n",
    "    )\n",
    "\n",
    "    # Load model if specified\n",
    "    if args.load_model:\n",
    "        logging.info(f\"Loading model from {args.load_model}\")\n",
    "        agent.load(args.load_model)\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = SARSATrainer(\n",
    "        agent=agent,\n",
    "        env=env,\n",
    "        train_sequences=train_data,\n",
    "        val_sequences=val_data,\n",
    "        log_dir=args.log_dir\n",
    "    )\n",
    "\n",
    "    # Evaluate only if specified\n",
    "    if args.evaluate_only:\n",
    "        logging.info(\"Running evaluation only\")\n",
    "        eval_metrics = trainer._evaluate(n_episodes=min(100, len(val_data)))\n",
    "        clinical_impact = trainer._calculate_clinical_impact()\n",
    "\n",
    "        logging.info(f\"Evaluation metrics: {json.dumps(eval_metrics, indent=2)}\")\n",
    "        logging.info(f\"Clinical impact: {json.dumps(clinical_impact, indent=2)}\")\n",
    "\n",
    "        # Save results\n",
    "        trainer.save_results(os.path.join(args.log_dir, \"eval_results.json\"))\n",
    "        return 0\n",
    "\n",
    "    # Train model\n",
    "    logging.info(\"Starting training...\")\n",
    "    final_metrics, clinical_impact = trainer.train(\n",
    "        n_epochs=args.n_epochs,\n",
    "        batch_size=args.batch_size,\n",
    "        eval_freq=args.eval_freq,\n",
    "        checkpoint_freq=args.checkpoint_freq,\n",
    "        updates_per_epoch=200,\n",
    "        early_stopping_patience=30\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    trainer.save_results()\n",
    "\n",
    "    # Print final metrics\n",
    "    logging.info(\"Training complete!\")\n",
    "    logging.info(f\"Final metrics: {json.dumps(final_metrics, indent=2)}\")\n",
    "    logging.info(f\"Clinical impact: NNT={clinical_impact['nnt']:.2f}, \" +\n",
    "               f\"Acute reduction: {clinical_impact['acute_reduction']*100:.2f}%\")\n",
    "\n",
    "    # Final evaluation on test set if available\n",
    "    if test_data:\n",
    "        logging.info(\"Evaluating on test set...\")\n",
    "        # Set test data in environment\n",
    "        env.set_sequences(test_data)\n",
    "\n",
    "        # Evaluate\n",
    "        test_metrics = agent.evaluate(env, num_episodes=min(100, len(test_data)))\n",
    "        logging.info(f\"Test metrics: {json.dumps(test_metrics, indent=2)}\")\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_deployment_strategy(val_data, sarsa_agent, status_quo_function):\n",
    "    \"\"\"Evaluate the risk-stratified deployment approach.\"\"\"\n",
    "    deployment = DeploymentManager(sarsa_agent, status_quo_function)\n",
    "\n",
    "    results = {'sarsa_events': 0, 'status_quo_events': 0, 'hybrid_events': 0, 'total': 0}\n",
    "\n",
    "    for patient in val_data:\n",
    "        # Calculate what would happen with each approach\n",
    "        sarsa_outcome = simulate_sarsa_outcome(patient, sarsa_agent)\n",
    "        status_quo_outcome = simulate_status_quo_outcome(patient, status_quo_function)\n",
    "\n",
    "        # Get hybrid recommendation and outcome\n",
    "        recommendation = deployment.get_recommendation(patient)\n",
    "        hybrid_outcome = simulate_outcome(patient, recommendation)\n",
    "\n",
    "        # Track results\n",
    "        results['sarsa_events'] += sarsa_outcome['acute_events']\n",
    "        results['status_quo_events'] += status_quo_outcome['acute_events']\n",
    "        results['hybrid_events'] += hybrid_outcome['acute_events']\n",
    "        results['total'] += 1\n",
    "\n",
    "    # Calculate rates\n",
    "    results['sarsa_rate'] = results['sarsa_events'] / results['total']\n",
    "    results['status_quo_rate'] = results['status_quo_events'] / results['total']\n",
    "    results['hybrid_rate'] = results['hybrid_events'] / results['total']\n",
    "\n",
    "    # Calculate improvements\n",
    "    results['hybrid_vs_sarsa'] = (results['sarsa_rate'] - results['hybrid_rate']) / results['sarsa_rate'] * 100\n",
    "    results['hybrid_vs_status_quo'] = (results['status_quo_rate'] - results['hybrid_rate']) / results['status_quo_rate'] * 100\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sarsa_comparison(n_episodes=500, n_val_episodes=300):\n",
    "    \"\"\"\n",
    "    Run a focused comparison between SARSA and status quo approaches.\n",
    "\n",
    "    Args:\n",
    "        n_episodes: Number of episodes to train on\n",
    "        n_val_episodes: Number of episodes to evaluate on\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of comparison results\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "\n",
    "    # Create synthetic data if real data not available\n",
    "    train_sequences = []\n",
    "    val_sequences = []\n",
    "\n",
    "    for i in range(max(1000, n_episodes * 2)):\n",
    "        # Create synthetic patient data\n",
    "        seq = {\n",
    "            'patient_id': f'patient_{i}',\n",
    "            'features': {\n",
    "                'age': random.uniform(18, 80),\n",
    "                'gender': random.choice(['Male', 'Female']),\n",
    "                'race': random.choice(['White', 'Black', 'Hispanic', 'Asian', 'Other']),\n",
    "                'region': random.choice(['Virginia', 'Washington']),\n",
    "                'riskScore': random.uniform(0.2, 0.8)\n",
    "            },\n",
    "            'encounters': [{'daysSinceLastEncounter': random.randint(1, 14)} for _ in range(10)],\n",
    "            'history': []\n",
    "        }\n",
    "\n",
    "        # Add risk factors with varying distributions\n",
    "        if i % 5 == 0:  # 20% high medical risk\n",
    "            seq['medical_risk'] = random.uniform(2.0, 4.0)\n",
    "            seq['behavioral_risk'] = random.uniform(0.5, 2.0)\n",
    "            seq['social_risk'] = random.uniform(0.5, 2.0)\n",
    "        elif i % 5 == 1:  # 20% high behavioral risk\n",
    "            seq['medical_risk'] = random.uniform(0.5, 2.0)\n",
    "            seq['behavioral_risk'] = random.uniform(2.0, 4.0)\n",
    "            seq['social_risk'] = random.uniform(0.5, 2.0)\n",
    "        elif i % 5 == 2:  # 20% high social risk\n",
    "            seq['medical_risk'] = random.uniform(0.5, 2.0)\n",
    "            seq['behavioral_risk'] = random.uniform(0.5, 2.0)\n",
    "            seq['social_risk'] = random.uniform(2.0, 4.0)\n",
    "        else:  # 40% mixed risk\n",
    "            seq['medical_risk'] = random.uniform(0.5, 3.0)\n",
    "            seq['behavioral_risk'] = random.uniform(0.5, 3.0)\n",
    "            seq['social_risk'] = random.uniform(0.5, 3.0)\n",
    "            \n",
    "        #Adding the risk summary\n",
    "        seq['risk_summary'] = {\n",
    "            'medical_risk_mentions': seq['medical_risk'],\n",
    "            'behavioral_risk_mentions': seq['behavioral_risk'],\n",
    "            'social_risk_mentions': seq['social_risk']\n",
    "        }\n",
    "\n",
    "        if i < (max(1000, n_episodes * 2) * 0.8):  # 80% for training\n",
    "            train_sequences.append(seq)\n",
    "        else:  # 20% for validation\n",
    "            val_sequences.append(seq)\n",
    "\n",
    "    # Initialize environment\n",
    "    env = ClinicalEnvironment(max_sequence_length=20)\n",
    "    env.set_sequences(train_sequences)\n",
    "\n",
    "    # Get state dimension from first state\n",
    "    sample_state = env.reset()\n",
    "    state_tensor = sample_state.to_tensor(env.state_cache)\n",
    "    state_dim = state_tensor.shape[0]\n",
    "\n",
    "    print(f\"State dimension: {state_dim}\")\n",
    "\n",
    "    # Initialize SARSA agent\n",
    "    agent = EnhancedSARSAAgent(\n",
    "        state_dim=state_dim,\n",
    "        n_actions=len(INTERVENTIONS),\n",
    "        learning_rate=3e-4,\n",
    "        hidden_dim=256,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.05,  # Lower epsilon end for better exploitation\n",
    "        epsilon_decay=0.99,  # Slower decay\n",
    "        max_buffer_size=100000\n",
    "    )\n",
    "\n",
    "    # Initialize trainer with output directory\n",
    "    log_dir = f\"sarsa_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    trainer = SARSATrainer(\n",
    "        agent=agent,\n",
    "        env=env,\n",
    "        train_sequences=train_sequences,\n",
    "        val_sequences=val_sequences,\n",
    "        log_dir=log_dir\n",
    "    )\n",
    "\n",
    "    # Run experience collection with more episodes\n",
    "    print(\"\\n--- Starting Comprehensive Training ---\")\n",
    "    print(f\"Training on {n_episodes} episodes\")\n",
    "    trainer._collect_experience(n_episodes=n_episodes)\n",
    "\n",
    "    # Train model with substantially more updates\n",
    "    total_updates = 200  # Increased from 20\n",
    "    for i in range(total_updates):\n",
    "        # Only print updates periodically to reduce console spam\n",
    "        if i % 10 == 0 or i == total_updates - 1:\n",
    "            print(f\"Training batch {i+1}/{total_updates}\")\n",
    "        else:\n",
    "            print(f\"Training batch {i+1}/{total_updates}\", end=\"\\r\")\n",
    "\n",
    "        # Train on batch\n",
    "        agent.train_on_batch(256)  # Larger batch size\n",
    "\n",
    "        # Learning rate decay\n",
    "        if i > 0 and i % 50 == 0:\n",
    "            current_lr = agent.optimizer.param_groups[0]['lr']\n",
    "            for param_group in agent.optimizer.param_groups:\n",
    "                param_group['lr'] *= 0.75\n",
    "            print(f\"\\nReducing learning rate: {current_lr:.6f}  {agent.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    print(\"\\n--- Training complete, running evaluation ---\")\n",
    "\n",
    "    # Run evaluation with more episodes\n",
    "    eval_metrics = trainer._evaluate(n_episodes=min(n_val_episodes, len(val_sequences)))\n",
    "\n",
    "\n",
    "    # Calculate clinical impact with more sequences\n",
    "    clinical_impact = trainer._calculate_clinical_impact()\n",
    "\n",
    "    # Calculate and add relative reduction\n",
    "    if clinical_impact['status_quo_acute_rate'] > 0:\n",
    "        clinical_impact['relative_reduction'] = (clinical_impact['acute_reduction'] /\n",
    "                                              clinical_impact['status_quo_acute_rate']) * 100\n",
    "    else:\n",
    "        clinical_impact['relative_reduction'] = 0.0\n",
    "\n",
    "    # Create visualizations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(['SARSA', 'Status Quo'],\n",
    "            [clinical_impact['sarsa_acute_rate'], clinical_impact['status_quo_acute_rate']],\n",
    "            color=['#2171b5', '#cb181d'])\n",
    "\n",
    "    # Add labels\n",
    "    plt.text(0, clinical_impact['sarsa_acute_rate'] + 0.05,\n",
    "             f\"{clinical_impact['sarsa_acute_rate']:.2f}\", ha='center')\n",
    "    plt.text(1, clinical_impact['status_quo_acute_rate'] + 0.05,\n",
    "             f\"{clinical_impact['status_quo_acute_rate']:.2f}\", ha='center')\n",
    "\n",
    "    # Rest of the plotting code remains unchanged...\n",
    "    # Add reduction indicator\n",
    "    if clinical_impact['acute_reduction'] > 0:\n",
    "        plt.annotate(\n",
    "            f\"{clinical_impact['relative_reduction']:.1f}% Reduction\\nNNT = {clinical_impact['nnt']:.1f}\",\n",
    "            xy=(0.5, max(clinical_impact['sarsa_acute_rate'], clinical_impact['status_quo_acute_rate']) * 1.1),\n",
    "            xytext=(0.5, max(clinical_impact['sarsa_acute_rate'], clinical_impact['status_quo_acute_rate']) * 1.3),\n",
    "            ha='center',\n",
    "            arrowprops=dict(arrowstyle='->'),\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8)\n",
    "        )\n",
    "    else:\n",
    "        plt.annotate(\n",
    "            f\"{abs(clinical_impact['relative_reduction']):.1f}% Increase\\nNNH = {clinical_impact['nnh']:.1f}\",\n",
    "            xy=(0.5, max(clinical_impact['sarsa_acute_rate'], clinical_impact['status_quo_acute_rate']) * 1.1),\n",
    "            xytext=(0.5, max(clinical_impact['sarsa_acute_rate'], clinical_impact['status_quo_acute_rate']) * 1.3),\n",
    "            ha='center',\n",
    "            arrowprops=dict(arrowstyle='->'),\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8)\n",
    "        )\n",
    "\n",
    "    plt.title('Acute Event Rate Comparison')\n",
    "    plt.ylabel('Acute Events per Patient')\n",
    "    plt.savefig(os.path.join(log_dir, 'acute_event_comparison.png'))\n",
    "\n",
    "    # Generate publication-quality figures\n",
    "    generate_paper_figures(clinical_impact, log_dir)\n",
    "\n",
    "    # Print key metrics\n",
    "    print(\"\\n=== SARSA vs. Status Quo Comparison ===\")\n",
    "    print(f\"SARSA acute event rate: {clinical_impact['sarsa_acute_rate']:.4f}\")\n",
    "    print(f\"Status quo acute event rate: {clinical_impact['status_quo_acute_rate']:.4f}\")\n",
    "    print(f\"Absolute reduction: {clinical_impact['acute_reduction']:.4f}\")\n",
    "    print(f\"Relative reduction: {clinical_impact['relative_reduction']:.2f}%\")\n",
    "\n",
    "    if clinical_impact['acute_reduction'] > 0:\n",
    "        print(f\"Number needed to treat (NNT): {clinical_impact['nnt']:.2f}\")\n",
    "    else:\n",
    "        print(f\"Number needed to harm (NNH): {clinical_impact['nnh']:.2f}\")\n",
    "\n",
    "\n",
    "    # Save results to JSON\n",
    "    with open(os.path.join(log_dir, 'comparison_results.json'), 'w') as f:\n",
    "        json.dump({\n",
    "            'clinical_impact': clinical_impact,\n",
    "            'eval_metrics': {k: float(v) if isinstance(v, (int, float, np.number)) else v\n",
    "                           for k, v in eval_metrics.items() if not isinstance(v, dict)}\n",
    "        }, f, indent=2)\n",
    "\n",
    "    return {\n",
    "        'clinical_impact': clinical_impact,\n",
    "        'eval_metrics': eval_metrics,\n",
    "        'log_dir': log_dir\n",
    "    }\n",
    "\n",
    "def generate_paper_figures(clinical_impact, output_dir=\"paper_figures\"):\n",
    "    \"\"\"\n",
    "    Generate publication-quality figures for research paper.\n",
    "\n",
    "    Args:\n",
    "        clinical_impact: Clinical impact metrics dictionary\n",
    "        output_dir: Directory to save figures\n",
    "\n",
    "    Returns:\n",
    "        List of figure paths\n",
    "    \"\"\"\n",
    "    import matplotlib as mpl\n",
    "\n",
    "    # Set up publication-quality styles\n",
    "    mpl.rcParams['font.family'] = 'Arial'\n",
    "    mpl.rcParams['font.size'] = 10\n",
    "    mpl.rcParams['axes.linewidth'] = 0.5\n",
    "    mpl.rcParams['axes.labelsize'] = 11\n",
    "    mpl.rcParams['axes.titlesize'] = 12\n",
    "    mpl.rcParams['xtick.labelsize'] = 9\n",
    "    mpl.rcParams['ytick.labelsize'] = 9\n",
    "    mpl.rcParams['legend.fontsize'] = 9\n",
    "    mpl.rcParams['figure.figsize'] = [6.4, 4.8]\n",
    "    mpl.rcParams['savefig.dpi'] = 300\n",
    "    mpl.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    figure_paths = []\n",
    "\n",
    "    # Figure 1: Acute Event Reduction\n",
    "    plt.figure(figsize=(4, 3.5))\n",
    "\n",
    "    # Create bar chart\n",
    "    approaches = ['SARSA', 'Status Quo']\n",
    "    acute_rates = [clinical_impact['sarsa_acute_rate'], clinical_impact['status_quo_acute_rate']]\n",
    "\n",
    "    bars = plt.bar(approaches, acute_rates, color=['#2171b5', '#cb181d'], width=0.6)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, rate in enumerate(acute_rates):\n",
    "        plt.text(i, rate + 0.01, f\"{rate:.2f}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # Add reduction indicator\n",
    "    if clinical_impact['acute_reduction'] > 0:\n",
    "        reduction_pct = clinical_impact['relative_reduction']\n",
    "        plt.annotate(\n",
    "            f\"{reduction_pct:.1f}% Reduction\",\n",
    "            xy=(0.5, max(acute_rates) * 1.1),\n",
    "            xytext=(0.5, max(acute_rates) * 1.25),\n",
    "            ha='center',\n",
    "            va='center',\n",
    "            arrowprops=dict(arrowstyle='->', lw=0.8, color='black'),\n",
    "            bbox=dict(boxstyle='round,pad=0.3', fc='white', ec='gray', lw=0.5)\n",
    "        )\n",
    "\n",
    "    # Add NNT\n",
    "    plt.text(1.0, acute_rates[1] * 0.5, f\"NNT = {clinical_impact['nnt']:.1f}\",\n",
    "             ha='center', va='center', fontsize=9,\n",
    "             bbox=dict(boxstyle='round,pad=0.3', fc='white', ec='gray', lw=0.5, alpha=0.8))\n",
    "\n",
    "    # Add p-value\n",
    "    p_value = clinical_impact['p_value']\n",
    "    sig_text = f\"p = {p_value:.3f}\"\n",
    "    if p_value < 0.05:\n",
    "        sig_text += \" *\"\n",
    "        if p_value < 0.01:\n",
    "            sig_text += \"*\"\n",
    "            if p_value < 0.001:\n",
    "                sig_text += \"*\"\n",
    "\n",
    "    plt.text(0.5, -0.05, sig_text, ha='center', va='top', transform=plt.gca().transAxes, fontsize=9)\n",
    "\n",
    "    # Formatting\n",
    "    plt.ylabel('Acute Events per Patient')\n",
    "    plt.title('Comparison of Acute Event Rates')\n",
    "    plt.ylim(0, max(acute_rates) * 1.4)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "\n",
    "    # Save figure\n",
    "    fig1_path = os.path.join(output_dir, 'figure1_acute_events.png')\n",
    "    plt.savefig(fig1_path)\n",
    "    plt.savefig(fig1_path.replace('.png', '.pdf'))\n",
    "    figure_paths.append(fig1_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Figure 2: Risk-Stratified Analysis\n",
    "    plt.figure(figsize=(5, 3.5))\n",
    "\n",
    "    # Create data\n",
    "    risk_groups = ['Low Risk', 'Medium Risk', 'High Risk']\n",
    "    nnt_values = [\n",
    "        clinical_impact['low_risk_nnt'],\n",
    "        clinical_impact['medium_risk_nnt'],\n",
    "        clinical_impact['high_risk_nnt']\n",
    "    ]\n",
    "\n",
    "    # Cap infinite values for visualization\n",
    "    nnt_values = [min(v, 40) for v in nnt_values]\n",
    "\n",
    "    # Create bar chart\n",
    "    bars = plt.bar(risk_groups, nnt_values, color='#2171b5', width=0.6)\n",
    "\n",
    "    # Add labels\n",
    "    for i, nnt in enumerate(nnt_values):\n",
    "        if nnt >= 40:\n",
    "            plt.text(i, nnt - 2, \"\", ha='center', va='top', fontsize=16, color='white')\n",
    "        else:\n",
    "            risk_keys = ['low_risk_nnt', 'medium_risk_nnt', 'high_risk_nnt']\n",
    "            plt.text(i, nnt + 1, f\"{clinical_impact.get(risk_keys[i], float('inf')):.1f}\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # Add reference line for NNT = 10\n",
    "    plt.axhline(y=10, color='r', linestyle='--', alpha=0.6, lw=0.8)\n",
    "    plt.text(len(risk_groups)-1, 10.5, 'NNT = 10', color='r', ha='right', fontsize=8)\n",
    "\n",
    "    # Formatting\n",
    "    plt.ylabel('Number Needed to Treat')\n",
    "    plt.title('Risk-Stratified Analysis: NNT by Risk Level')\n",
    "    plt.ylim(0, 45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "\n",
    "    # Save figure\n",
    "    fig2_path = os.path.join(output_dir, 'figure2_risk_stratified.png')\n",
    "    plt.savefig(fig2_path)\n",
    "    plt.savefig(fig2_path.replace('.png', '.pdf'))\n",
    "    figure_paths.append(fig2_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Generated {len(figure_paths)} figures in {output_dir}\")\n",
    "    return figure_paths\n",
    "\n",
    "\n",
    "# Execute this to run your SARSA evaluation\n",
    "results = run_sarsa_comparison(\n",
    "    n_episodes=200,       # Number of episodes to train on\n",
    "    n_val_episodes=100    # Number of episodes for evaluation\n",
    ")\n",
    "\n",
    "# You can then access the results\n",
    "print(f\"Clinical impact: NNT={results['clinical_impact']['nnt']:.2f}\")\n",
    "print(f\"Acute reduction: {results['clinical_impact']['acute_reduction']*100:.2f}%\")\n",
    "print(f\"Output saved to: {results['log_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    pip install scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_fairness_by_demographics(model_agent, status_quo_function, val_data, log_dir):\n",
    "    \"\"\"\n",
    "    Analyze model fairness across demographic groups using equalized odds method.\n",
    "    \n",
    "    Args:\n",
    "        model_agent: Trained SARSA agent\n",
    "        status_quo_function: Function implementing status quo recommendations\n",
    "        val_data: Validation dataset\n",
    "        log_dir: Directory to save results and visualizations\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with fairness metrics by demographic group\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from collections import defaultdict\n",
    "    import torch\n",
    "    \n",
    "    # Initialize environment for simulation\n",
    "    env = ClinicalEnvironment(max_sequence_length=20)\n",
    "    \n",
    "    # Initialize result containers\n",
    "    demographic_metrics = {\n",
    "        'gender': defaultdict(lambda: {'sarsa': {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0, 'total': 0},\n",
    "                                      'status_quo': {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0, 'total': 0}}),\n",
    "        'age_group': defaultdict(lambda: {'sarsa': {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0, 'total': 0},\n",
    "                                         'status_quo': {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0, 'total': 0}}),\n",
    "        'race': defaultdict(lambda: {'sarsa': {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0, 'total': 0},\n",
    "                                    'status_quo': {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0, 'total': 0}})\n",
    "    }\n",
    "    \n",
    "    print(\"\\n--- Starting Fairness Analysis by Demographics ---\")\n",
    "    \n",
    "    # Process each patient in validation data\n",
    "    for patient_idx, patient in enumerate(val_data):\n",
    "        if patient_idx % 50 == 0:\n",
    "            print(f\"Processing patient {patient_idx}/{len(val_data)}\")\n",
    "            \n",
    "        # Skip if essential features are missing\n",
    "        if not isinstance(patient, dict) or 'features' not in patient:\n",
    "            continue\n",
    "            \n",
    "        # Extract demographic information\n",
    "        gender = patient['features'].get('gender', 'Unknown')\n",
    "        \n",
    "        # Calculate age group from birthDate if available\n",
    "        age = None\n",
    "        birth_date = None\n",
    "        for field in ['birthDate', 'birth_date']:\n",
    "            if field in patient['features']:\n",
    "                birth_date = patient['features'][field]\n",
    "                break\n",
    "                \n",
    "        if birth_date:\n",
    "            # Handle different date formats\n",
    "            if isinstance(birth_date, str):\n",
    "                try:\n",
    "                    from datetime import datetime\n",
    "                    birth_date = datetime.fromisoformat(birth_date.replace('Z', '+00:00'))\n",
    "                    current_date = datetime.now()\n",
    "                    age = current_date.year - birth_date.year - ((current_date.month, current_date.day) < (birth_date.month, birth_date.day))\n",
    "                except:\n",
    "                    age = None\n",
    "            else:\n",
    "                # Try to extract from date object\n",
    "                try:\n",
    "                    from datetime import datetime\n",
    "                    current_date = datetime.now()\n",
    "                    age = current_date.year - birth_date.year - ((current_date.month, current_date.day) < (birth_date.month, birth_date.day))\n",
    "                except:\n",
    "                    age = None\n",
    "                    \n",
    "        # Determine age group\n",
    "        if age is not None:\n",
    "            if age < 35:\n",
    "                age_group = \"18-34\"\n",
    "            elif age < 50:\n",
    "                age_group = \"35-49\"\n",
    "            elif age < 65:\n",
    "                age_group = \"50-64\"\n",
    "            else:\n",
    "                age_group = \"65+\"\n",
    "        else:\n",
    "            age_group = \"Unknown\"\n",
    "            \n",
    "        # Extract race/ethnicity\n",
    "        race = patient['features'].get('race', 'Unknown')\n",
    "        \n",
    "        # If race is complex (e.g., dictionary or list), simplify\n",
    "        if isinstance(race, (dict, list)) or ',' in str(race):\n",
    "            # Try to extract the first race mentioned\n",
    "            try:\n",
    "                if isinstance(race, dict) and 'primary' in race:\n",
    "                    race = race['primary']\n",
    "                elif isinstance(race, list) and len(race) > 0:\n",
    "                    race = race[0]\n",
    "                elif isinstance(race, str) and ',' in race:\n",
    "                    race = race.split(',')[0].strip()\n",
    "                else:\n",
    "                    race = \"Multiple\"\n",
    "            except:\n",
    "                race = \"Unknown\"\n",
    "                \n",
    "        # Simplify race categories for analysis\n",
    "        if race in ['White', 'Caucasian', 'white']:\n",
    "            race = 'White'\n",
    "        elif race in ['Black', 'African American', 'black']:\n",
    "            race = 'Black'\n",
    "        elif race in ['Hispanic', 'Latino', 'Latinx', 'hispanic', 'latino']:\n",
    "            race = 'Hispanic'\n",
    "        elif race in ['Asian', 'asian']:\n",
    "            race = 'Asian'\n",
    "        elif race in ['Unknown', 'unknown', None, 'null', 'NULL', '']:\n",
    "            race = 'Unknown'\n",
    "        else:\n",
    "            race = 'Other'\n",
    "            \n",
    "        # Simulate both SARSA and status quo trajectories\n",
    "        env.set_sequences([patient])\n",
    "        \n",
    "        try:\n",
    "            # Run SARSA trajectory\n",
    "            sarsa_outcome = simulate_trajectory(env, model_agent, True)\n",
    "            \n",
    "            # Reset for status quo\n",
    "            env.reset()\n",
    "            status_quo_outcome = simulate_trajectory(env, status_quo_function, False)\n",
    "            \n",
    "            # Get ground truth outcome (if acute event happened)\n",
    "            sarsa_acute = any(step.get('is_acute', False) for step in sarsa_outcome)\n",
    "            status_quo_acute = any(step.get('is_acute', False) for step in status_quo_outcome)\n",
    "            \n",
    "            # For simplicity, we'll use whether acute events happened as our binary outcome\n",
    "            # This is a simplification for demonstration purposes\n",
    "            \n",
    "            # Calculate confusion matrix statistics\n",
    "            # For SARSA\n",
    "            if sarsa_acute:  # Predicted positive\n",
    "                if True:  # Assuming ground truth would be acute (simplified for demo)\n",
    "                    demographic_metrics['gender'][gender]['sarsa']['tp'] += 1\n",
    "                    demographic_metrics['age_group'][age_group]['sarsa']['tp'] += 1\n",
    "                    demographic_metrics['race'][race]['sarsa']['tp'] += 1\n",
    "                else:\n",
    "                    demographic_metrics['gender'][gender]['sarsa']['fp'] += 1\n",
    "                    demographic_metrics['age_group'][age_group]['sarsa']['fp'] += 1\n",
    "                    demographic_metrics['race'][race]['sarsa']['fp'] += 1\n",
    "            else:  # Predicted negative\n",
    "                if False:  # Assuming ground truth would be non-acute (simplified for demo)\n",
    "                    demographic_metrics['gender'][gender]['sarsa']['tn'] += 1\n",
    "                    demographic_metrics['age_group'][age_group]['sarsa']['tn'] += 1\n",
    "                    demographic_metrics['race'][race]['sarsa']['tn'] += 1\n",
    "                else:\n",
    "                    demographic_metrics['gender'][gender]['sarsa']['fn'] += 1\n",
    "                    demographic_metrics['age_group'][age_group]['sarsa']['fn'] += 1\n",
    "                    demographic_metrics['race'][race]['sarsa']['fn'] += 1\n",
    "            \n",
    "            # For status quo\n",
    "            if status_quo_acute:  # Predicted positive\n",
    "                if True:  # Assuming ground truth would be acute (simplified for demo)\n",
    "                    demographic_metrics['gender'][gender]['status_quo']['tp'] += 1\n",
    "                    demographic_metrics['age_group'][age_group]['status_quo']['tp'] += 1\n",
    "                    demographic_metrics['race'][race]['status_quo']['tp'] += 1\n",
    "                else:\n",
    "                    demographic_metrics['gender'][gender]['status_quo']['fp'] += 1\n",
    "                    demographic_metrics['age_group'][age_group]['status_quo']['fp'] += 1\n",
    "                    demographic_metrics['race'][race]['status_quo']['fp'] += 1\n",
    "            else:  # Predicted negative\n",
    "                if False:  # Assuming ground truth would be non-acute (simplified for demo)\n",
    "                    demographic_metrics['gender'][gender]['status_quo']['tn'] += 1\n",
    "                    demographic_metrics['age_group'][age_group]['status_quo']['tn'] += 1\n",
    "                    demographic_metrics['race'][race]['status_quo']['tn'] += 1\n",
    "                else:\n",
    "                    demographic_metrics['gender'][gender]['status_quo']['fn'] += 1\n",
    "                    demographic_metrics['age_group'][age_group]['status_quo']['fn'] += 1\n",
    "                    demographic_metrics['race'][race]['status_quo']['fn'] += 1\n",
    "                    \n",
    "            # Update totals\n",
    "            demographic_metrics['gender'][gender]['sarsa']['total'] += 1\n",
    "            demographic_metrics['age_group'][age_group]['sarsa']['total'] += 1\n",
    "            demographic_metrics['race'][race]['sarsa']['total'] += 1\n",
    "            demographic_metrics['gender'][gender]['status_quo']['total'] += 1\n",
    "            demographic_metrics['age_group'][age_group]['status_quo']['total'] += 1\n",
    "            demographic_metrics['race'][race]['status_quo']['total'] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing patient {patient_idx}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate fairness metrics\n",
    "    fairness_results = {}\n",
    "    for demo_type, demo_groups in demographic_metrics.items():\n",
    "        fairness_results[demo_type] = {}\n",
    "        print(f\"\\n--- {demo_type.title()} Fairness Analysis ---\")\n",
    "        print(f\"{'Group':<15} {'SARSA TPR':<10} {'SARSA FPR':<10} {'Status TPR':<10} {'Status FPR':<10} {'Total':<8}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        group_metrics = []\n",
    "        for group, metrics in demo_groups.items():\n",
    "            # Skip groups with too few samples\n",
    "            if metrics['sarsa']['total'] < 5:\n",
    "                continue\n",
    "                \n",
    "            # Calculate true positive rate (TPR) and false positive rate (FPR)\n",
    "            # For SARSA\n",
    "            sarsa_tp = metrics['sarsa']['tp']\n",
    "            sarsa_fp = metrics['sarsa']['fp']\n",
    "            sarsa_tn = metrics['sarsa']['tn']\n",
    "            sarsa_fn = metrics['sarsa']['fn']\n",
    "            \n",
    "            # Handle case where denominators are zero\n",
    "            sarsa_tpr = sarsa_tp / max(1, (sarsa_tp + sarsa_fn))\n",
    "            sarsa_fpr = sarsa_fp / max(1, (sarsa_fp + sarsa_tn))\n",
    "            \n",
    "            # For status quo\n",
    "            status_tp = metrics['status_quo']['tp']\n",
    "            status_fp = metrics['status_quo']['fp']\n",
    "            status_tn = metrics['status_quo']['tn']\n",
    "            status_fn = metrics['status_quo']['fn']\n",
    "            \n",
    "            # Handle case where denominators are zero\n",
    "            status_tpr = status_tp / max(1, (status_tp + status_fn))\n",
    "            status_fpr = status_fp / max(1, (status_fp + status_tn))\n",
    "            \n",
    "            total = metrics['sarsa']['total']\n",
    "            \n",
    "            group_metrics.append({\n",
    "                'group': group,\n",
    "                'sarsa_tpr': sarsa_tpr,\n",
    "                'sarsa_fpr': sarsa_fpr,\n",
    "                'status_tpr': status_tpr,\n",
    "                'status_fpr': status_fpr,\n",
    "                'total': total\n",
    "            })\n",
    "            \n",
    "            print(f\"{group:<15} {sarsa_tpr:.4f}     {sarsa_fpr:.4f}     {status_tpr:.4f}     {status_fpr:.4f}     {total:<8}\")\n",
    "            \n",
    "            fairness_results[demo_type][group] = {\n",
    "                'sarsa_tpr': sarsa_tpr,\n",
    "                'sarsa_fpr': sarsa_fpr,\n",
    "                'status_tpr': status_tpr,\n",
    "                'status_fpr': status_fpr,\n",
    "                'sarsa_confusion_matrix': {\n",
    "                    'tp': sarsa_tp, 'fp': sarsa_fp, 'tn': sarsa_tn, 'fn': sarsa_fn\n",
    "                },\n",
    "                'status_quo_confusion_matrix': {\n",
    "                    'tp': status_tp, 'fp': status_fp, 'tn': status_tn, 'fn': status_fn\n",
    "                },\n",
    "                'total': total\n",
    "            }\n",
    "            \n",
    "        # Create TPR and FPR plots\n",
    "        if len(group_metrics) > 1:\n",
    "            plot_fairness_metrics(group_metrics, demo_type, log_dir)\n",
    "    \n",
    "    # Calculate equalized odds discrepancy\n",
    "    calculate_equalized_odds_discrepancy(fairness_results, log_dir)\n",
    "    \n",
    "    return fairness_results\n",
    "\n",
    "def simulate_trajectory(env, agent, use_sarsa=True):\n",
    "    \"\"\"Simulate a trajectory using either SARSA or status quo policy\"\"\"\n",
    "    state = env.reset()\n",
    "    trajectory = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        try:\n",
    "            # Get action mask\n",
    "            action_mask = env.generate_action_mask(state)\n",
    "            \n",
    "            # Select action based on policy\n",
    "            if use_sarsa:\n",
    "                # Handle tensor dimensionality issues\n",
    "                try:\n",
    "                    state_tensor = state.to_tensor(env.state_cache)\n",
    "                    \n",
    "                    # Check if we got a 1D tensor instead of 2D\n",
    "                    if state_tensor.dim() == 1:\n",
    "                        # Add batch dimension\n",
    "                        state_tensor = state_tensor.unsqueeze(0)\n",
    "                        \n",
    "                    # Get action using SARSA agent\n",
    "                    action, _ = agent.select_action(state_tensor, action_mask, training=False)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error selecting SARSA action: {e}\")\n",
    "                    # Fallback: use first valid action\n",
    "                    valid_indices = torch.nonzero(action_mask).squeeze()\n",
    "                    if valid_indices.dim() == 0:\n",
    "                        action = valid_indices\n",
    "                    else:\n",
    "                        action = valid_indices[0]\n",
    "            else:\n",
    "                # Status quo - use either function or method\n",
    "                if callable(agent):\n",
    "                    action = agent(state, action_mask)\n",
    "                else:\n",
    "                    # Try to find _get_status_quo_action method\n",
    "                    try:\n",
    "                        if hasattr(agent, '_get_status_quo_action'):\n",
    "                            action = agent._get_status_quo_action(state, action_mask)\n",
    "                        else:\n",
    "                            # Default: use first valid action\n",
    "                            valid_indices = torch.nonzero(action_mask).squeeze()\n",
    "                            if valid_indices.dim() == 0:\n",
    "                                action = valid_indices\n",
    "                            else:\n",
    "                                action = valid_indices[0]\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error getting status quo action: {e}\")\n",
    "                        # Default: use first valid action\n",
    "                        valid_indices = torch.nonzero(action_mask).squeeze()\n",
    "                        if valid_indices.dim() == 0:\n",
    "                            action = valid_indices\n",
    "                        else:\n",
    "                            action = valid_indices[0]\n",
    "            \n",
    "            # Take step\n",
    "            action_item = action.item() if hasattr(action, 'item') else action\n",
    "            next_state, reward, done, info = env.step(state, action_item)\n",
    "            \n",
    "            # Store step details\n",
    "            trajectory.append({\n",
    "                'action': action_item,\n",
    "                'intervention': info.get('intervention', 'UNKNOWN'),\n",
    "                'reward': reward,\n",
    "                'is_acute': info.get('is_acute', False),\n",
    "                'risk': info.get('post_risk', 0.5),\n",
    "                'risk_reduction': info.get('risk_reduction', 0),\n",
    "                'safety_violation': info.get('safety_violation', False)\n",
    "            })\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in trajectory simulation: {e}\")\n",
    "            # Break the loop if there's an error to avoid infinite loops\n",
    "            break\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "def plot_fairness_metrics(group_metrics, demo_type, log_dir):\n",
    "    \"\"\"Generate and save plots for TPR and FPR comparisons across groups\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "    \n",
    "    # Sort by group name for consistent ordering\n",
    "    sorted_metrics = sorted(group_metrics, key=lambda x: x['group'])\n",
    "    \n",
    "    groups = [m['group'] for m in sorted_metrics]\n",
    "    sarsa_tpr = [m['sarsa_tpr'] for m in sorted_metrics]\n",
    "    sarsa_fpr = [m['sarsa_fpr'] for m in sorted_metrics]\n",
    "    status_tpr = [m['status_tpr'] for m in sorted_metrics]\n",
    "    status_fpr = [m['status_fpr'] for m in sorted_metrics]\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Plot TPR comparison\n",
    "    x = range(len(groups))\n",
    "    width = 0.35\n",
    "    ax1.bar([i - width/2 for i in x], sarsa_tpr, width, label='SARSA', color='#2171b5')\n",
    "    ax1.bar([i + width/2 for i in x], status_tpr, width, label='Status Quo', color='#cb181d')\n",
    "    \n",
    "    ax1.set_xlabel(f'{demo_type.title()} Group')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title(f'TPR by {demo_type.title()} Group')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(groups, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot FPR comparison\n",
    "    ax2.bar([i - width/2 for i in x], sarsa_fpr, width, label='SARSA', color='#2171b5')\n",
    "    ax2.bar([i + width/2 for i in x], status_fpr, width, label='Status Quo', color='#cb181d')\n",
    "    \n",
    "    ax2.set_xlabel(f'{demo_type.title()} Group')\n",
    "    ax2.set_ylabel('False Positive Rate')\n",
    "    ax2.set_title(f'FPR by {demo_type.title()} Group')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(groups, rotation=45, ha='right')\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add equalized odds interpretation\n",
    "    fig.suptitle(f'Equalized Odds Analysis for {demo_type.title()}', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(os.path.join(log_dir, f'fairness_{demo_type}.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def calculate_equalized_odds_discrepancy(fairness_results, log_dir):\n",
    "    \"\"\"Calculate and visualize equalized odds discrepancy measures\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import json\n",
    "    \n",
    "    print(\"\\n--- Equalized Odds Discrepancy Analysis ---\")\n",
    "    \n",
    "    # Calculate equalized odds discrepancy for each demographic type\n",
    "    discrepancy_results = {}\n",
    "    \n",
    "    for demo_type, groups in fairness_results.items():\n",
    "        # Skip if we don't have enough groups\n",
    "        if len(groups) < 2:\n",
    "            continue\n",
    "            \n",
    "        # Calculate max TPR and FPR difference across groups for each model\n",
    "        sarsa_tpr_values = [g['sarsa_tpr'] for g in groups.values() if g['total'] >= 10]\n",
    "        sarsa_fpr_values = [g['sarsa_fpr'] for g in groups.values() if g['total'] >= 10]\n",
    "        status_tpr_values = [g['status_tpr'] for g in groups.values() if g['total'] >= 10]\n",
    "        status_fpr_values = [g['status_fpr'] for g in groups.values() if g['total'] >= 10]\n",
    "        \n",
    "        # Skip if we don't have enough valid groups\n",
    "        if len(sarsa_tpr_values) < 2:\n",
    "            continue\n",
    "            \n",
    "        sarsa_tpr_discrepancy = max(sarsa_tpr_values) - min(sarsa_tpr_values)\n",
    "        sarsa_fpr_discrepancy = max(sarsa_fpr_values) - min(sarsa_fpr_values)\n",
    "        status_tpr_discrepancy = max(status_tpr_values) - min(status_tpr_values)\n",
    "        status_fpr_discrepancy = max(status_fpr_values) - min(status_fpr_values)\n",
    "        \n",
    "        # Calculate overall equalized odds discrepancy (max of TPR and FPR discrepancy)\n",
    "        sarsa_eod = max(sarsa_tpr_discrepancy, sarsa_fpr_discrepancy)\n",
    "        status_eod = max(status_tpr_discrepancy, status_fpr_discrepancy)\n",
    "        \n",
    "        discrepancy_results[demo_type] = {\n",
    "            'sarsa_tpr_discrepancy': sarsa_tpr_discrepancy,\n",
    "            'sarsa_fpr_discrepancy': sarsa_fpr_discrepancy,\n",
    "            'status_tpr_discrepancy': status_tpr_discrepancy,\n",
    "            'status_fpr_discrepancy': status_fpr_discrepancy,\n",
    "            'sarsa_eod': sarsa_eod,\n",
    "            'status_eod': status_eod\n",
    "        }\n",
    "        \n",
    "        print(f\"{demo_type.title()}:\")\n",
    "        print(f\"  SARSA EOD: {sarsa_eod:.4f} (TPR : {sarsa_tpr_discrepancy:.4f}, FPR : {sarsa_fpr_discrepancy:.4f})\")\n",
    "        print(f\"  Status Quo EOD: {status_eod:.4f} (TPR : {status_tpr_discrepancy:.4f}, FPR : {status_fpr_discrepancy:.4f})\")\n",
    "        \n",
    "        # Differential improvement\n",
    "        eod_improvement = status_eod - sarsa_eod\n",
    "        print(f\"  Improvement: {eod_improvement:.4f} ({'better' if eod_improvement > 0 else 'worse'})\")\n",
    "    \n",
    "    # Create a summary plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    demo_types = list(discrepancy_results.keys())\n",
    "    sarsa_eod = [discrepancy_results[d]['sarsa_eod'] for d in demo_types]\n",
    "    status_eod = [discrepancy_results[d]['status_eod'] for d in demo_types]\n",
    "    \n",
    "    x = range(len(demo_types))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar([i - width/2 for i in x], sarsa_eod, width, label='SARSA', color='#2171b5')\n",
    "    ax.bar([i + width/2 for i in x], status_eod, width, label='Status Quo', color='#cb181d')\n",
    "    \n",
    "    ax.set_xlabel('Demographic Characteristic')\n",
    "    ax.set_ylabel('Equalized Odds Discrepancy')\n",
    "    ax.set_title('Equalized Odds Discrepancy by Demographic Characteristic')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([d.title() for d in demo_types])\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add threshold for interpretability\n",
    "    ax.axhline(y=0.2, color='green', linestyle='--', alpha=0.6)\n",
    "    ax.text(len(demo_types)-1, 0.22, 'Fair (0.2)', color='green', ha='right')\n",
    "    \n",
    "    ax.axhline(y=0.4, color='orange', linestyle='--', alpha=0.6)\n",
    "    ax.text(len(demo_types)-1, 0.42, 'Concerning (0.4)', color='orange', ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(log_dir, 'equalized_odds_discrepancy.png'), dpi=300)\n",
    "    plt.savefig(os.path.join(log_dir, 'equalized_odds_discrepancy.pdf'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Save discrepancy results\n",
    "    with open(os.path.join(log_dir, 'equalized_odds_results.json'), 'w') as f:\n",
    "        json.dump(discrepancy_results, f, indent=2)\n",
    "    \n",
    "    return discrepancy_results\n",
    "\n",
    "# Function to create a status quo decision function\n",
    "def create_status_quo_function(agent):\n",
    "    \"\"\"Create a status quo function that can be called with the same interface as the agent\"\"\"\n",
    "    \n",
    "    def status_quo_action(state, action_mask):\n",
    "        \"\"\"Rule-based status quo action selection\"\"\"\n",
    "        if hasattr(agent, '_get_status_quo_action'):\n",
    "            return agent._get_status_quo_action(state, action_mask)\n",
    "        else:\n",
    "            # Implement basic status quo logic if not available\n",
    "            # This is a simplified version - the actual implementation is more complex\n",
    "            risk_score = state.features.get('riskScore', 0.5)\n",
    "            \n",
    "            # Default action: chronic condition management\n",
    "            default_action = INTERVENTIONS.get('CHRONIC_CONDITION_MANAGEMENT', 2)\n",
    "            \n",
    "            # For high risk, prioritize medical interventions\n",
    "            if risk_score > 0.7:\n",
    "                priorities = [\n",
    "                    INTERVENTIONS.get('CHRONIC_CONDITION_MANAGEMENT', 2),\n",
    "                    INTERVENTIONS.get('MENTAL_HEALTH_SUPPORT', 1),\n",
    "                    INTERVENTIONS.get('SUBSTANCE_USE_SUPPORT', 0),\n",
    "                ]\n",
    "            # For medium risk, balanced approach\n",
    "            elif risk_score > 0.3:\n",
    "                priorities = [\n",
    "                    INTERVENTIONS.get('CHRONIC_CONDITION_MANAGEMENT', 2),\n",
    "                    INTERVENTIONS.get('HOUSING_ASSISTANCE', 4),\n",
    "                    INTERVENTIONS.get('FOOD_ASSISTANCE', 3),\n",
    "                ]\n",
    "            # For low risk, less intensive interventions\n",
    "            else:\n",
    "                priorities = [\n",
    "                    INTERVENTIONS.get('WATCHFUL_WAITING', 8),\n",
    "                    INTERVENTIONS.get('TRANSPORTATION_ASSISTANCE', 5),\n",
    "                    INTERVENTIONS.get('FOOD_ASSISTANCE', 3),\n",
    "                ]\n",
    "                \n",
    "            # Find first valid action in priority list\n",
    "            for action in priorities:\n",
    "                if action_mask[action]:\n",
    "                    return torch.tensor(action, device=DEVICE)\n",
    "                    \n",
    "            # Fall back to first valid action\n",
    "            valid_actions = torch.nonzero(action_mask).squeeze(1)\n",
    "            if len(valid_actions) > 0:\n",
    "                return valid_actions[0]\n",
    "            else:\n",
    "                return torch.tensor(default_action, device=DEVICE)\n",
    "    \n",
    "    return status_quo_action\n",
    "\n",
    "# Main execution function for fairness analysis\n",
    "def run_fairness_analysis(agent, val_data, results_dir=None):\n",
    "    \"\"\"Run fairness analysis on validation data\"\"\"\n",
    "    if results_dir is None:\n",
    "        results_dir = \"fairness_analysis_results\"\n",
    "        \n",
    "    # Create output directory\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Create status quo function\n",
    "    status_quo_function = create_status_quo_function(agent)\n",
    "    \n",
    "    # Run fairness analysis\n",
    "    fairness_results = analyze_fairness_by_demographics(\n",
    "        agent, \n",
    "        status_quo_function,\n",
    "        val_data,\n",
    "        results_dir\n",
    "    )\n",
    "    \n",
    "    # Save complete results\n",
    "    with open(os.path.join(results_dir, 'fairness_analysis_complete.json'), 'w') as f:\n",
    "        # Convert defaultdicts to regular dicts\n",
    "        serializable_results = {}\n",
    "        for demo_type, groups in fairness_results.items():\n",
    "            serializable_results[demo_type] = dict(groups)\n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nFairness analysis complete. Results saved to {results_dir}\")\n",
    "    return fairness_results\n",
    "\n",
    "# This section will be added to the run_fairness_evaluation function instead of standalone execution\n",
    "    # Additional summary printed based on results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistical_significance(fairness_results):\n",
    "    \"\"\"Calculate statistical significance of differences between demographic groups\"\"\"\n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "    \n",
    "    significance_results = {}\n",
    "    \n",
    "    for demo_type, groups in fairness_results.items():\n",
    "        if demo_type == 'overall' or len(groups) < 2:\n",
    "            continue\n",
    "            \n",
    "        significance_results[demo_type] = {\n",
    "            'sarsa': {'chi_square_p': None, 'significant_pairs': []},\n",
    "            'status_quo': {'chi_square_p': None, 'significant_pairs': []}\n",
    "        }\n",
    "        \n",
    "        # Prepare data for chi-square test\n",
    "        sarsa_counts = []\n",
    "        status_quo_counts = []\n",
    "        sample_sizes = []\n",
    "        group_names = []\n",
    "        \n",
    "        for group, metrics in groups.items():\n",
    "            group_names.append(group)\n",
    "            sample_size = metrics['total']\n",
    "            sample_sizes.append(sample_size)\n",
    "            \n",
    "            # Convert rates to counts\n",
    "            sarsa_acute_count = int(metrics['sarsa_tpr'] * sample_size)\n",
    "            sarsa_counts.append([sarsa_acute_count, sample_size - sarsa_acute_count])\n",
    "            \n",
    "            status_quo_acute_count = int(metrics['status_tpr'] * sample_size)\n",
    "            status_quo_counts.append([status_quo_acute_count, sample_size - status_quo_acute_count])\n",
    "        \n",
    "        # Chi-square test for overall significance\n",
    "        try:\n",
    "            sarsa_chi2, sarsa_p = stats.chi2_contingency(sarsa_counts)[0:2]\n",
    "            status_chi2, status_p = stats.chi2_contingency(status_quo_counts)[0:2]\n",
    "            \n",
    "            significance_results[demo_type]['sarsa']['chi_square_p'] = sarsa_p\n",
    "            significance_results[demo_type]['status_quo']['chi_square_p'] = status_p\n",
    "        except:\n",
    "            # Handle potential errors in chi-square calculation\n",
    "            significance_results[demo_type]['sarsa']['chi_square_p'] = None\n",
    "            significance_results[demo_type]['status_quo']['chi_square_p'] = None\n",
    "        \n",
    "        # Pairwise z-tests for specific group comparisons\n",
    "        for i in range(len(group_names)):\n",
    "            for j in range(i+1, len(group_names)):\n",
    "                # SARSA comparison\n",
    "                p1 = sarsa_counts[i][0] / sample_sizes[i]\n",
    "                p2 = sarsa_counts[j][0] / sample_sizes[j]\n",
    "                \n",
    "                z_score, p_value = proportion_z_test(p1, p2, sample_sizes[i], sample_sizes[j])\n",
    "                \n",
    "                if p_value < 0.05:\n",
    "                    significance_results[demo_type]['sarsa']['significant_pairs'].append({\n",
    "                        'group1': group_names[i],\n",
    "                        'group2': group_names[j],\n",
    "                        'p_value': p_value\n",
    "                    })\n",
    "                \n",
    "                # Status quo comparison\n",
    "                p1 = status_quo_counts[i][0] / sample_sizes[i]\n",
    "                p2 = status_quo_counts[j][0] / sample_sizes[j]\n",
    "                \n",
    "                z_score, p_value = proportion_z_test(p1, p2, sample_sizes[i], sample_sizes[j])\n",
    "                \n",
    "                if p_value < 0.05:\n",
    "                    significance_results[demo_type]['status_quo']['significant_pairs'].append({\n",
    "                        'group1': group_names[i],\n",
    "                        'group2': group_names[j],\n",
    "                        'p_value': p_value\n",
    "                    })\n",
    "    \n",
    "    return significance_results\n",
    "\n",
    "def proportion_z_test(p1, p2, n1, n2):\n",
    "    \"\"\"Perform z-test for two proportions\"\"\"\n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "    \n",
    "    # Pooled proportion\n",
    "    p_pooled = (p1 * n1 + p2 * n2) / (n1 + n2)\n",
    "    \n",
    "    # Standard error\n",
    "    se = np.sqrt(p_pooled * (1 - p_pooled) * (1/n1 + 1/n2))\n",
    "    \n",
    "    # Z-score\n",
    "    if se == 0:  # Handle zero standard error\n",
    "        return 0, 1.0\n",
    "        \n",
    "    z = (p1 - p2) / se\n",
    "    \n",
    "    # Two-tailed p-value\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(z)))\n",
    "    \n",
    "    return z, p_value\n",
    "\n",
    "def calculate_confidence_intervals(fairness_results):\n",
    "    \"\"\"Calculate 95% confidence intervals for rates by demographic group\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    ci_results = {}\n",
    "    \n",
    "    for demo_type, groups in fairness_results.items():\n",
    "        ci_results[demo_type] = {}\n",
    "        \n",
    "        for group, metrics in groups.items():\n",
    "            # Calculate confidence intervals for SARSA\n",
    "            sarsa_rate = metrics['sarsa_tpr']\n",
    "            n = metrics['total']\n",
    "            sarsa_ci = wilson_score_interval(sarsa_rate, n)\n",
    "            \n",
    "            # Calculate confidence intervals for Status Quo\n",
    "            status_rate = metrics['status_tpr']\n",
    "            status_ci = wilson_score_interval(status_rate, n)\n",
    "            \n",
    "            ci_results[demo_type][group] = {\n",
    "                'sarsa': {\n",
    "                    'rate': sarsa_rate,\n",
    "                    'ci_lower': sarsa_ci[0],\n",
    "                    'ci_upper': sarsa_ci[1],\n",
    "                },\n",
    "                'status_quo': {\n",
    "                    'rate': status_rate,\n",
    "                    'ci_lower': status_ci[0],\n",
    "                    'ci_upper': status_ci[1],\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    return ci_results\n",
    "\n",
    "def wilson_score_interval(p, n, z=1.96):\n",
    "    \"\"\"\n",
    "    Calculate Wilson score interval for a proportion\n",
    "    \n",
    "    Args:\n",
    "        p: Proportion\n",
    "        n: Sample size\n",
    "        z: z-score (1.96 for 95% confidence)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (lower_bound, upper_bound)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if n == 0:\n",
    "        return (0, 1)\n",
    "    \n",
    "    # Wilson score interval formula\n",
    "    denominator = 1 + z**2/n\n",
    "    center = (p + z**2/(2*n))/denominator\n",
    "    pm = z/denominator * np.sqrt(p*(1-p)/n + z**2/(4*n**2))\n",
    "    \n",
    "    return (max(0, center - pm), min(1, center + pm))\n",
    "\n",
    "def print_significance_results(significance_results, ci_results):\n",
    "    \"\"\"Print statistical significance results in a readable format\"\"\"\n",
    "    print(\"\\n=== Statistical Significance Analysis ===\")\n",
    "    \n",
    "    for demo_type, results in significance_results.items():\n",
    "        print(f\"\\n{demo_type.title()}\")\n",
    "        \n",
    "        # Chi-square test results\n",
    "        sarsa_p = results['sarsa']['chi_square_p']\n",
    "        status_p = results['status_quo']['chi_square_p']\n",
    "        \n",
    "        if sarsa_p is not None:\n",
    "            print(f\"  SARSA Chi-square p-value: {sarsa_p:.4f} \" + \n",
    "                  f\"({'Significant' if sarsa_p < 0.05 else 'Not significant'} at =0.05)\")\n",
    "            \n",
    "        if status_p is not None:\n",
    "            print(f\"  Status Quo Chi-square p-value: {status_p:.4f} \" + \n",
    "                  f\"({'Significant' if status_p < 0.05 else 'Not significant'} at =0.05)\")\n",
    "        \n",
    "        # Significant pairwise comparisons\n",
    "        sarsa_pairs = results['sarsa']['significant_pairs']\n",
    "        status_pairs = results['status_quo']['significant_pairs']\n",
    "        \n",
    "        if sarsa_pairs:\n",
    "            print(\"\\n  SARSA significant group differences:\")\n",
    "            for pair in sarsa_pairs:\n",
    "                print(f\"    {pair['group1']} vs {pair['group2']}: p={pair['p_value']:.4f}\")\n",
    "        else:\n",
    "            print(\"\\n  SARSA: No significant pairwise differences\")\n",
    "            \n",
    "        if status_pairs:\n",
    "            print(\"\\n  Status Quo significant group differences:\")\n",
    "            for pair in status_pairs:\n",
    "                print(f\"    {pair['group1']} vs {pair['group2']}: p={pair['p_value']:.4f}\")\n",
    "        else:\n",
    "            print(\"\\n  Status Quo: No significant pairwise differences\")\n",
    "        \n",
    "        # Print confidence intervals\n",
    "        if demo_type in ci_results:\n",
    "            print(\"\\n  Rates with 95% Confidence Intervals:\")\n",
    "            group_ci = ci_results[demo_type]\n",
    "            \n",
    "            print(f\"  {'Group':<15} {'SARSA Rate':<25} {'Status Quo Rate':<25}\")\n",
    "            print(\"  \" + \"-\" * 65)\n",
    "            \n",
    "            for group, ci in group_ci.items():\n",
    "                sarsa_ci = f\"{ci['sarsa']['rate']:.4f} ({ci['sarsa']['ci_lower']:.4f}-{ci['sarsa']['ci_upper']:.4f})\"\n",
    "                status_ci = f\"{ci['status_quo']['rate']:.4f} ({ci['status_quo']['ci_lower']:.4f}-{ci['status_quo']['ci_upper']:.4f})\"\n",
    "                print(f\"  {group:<15} {sarsa_ci:<25} {status_ci:<25}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "def calculate_model_metrics(y_true, y_pred_proba, y_pred_status_quo_proba):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive model metrics including AUC, calibration, and NNH.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (array): Binary outcome labels (1=acute event, 0=no acute event)\n",
    "    y_pred_proba (array): SARSA model predicted probabilities\n",
    "    y_pred_status_quo_proba (array): Status quo model predicted probabilities\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing all calculated metrics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Calculate AUC-ROC and 95% CI using bootstrap\n",
    "    n_bootstraps = 1000\n",
    "    rng = np.random.RandomState(42)\n",
    "    \n",
    "    # SARSA AUC\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    # Bootstrap 95% CI for AUC\n",
    "    bootstrapped_aucs = []\n",
    "    for i in range(n_bootstraps):\n",
    "        indices = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            # Skip this bootstrap if all class labels are the same\n",
    "            continue\n",
    "        auc_boot = roc_auc_score(y_true[indices], y_pred_proba[indices])\n",
    "        bootstrapped_aucs.append(auc_boot)\n",
    "    \n",
    "    auc_ci_lower = np.percentile(bootstrapped_aucs, 2.5)\n",
    "    auc_ci_upper = np.percentile(bootstrapped_aucs, 97.5)\n",
    "    \n",
    "    results['sarsa_auc'] = auc\n",
    "    results['sarsa_auc_ci_lower'] = auc_ci_lower\n",
    "    results['sarsa_auc_ci_upper'] = auc_ci_upper\n",
    "    \n",
    "    # Status quo AUC\n",
    "    status_quo_auc = roc_auc_score(y_true, y_pred_status_quo_proba)\n",
    "    results['status_quo_auc'] = status_quo_auc\n",
    "    \n",
    "    # DeLong's test for comparing AUCs\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Function for DeLong's test\n",
    "    def delong_test(y_true, y_pred1, y_pred2):\n",
    "        # Adapted from https://github.com/yandexdataschool/roc_comparison\n",
    "        # Calculate AUC and variance\n",
    "        n = len(y_true)\n",
    "        pos_count = np.sum(y_true == 1)\n",
    "        neg_count = n - pos_count\n",
    "        \n",
    "        # Rank predictions\n",
    "        ranked_pred1 = stats.rankdata(y_pred1)\n",
    "        ranked_pred2 = stats.rankdata(y_pred2)\n",
    "        \n",
    "        # Separate ranks for positive and negative cases\n",
    "        pos_ranks1 = ranked_pred1[y_true == 1]\n",
    "        neg_ranks1 = ranked_pred1[y_true == 0]\n",
    "        pos_ranks2 = ranked_pred2[y_true == 1]\n",
    "        neg_ranks2 = ranked_pred2[y_true == 0]\n",
    "        \n",
    "        # Calculate AUC\n",
    "        auc1 = (np.sum(pos_ranks1) - pos_count*(pos_count+1)/2) / (pos_count*neg_count)\n",
    "        auc2 = (np.sum(pos_ranks2) - pos_count*(pos_count+1)/2) / (pos_count*neg_count)\n",
    "        \n",
    "        # Calculate variance and covariance\n",
    "        var_auc1 = calculate_auc_variance(pos_ranks1, neg_ranks1, pos_count, neg_count)\n",
    "        var_auc2 = calculate_auc_variance(pos_ranks2, neg_ranks2, pos_count, neg_count)\n",
    "        cov_auc = calculate_auc_covariance(pos_ranks1, neg_ranks1, pos_ranks2, neg_ranks2, pos_count, neg_count)\n",
    "        \n",
    "        # Calculate z-score and p-value\n",
    "        z = (auc1 - auc2) / np.sqrt(var_auc1 + var_auc2 - 2*cov_auc)\n",
    "        p = 2 * (1 - stats.norm.cdf(abs(z)))\n",
    "        \n",
    "        return z, p\n",
    "    \n",
    "    # Helper functions for DeLong test\n",
    "    def calculate_auc_variance(pos_ranks, neg_ranks, pos_count, neg_count):\n",
    "        pos_cdf = np.searchsorted(np.sort(pos_ranks), neg_ranks, side='right') / pos_count\n",
    "        neg_cdf = np.searchsorted(np.sort(neg_ranks), pos_ranks, side='right') / neg_count\n",
    "        return (np.sum(pos_cdf * (1-pos_cdf)) / (pos_count-1) + \n",
    "                np.sum(neg_cdf * (1-neg_cdf)) / (neg_count-1)) / (pos_count * neg_count)\n",
    "    \n",
    "    def calculate_auc_covariance(pos_ranks1, neg_ranks1, pos_ranks2, neg_ranks2, pos_count, neg_count):\n",
    "        # This is an approximation for the covariance\n",
    "        pos_cdf1 = np.searchsorted(np.sort(pos_ranks1), neg_ranks1, side='right') / pos_count\n",
    "        pos_cdf2 = np.searchsorted(np.sort(pos_ranks2), neg_ranks1, side='right') / pos_count\n",
    "        neg_cdf1 = np.searchsorted(np.sort(neg_ranks1), pos_ranks1, side='right') / neg_count\n",
    "        neg_cdf2 = np.searchsorted(np.sort(neg_ranks2), pos_ranks1, side='right') / neg_count\n",
    "        \n",
    "        cov_pos = np.sum((pos_cdf1 - np.mean(pos_cdf1)) * (pos_cdf2 - np.mean(pos_cdf2))) / (pos_count-1)\n",
    "        cov_neg = np.sum((neg_cdf1 - np.mean(neg_cdf1)) * (neg_cdf2 - np.mean(neg_cdf2))) / (neg_count-1)\n",
    "        \n",
    "        return (cov_pos + cov_neg) / (pos_count * neg_count)\n",
    "    \n",
    "    # Perform DeLong's test\n",
    "    z, p_value = delong_test(y_true, y_pred_proba, y_pred_status_quo_proba)\n",
    "    results['auc_comparison_p_value'] = p_value\n",
    "    \n",
    "    # 2. Calibration metrics\n",
    "    # Expected Calibration Error (ECE)\n",
    "    def calculate_ece(y_true, y_pred, n_bins=10):\n",
    "        \"\"\"Calculate expected calibration error\"\"\"\n",
    "        bins = np.linspace(0, 1, n_bins + 1)\n",
    "        binids = np.digitize(y_pred, bins) - 1\n",
    "        \n",
    "        bin_sums = np.bincount(binids, weights=y_pred, minlength=n_bins)\n",
    "        bin_true = np.bincount(binids, weights=y_true, minlength=n_bins)\n",
    "        bin_counts = np.bincount(binids, minlength=n_bins)\n",
    "        \n",
    "        nonzero = bin_counts != 0\n",
    "        prob_true = bin_true[nonzero] / bin_counts[nonzero]\n",
    "        prob_pred = bin_sums[nonzero] / bin_counts[nonzero]\n",
    "        \n",
    "        # ECE is a weighted average of |accuracy - confidence|\n",
    "        ece = np.sum(np.abs(prob_true - prob_pred) * (bin_counts[nonzero] / len(y_true)))\n",
    "        return ece\n",
    "    \n",
    "    # Calculate ECE for both models\n",
    "    sarsa_ece = calculate_ece(y_true, y_pred_proba)\n",
    "    status_quo_ece = calculate_ece(y_true, y_pred_status_quo_proba)\n",
    "    \n",
    "    results['sarsa_ece'] = sarsa_ece\n",
    "    results['status_quo_ece'] = status_quo_ece\n",
    "    \n",
    "    # Calculate Brier score for both models\n",
    "    sarsa_brier = brier_score_loss(y_true, y_pred_proba)\n",
    "    status_quo_brier = brier_score_loss(y_true, y_pred_status_quo_proba)\n",
    "    \n",
    "    results['sarsa_brier'] = sarsa_brier\n",
    "    results['status_quo_brier'] = status_quo_brier\n",
    "    \n",
    "    # 3. Calculate Number Needed to Harm (NNH)\n",
    "    # Convert probabilities to decisions using a threshold\n",
    "    threshold = 0.5  # Adjust as needed\n",
    "    sarsa_decisions = (y_pred_proba >= threshold).astype(int)\n",
    "    status_quo_decisions = (y_pred_status_quo_proba >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate differences in decisions\n",
    "    diff_decisions = sarsa_decisions != status_quo_decisions\n",
    "    \n",
    "    # Among cases where decisions differ, calculate harm rates\n",
    "    if np.sum(diff_decisions) > 0:\n",
    "        sarsa_harm_rate = np.mean(y_true[diff_decisions & (sarsa_decisions == 1)])\n",
    "        status_quo_harm_rate = np.mean(y_true[diff_decisions & (status_quo_decisions == 1)])\n",
    "        \n",
    "        harm_difference = sarsa_harm_rate - status_quo_harm_rate\n",
    "        \n",
    "        # NNH is 1/absolute risk increase (if harm_difference is positive)\n",
    "        if harm_difference > 0:\n",
    "            nnh = 1 / harm_difference\n",
    "            results['nnh'] = nnh\n",
    "            \n",
    "            # Calculate 95% CI for NNH using bootstrap\n",
    "            nnh_bootstrapped = []\n",
    "            for i in range(n_bootstraps):\n",
    "                indices = rng.randint(0, len(y_true), len(y_true))\n",
    "                diff_indices = diff_decisions[indices]\n",
    "                \n",
    "                if np.sum(diff_indices) > 0:\n",
    "                    sarsa_harm = np.mean(y_true[indices][diff_indices & (sarsa_decisions[indices] == 1)])\n",
    "                    status_harm = np.mean(y_true[indices][diff_indices & (status_quo_decisions[indices] == 1)])\n",
    "                    harm_diff = sarsa_harm - status_harm\n",
    "                    \n",
    "                    if harm_diff > 0:\n",
    "                        nnh_bootstrapped.append(1 / harm_diff)\n",
    "            \n",
    "            if nnh_bootstrapped:\n",
    "                results['nnh_ci_lower'] = np.percentile(nnh_bootstrapped, 2.5)\n",
    "                results['nnh_ci_upper'] = np.percentile(nnh_bootstrapped, 97.5)\n",
    "        else:\n",
    "            # No harm observed, NNH is undefined\n",
    "            results['nnh'] = float('inf')\n",
    "            results['nnh_ci_lower'] = float('inf')\n",
    "            results['nnh_ci_upper'] = float('inf')\n",
    "    else:\n",
    "        # No decision differences, NNH is undefined\n",
    "        results['nnh'] = float('inf')\n",
    "        results['nnh_ci_lower'] = float('inf')\n",
    "        results['nnh_ci_upper'] = float('inf')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# metrics = calculate_model_metrics(y_true, sarsa_predictions, status_quo_predictions)\n",
    "# print(f\"SARSA AUC: {metrics['sarsa_auc']:.3f} (95% CI: {metrics['sarsa_auc_ci_lower']:.3f}-{metrics['sarsa_auc_ci_upper']:.3f})\")\n",
    "# print(f\"Status Quo AUC: {metrics['status_quo_auc']:.3f}\")\n",
    "# print(f\"AUC Comparison p-value: {metrics['auc_comparison_p_value']:.4f}\")\n",
    "# print(f\"SARSA ECE: {metrics['sarsa_ece']:.3f}\")\n",
    "# print(f\"Status Quo ECE: {metrics['status_quo_ece']:.3f}\")\n",
    "# print(f\"NNH: {metrics['nnh']:.1f} (95% CI: {metrics['nnh_ci_lower']:.1f}-{metrics['nnh_ci_upper']:.1f})\")\n",
    "\n",
    "# Create calibration curve plot\n",
    "def plot_calibration_curves(y_true, sarsa_probs, status_quo_probs, filename=\"calibration_curves.png\"):\n",
    "    \"\"\"\n",
    "    Plot calibration curves for both models\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Compute calibration curves\n",
    "    sarsa_prob_true, sarsa_prob_pred = calibration_curve(y_true, sarsa_probs, n_bins=10)\n",
    "    sq_prob_true, sq_prob_pred = calibration_curve(y_true, status_quo_probs, n_bins=10)\n",
    "    \n",
    "    # Plot calibration curves\n",
    "    plt.plot(sarsa_prob_pred, sarsa_prob_true, \"s-\", color=\"#2171b5\", label=f\"SARSA (ECE={calculate_ece(y_true, sarsa_probs):.3f})\")\n",
    "    plt.plot(sq_prob_pred, sq_prob_true, \"s-\", color=\"#cb181d\", label=f\"Status Quo (ECE={calculate_ece(y_true, status_quo_probs):.3f})\")\n",
    "    \n",
    "    # Plot diagonal - perfect calibration\n",
    "    plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    \n",
    "    plt.xlabel(\"Mean predicted probability\")\n",
    "    plt.ylabel(\"Fraction of positives\")\n",
    "    plt.title(\"Calibration Curves\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "def calculate_model_metrics(y_true, y_pred_proba, y_pred_status_quo_proba):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive model metrics including AUC, calibration, and NNH.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (array): Binary outcome labels (1=acute event, 0=no acute event)\n",
    "    y_pred_proba (array): SARSA model predicted probabilities\n",
    "    y_pred_status_quo_proba (array): Status quo model predicted probabilities\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing all calculated metrics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Calculate AUC-ROC and 95% CI using bootstrap\n",
    "    n_bootstraps = 1000\n",
    "    rng = np.random.RandomState(42)\n",
    "    \n",
    "    # SARSA AUC\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    # Bootstrap 95% CI for AUC\n",
    "    bootstrapped_aucs = []\n",
    "    for i in range(n_bootstraps):\n",
    "        indices = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            # Skip this bootstrap if all class labels are the same\n",
    "            continue\n",
    "        auc_boot = roc_auc_score(y_true[indices], y_pred_proba[indices])\n",
    "        bootstrapped_aucs.append(auc_boot)\n",
    "    \n",
    "    auc_ci_lower = np.percentile(bootstrapped_aucs, 2.5)\n",
    "    auc_ci_upper = np.percentile(bootstrapped_aucs, 97.5)\n",
    "    \n",
    "    results['sarsa_auc'] = auc\n",
    "    results['sarsa_auc_ci_lower'] = auc_ci_lower\n",
    "    results['sarsa_auc_ci_upper'] = auc_ci_upper\n",
    "    \n",
    "    # Status quo AUC\n",
    "    status_quo_auc = roc_auc_score(y_true, y_pred_status_quo_proba)\n",
    "    results['status_quo_auc'] = status_quo_auc\n",
    "    \n",
    "    # Calculate expected calibration error (ECE)\n",
    "    def calculate_ece(y_true, y_pred, n_bins=10):\n",
    "        \"\"\"Calculate expected calibration error\"\"\"\n",
    "        bins = np.linspace(0, 1, n_bins + 1)\n",
    "        binids = np.digitize(y_pred, bins) - 1\n",
    "        \n",
    "        bin_sums = np.bincount(binids, weights=y_pred, minlength=n_bins)\n",
    "        bin_true = np.bincount(binids, weights=y_true, minlength=n_bins)\n",
    "        bin_counts = np.bincount(binids, minlength=n_bins)\n",
    "        \n",
    "        nonzero = bin_counts != 0\n",
    "        prob_true = bin_true[nonzero] / bin_counts[nonzero]\n",
    "        prob_pred = bin_sums[nonzero] / bin_counts[nonzero]\n",
    "        \n",
    "        ece = np.sum(np.abs(prob_true - prob_pred) * (bin_counts[nonzero] / len(y_true)))\n",
    "        return ece\n",
    "    \n",
    "    # Calculate ECE for both models\n",
    "    sarsa_ece = calculate_ece(y_true, y_pred_proba)\n",
    "    status_quo_ece = calculate_ece(y_true, y_pred_status_quo_proba)\n",
    "    \n",
    "    results['sarsa_ece'] = sarsa_ece\n",
    "    results['status_quo_ece'] = status_quo_ece\n",
    "    \n",
    "    # 3. Calculate Number Needed to Harm (NNH)\n",
    "    # Convert probabilities to decisions using a threshold\n",
    "    threshold = 0.5  # Adjust as needed\n",
    "    sarsa_decisions = (y_pred_proba >= threshold).astype(int)\n",
    "    status_quo_decisions = (y_pred_status_quo_proba >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate differences in decisions\n",
    "    diff_decisions = sarsa_decisions != status_quo_decisions\n",
    "    \n",
    "    # Among cases where decisions differ, calculate harm rates\n",
    "    if np.sum(diff_decisions) > 0:\n",
    "        sarsa_harm_rate = np.mean(y_true[diff_decisions & (sarsa_decisions == 1)])\n",
    "        status_quo_harm_rate = np.mean(y_true[diff_decisions & (status_quo_decisions == 1)])\n",
    "        \n",
    "        harm_difference = sarsa_harm_rate - status_quo_harm_rate\n",
    "        \n",
    "        # NNH is 1/absolute risk increase (if harm_difference is positive)\n",
    "        if harm_difference > 0:\n",
    "            nnh = 1 / harm_difference\n",
    "            results['nnh'] = nnh\n",
    "            \n",
    "            # Calculate 95% CI for NNH using bootstrap\n",
    "            nnh_bootstrapped = []\n",
    "            for i in range(n_bootstraps):\n",
    "                indices = rng.randint(0, len(y_true), len(y_true))\n",
    "                diff_indices = diff_decisions[indices]\n",
    "                \n",
    "                if np.sum(diff_indices) > 0:\n",
    "                    sarsa_harm = np.mean(y_true[indices][diff_indices & (sarsa_decisions[indices] == 1)])\n",
    "                    status_harm = np.mean(y_true[indices][diff_indices & (status_quo_decisions[indices] == 1)])\n",
    "                    harm_diff = sarsa_harm - status_harm\n",
    "                    \n",
    "                    if harm_diff > 0:\n",
    "                        nnh_bootstrapped.append(1 / harm_diff)\n",
    "            \n",
    "            if nnh_bootstrapped:\n",
    "                results['nnh_ci_lower'] = np.percentile(nnh_bootstrapped, 2.5)\n",
    "                results['nnh_ci_upper'] = np.percentile(nnh_bootstrapped, 97.5)\n",
    "        else:\n",
    "            # No harm observed, NNH is undefined\n",
    "            results['nnh'] = float('inf')\n",
    "            results['nnh_ci_lower'] = float('inf')\n",
    "            results['nnh_ci_upper'] = float('inf')\n",
    "    else:\n",
    "        # No decision differences, NNH is undefined\n",
    "        results['nnh'] = float('inf')\n",
    "        results['nnh_ci_lower'] = float('inf')\n",
    "        results['nnh_ci_upper'] = float('inf')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to create calibration curve plot\n",
    "def plot_calibration_curves(y_true, sarsa_probs, status_quo_probs, filename=\"calibration_curves.png\"):\n",
    "    \"\"\"\n",
    "    Plot calibration curves for both models\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Compute calibration curves\n",
    "    sarsa_prob_true, sarsa_prob_pred = calibration_curve(y_true, sarsa_probs, n_bins=10)\n",
    "    sq_prob_true, sq_prob_pred = calibration_curve(y_true, status_quo_probs, n_bins=10)\n",
    "    \n",
    "    # Plot calibration curves\n",
    "    plt.plot(sarsa_prob_pred, sarsa_prob_true, \"s-\", color=\"#2171b5\", \n",
    "             label=f\"SARSA (ECE={calculate_ece(y_true, sarsa_probs):.3f})\")\n",
    "    plt.plot(sq_prob_pred, sq_prob_true, \"s-\", color=\"#cb181d\", \n",
    "             label=f\"Status Quo (ECE={calculate_ece(y_true, status_quo_probs):.3f})\")\n",
    "    \n",
    "    # Plot diagonal - perfect calibration\n",
    "    plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    \n",
    "    plt.xlabel(\"Mean predicted probability\")\n",
    "    plt.ylabel(\"Fraction of positives\")\n",
    "    plt.title(\"Calibration Curves\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    return filename\n",
    "\n",
    "# Define calculate_ece function again for use in the plotting function\n",
    "def calculate_ece(y_true, y_pred, n_bins=10):\n",
    "    \"\"\"Calculate expected calibration error\"\"\"\n",
    "    bins = np.linspac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you only have binary decisions (no probabilities)\n",
    "# sarsa_decisions = np.array(...)  # 1 for intervention, 0 for no intervention\n",
    "# status_quo_decisions = np.array(...)  # 1 for intervention, 0 for no intervention\n",
    "# y_true = np.array(...)  # 1 for acute event, 0 for no event\n",
    "\n",
    "def calculate_nnh_binary(y_true, sarsa_decisions, status_quo_decisions):\n",
    "    # Calculate differences in decisions\n",
    "    diff_decisions = sarsa_decisions != status_quo_decisions\n",
    "    \n",
    "    # Among cases where decisions differ, calculate harm rates\n",
    "    if np.sum(diff_decisions) > 0:\n",
    "        sarsa_harm_rate = np.mean(y_true[diff_decisions & (sarsa_decisions == 1)])\n",
    "        status_quo_harm_rate = np.mean(y_true[diff_decisions & (status_quo_decisions == 1)])\n",
    "        \n",
    "        harm_difference = sarsa_harm_rate - status_quo_harm_rate\n",
    "        \n",
    "        # NNH is 1/absolute risk increase (if harm_difference is positive)\n",
    "        if harm_difference > 0:\n",
    "            return 1 / harm_difference\n",
    "        else:\n",
    "            return float('inf')  # No harm observed\n",
    "    else:\n",
    "        return float('inf')  # No decision differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, brier_score_loss, confusion_matrix\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "def calculate_evaluation_metrics(y_true, sarsa_predictions, status_quo_predictions):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive evaluation metrics for SARSA vs status quo approach.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (array): Binary outcome labels (1=acute event, 0=no acute event)\n",
    "    sarsa_predictions (array): Binary decisions or probability estimates from SARSA\n",
    "    status_quo_predictions (array): Binary decisions or probability estimates from status quo\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing all calculated metrics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Convert to numpy arrays if they're not already\n",
    "    y_true = np.array(y_true)\n",
    "    sarsa_predictions = np.array(sarsa_predictions)\n",
    "    status_quo_predictions = np.array(status_quo_predictions)\n",
    "    \n",
    "    # Check if predictions are probabilities or binary decisions\n",
    "    is_probability = (np.max(sarsa_predictions) <= 1.0 and np.max(status_quo_predictions) <= 1.0 and \n",
    "                      np.min(sarsa_predictions) >= 0.0 and np.min(status_quo_predictions) >= 0.0 and\n",
    "                      len(np.unique(sarsa_predictions)) > 2 and len(np.unique(status_quo_predictions)) > 2)\n",
    "    \n",
    "    # Convert to binary decisions if needed\n",
    "    if is_probability:\n",
    "        sarsa_decisions = (sarsa_predictions >= 0.5).astype(int)\n",
    "        status_quo_decisions = (status_quo_predictions >= 0.5).astype(int)\n",
    "    else:\n",
    "        # Assume they are already binary decisions\n",
    "        sarsa_decisions = sarsa_predictions\n",
    "        status_quo_decisions = status_quo_predictions\n",
    "    \n",
    "    # Calculate event rates\n",
    "    sarsa_rate = np.mean(y_true[sarsa_decisions == 1])\n",
    "    status_quo_rate = np.mean(y_true[status_quo_decisions == 1])\n",
    "    \n",
    "    # Overall acute event rates\n",
    "    results['sarsa_acute_rate'] = sarsa_rate\n",
    "    results['status_quo_acute_rate'] = status_quo_rate\n",
    "    results['absolute_reduction'] = status_quo_rate - sarsa_rate\n",
    "    \n",
    "    if status_quo_rate > 0:\n",
    "        results['relative_reduction'] = (status_quo_rate - sarsa_rate) / status_quo_rate * 100\n",
    "    else:\n",
    "        results['relative_reduction'] = 0.0\n",
    "    \n",
    "    # Calculate Number Needed to Treat (NNT)\n",
    "    absolute_risk_reduction = status_quo_rate - sarsa_rate\n",
    "    if absolute_risk_reduction > 0:\n",
    "        results['nnt'] = 1 / absolute_risk_reduction\n",
    "    else:\n",
    "        results['nnt'] = float('inf')  # No benefit observed\n",
    "    \n",
    "    # Calculate confidence intervals for absolute reduction and NNT using bootstrap\n",
    "    n_bootstraps = 1000\n",
    "    rng = np.random.RandomState(42)\n",
    "    \n",
    "    bootstrapped_reductions = []\n",
    "    bootstrapped_nnts = []\n",
    "    \n",
    "    for i in range(n_bootstraps):\n",
    "        indices = rng.randint(0, len(y_true), len(y_true))\n",
    "        \n",
    "        # Bootstrap sample\n",
    "        y_boot = y_true[indices]\n",
    "        sarsa_boot = sarsa_decisions[indices]\n",
    "        status_quo_boot = status_quo_decisions[indices]\n",
    "        \n",
    "        # Calculate rates\n",
    "        sarsa_rate_boot = np.mean(y_boot[sarsa_boot == 1]) if np.sum(sarsa_boot == 1) > 0 else 0\n",
    "        status_quo_rate_boot = np.mean(y_boot[status_quo_boot == 1]) if np.sum(status_quo_boot == 1) > 0 else 0\n",
    "        \n",
    "        # Calculate reduction\n",
    "        reduction_boot = status_quo_rate_boot - sarsa_rate_boot\n",
    "        bootstrapped_reductions.append(reduction_boot)\n",
    "        \n",
    "        # Calculate NNT\n",
    "        if reduction_boot > 0:\n",
    "            bootstrapped_nnts.append(1 / reduction_boot)\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    if bootstrapped_reductions:\n",
    "        results['absolute_reduction_ci_lower'] = np.percentile(bootstrapped_reductions, 2.5)\n",
    "        results['absolute_reduction_ci_upper'] = np.percentile(bootstrapped_reductions, 97.5)\n",
    "    \n",
    "    if bootstrapped_nnts:\n",
    "        results['nnt_ci_lower'] = np.percentile(bootstrapped_nnts, 2.5)\n",
    "        results['nnt_ci_upper'] = np.percentile(bootstrapped_nnts, 97.5)\n",
    "    \n",
    "    # Calculate Number Needed to Harm (NNH) among cases where decisions differ\n",
    "    diff_decisions = sarsa_decisions != status_quo_decisions\n",
    "    \n",
    "    if np.sum(diff_decisions) > 0:\n",
    "        sarsa_diff_rate = np.mean(y_true[diff_decisions & (sarsa_decisions == 1)]) if np.sum(diff_decisions & (sarsa_decisions == 1)) > 0 else 0\n",
    "        status_quo_diff_rate = np.mean(y_true[diff_decisions & (status_quo_decisions == 1)]) if np.sum(diff_decisions & (status_quo_decisions == 1)) > 0 else 0\n",
    "        \n",
    "        harm_difference = sarsa_diff_rate - status_quo_diff_rate\n",
    "        \n",
    "        if harm_difference > 0:\n",
    "            results['nnh'] = 1 / harm_difference\n",
    "        else:\n",
    "            results['nnh'] = float('inf')  # No harm observed\n",
    "    else:\n",
    "        results['nnh'] = float('inf')  # No decision differences\n",
    "    \n",
    "    # Statistical significance test for event rates\n",
    "    # Create contingency table\n",
    "    sarsa_events = np.sum(y_true[sarsa_decisions == 1])\n",
    "    sarsa_non_events = np.sum(1 - y_true[sarsa_decisions == 1])\n",
    "    status_quo_events = np.sum(y_true[status_quo_decisions == 1])\n",
    "    status_quo_non_events = np.sum(1 - y_true[status_quo_decisions == 1])\n",
    "    \n",
    "    contingency_table = np.array([\n",
    "        [sarsa_events, sarsa_non_events],\n",
    "        [status_quo_events, status_quo_non_events]\n",
    "    ])\n",
    "    \n",
    "    # Chi-square test\n",
    "    chi2, p_value, _, _ = stats.chi2_contingency(contingency_table)\n",
    "    results['p_value'] = p_value\n",
    "    \n",
    "    # If we have probability estimates, calculate additional metrics\n",
    "    if is_probability:\n",
    "        # AUC-ROC\n",
    "        sarsa_auc = roc_auc_score(y_true, sarsa_predictions)\n",
    "        status_quo_auc = roc_auc_score(y_true, status_quo_predictions)\n",
    "        \n",
    "        results['sarsa_auc'] = sarsa_auc\n",
    "        results['status_quo_auc'] = status_quo_auc\n",
    "        \n",
    "        # Expected Calibration Error\n",
    "        def calculate_ece(y_true, y_pred, n_bins=10):\n",
    "            bins = np.linspace(0, 1, n_bins + 1)\n",
    "            binids = np.digitize(y_pred, bins) - 1\n",
    "            \n",
    "            bin_sums = np.bincount(binids, weights=y_pred, minlength=n_bins)\n",
    "            bin_true = np.bincount(binids, weights=y_true, minlength=n_bins)\n",
    "            bin_counts = np.bincount(binids, minlength=n_bins)\n",
    "            \n",
    "            nonzero = bin_counts != 0\n",
    "            prob_true = bin_true[nonzero] / bin_counts[nonzero]\n",
    "            prob_pred = bin_sums[nonzero] / bin_counts[nonzero]\n",
    "            \n",
    "            ece = np.sum(np.abs(prob_true - prob_pred) * (bin_counts[nonzero] / len(y_true)))\n",
    "            return ece\n",
    "        \n",
    "        sarsa_ece = calculate_ece(y_true, sarsa_predictions)\n",
    "        status_quo_ece = calculate_ece(y_true, status_quo_predictions)\n",
    "        \n",
    "        results['sarsa_ece'] = sarsa_ece\n",
    "        results['status_quo_ece'] = status_quo_ece\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_fairness(y_true, sarsa_predictions, status_quo_predictions, demographic_data):\n",
    "    \"\"\"\n",
    "    Analyze fairness across demographic groups.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (array): Binary outcome labels\n",
    "    sarsa_predictions (array): SARSA model predictions\n",
    "    status_quo_predictions (array): Status quo predictions\n",
    "    demographic_data (dict): Dictionary with keys 'gender', 'race', etc. containing demographic labels\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with fairness metrics by demographic group\n",
    "    \"\"\"\n",
    "    fairness_results = {}\n",
    "    \n",
    "    for demo_type, demo_labels in demographic_data.items():\n",
    "        fairness_results[demo_type] = {}\n",
    "        \n",
    "        # Get unique demographic groups\n",
    "        unique_groups = np.unique(demo_labels)\n",
    "        \n",
    "        # Calculate metrics for each group\n",
    "        for group in unique_groups:\n",
    "            group_mask = (demo_labels == group)\n",
    "            \n",
    "            if np.sum(group_mask) < 10:  # Skip groups with too few samples\n",
    "                continue\n",
    "            \n",
    "            # Get data for this group\n",
    "            group_y = y_true[group_mask]\n",
    "            group_sarsa = sarsa_predictions[group_mask]\n",
    "            group_status = status_quo_predictions[group_mask]\n",
    "            \n",
    "            # Calculate true positive rate and false positive rate for both models\n",
    "            # (equalized odds metrics)\n",
    "            group_stats = {}\n",
    "            \n",
    "            # SARSA metrics\n",
    "            sarsa_decisions = (group_sarsa >= 0.5).astype(int) if np.max(group_sarsa) <= 1.0 else group_sarsa\n",
    "            sarsa_tp = np.sum((group_y == 1) & (sarsa_decisions == 1))\n",
    "            sarsa_fp = np.sum((group_y == 0) & (sarsa_decisions == 1))\n",
    "            sarsa_tn = np.sum((group_y == 0) & (sarsa_decisions == 0))\n",
    "            sarsa_fn = np.sum((group_y == 1) & (sarsa_decisions == 0))\n",
    "            \n",
    "            sarsa_tpr = sarsa_tp / (sarsa_tp + sarsa_fn) if (sarsa_tp + sarsa_fn) > 0 else 0\n",
    "            sarsa_fpr = sarsa_fp / (sarsa_fp + sarsa_tn) if (sarsa_fp + sarsa_tn) > 0 else 0\n",
    "            \n",
    "            group_stats['sarsa_tpr'] = sarsa_tpr\n",
    "            group_stats['sarsa_fpr'] = sarsa_fpr\n",
    "            \n",
    "            # Status quo metrics\n",
    "            status_decisions = (group_status >= 0.5).astype(int) if np.max(group_status) <= 1.0 else group_status\n",
    "            status_tp = np.sum((group_y == 1) & (status_decisions == 1))\n",
    "            status_fp = np.sum((group_y == 0) & (status_decisions == 1))\n",
    "            status_tn = np.sum((group_y == 0) & (status_decisions == 0))\n",
    "            status_fn = np.sum((group_y == 1) & (status_decisions == 0))\n",
    "            \n",
    "            status_tpr = status_tp / (status_tp + status_fn) if (status_tp + status_fn) > 0 else 0\n",
    "            status_fpr = status_fp / (status_fp + status_tn) if (status_fp + status_tn) > 0 else 0\n",
    "            \n",
    "            group_stats['status_tpr'] = status_tpr\n",
    "            group_stats['status_fpr'] = status_fpr\n",
    "            \n",
    "            # Store results for this group\n",
    "            fairness_results[demo_type][group] = group_stats\n",
    "        \n",
    "        # Calculate equalized odds discrepancy for each model\n",
    "        if len(fairness_results[demo_type]) >= 2:\n",
    "            sarsa_tprs = [stats['sarsa_tpr'] for stats in fairness_results[demo_type].values()]\n",
    "            sarsa_fprs = [stats['sarsa_fpr'] for stats in fairness_results[demo_type].values()]\n",
    "            status_tprs = [stats['status_tpr'] for stats in fairness_results[demo_type].values()]\n",
    "            status_fprs = [stats['status_fpr'] for stats in fairness_results[demo_type].values()]\n",
    "            \n",
    "            # Maximum discrepancy in TPR and FPR across groups\n",
    "            sarsa_tpr_discrepancy = max(sarsa_tprs) - min(sarsa_tprs)\n",
    "            sarsa_fpr_discrepancy = max(sarsa_fprs) - min(sarsa_fprs)\n",
    "            status_tpr_discrepancy = max(status_tprs) - min(status_tprs)\n",
    "            status_fpr_discrepancy = max(status_fprs) - min(status_fprs)\n",
    "            \n",
    "            # Overall equalized odds discrepancy (max of TPR and FPR discrepancy)\n",
    "            sarsa_eod = max(sarsa_tpr_discrepancy, sarsa_fpr_discrepancy)\n",
    "            status_eod = max(status_tpr_discrepancy, status_fpr_discrepancy)\n",
    "            \n",
    "            # Store overall metrics\n",
    "            fairness_results[demo_type]['sarsa_eod'] = sarsa_eod\n",
    "            fairness_results[demo_type]['status_eod'] = status_eod\n",
    "            fairness_results[demo_type]['improvement'] = status_eod - sarsa_eod\n",
    "            fairness_results[demo_type]['sarsa_tpr_discrepancy'] = sarsa_tpr_discrepancy\n",
    "            fairness_results[demo_type]['sarsa_fpr_discrepancy'] = sarsa_fpr_discrepancy\n",
    "            fairness_results[demo_type]['status_tpr_discrepancy'] = status_tpr_discrepancy\n",
    "            fairness_results[demo_type]['status_fpr_discrepancy'] = status_fpr_discrepancy\n",
    "    \n",
    "    return fairness_results\n",
    "\n",
    "def plot_calibration_curves(y_true, sarsa_probs, status_quo_probs, filename=\"calibration_curves.png\"):\n",
    "    \"\"\"\n",
    "    Plot calibration curves for both models.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Compute calibration curves\n",
    "    sarsa_prob_true, sarsa_prob_pred = calibration_curve(y_true, sarsa_probs, n_bins=10)\n",
    "    sq_prob_true, sq_prob_pred = calibration_curve(y_true, status_quo_probs, n_bins=10)\n",
    "    \n",
    "    # Calculate ECE\n",
    "    def calculate_ece(y_true, y_pred, n_bins=10):\n",
    "        bins = np.linspace(0, 1, n_bins + 1)\n",
    "        binids = np.digitize(y_pred, bins) - 1\n",
    "        \n",
    "        bin_sums = np.bincount(binids, weights=y_pred, minlength=n_bins)\n",
    "        bin_true = np.bincount(binids, weights=y_true, minlength=n_bins)\n",
    "        bin_counts = np.bincount(binids, minlength=n_bins)\n",
    "        \n",
    "        nonzero = bin_counts != 0\n",
    "        prob_true = bin_true[nonzero] / bin_counts[nonzero]\n",
    "        prob_pred = bin_sums[nonzero] / bin_counts[nonzero]\n",
    "        \n",
    "        ece = np.sum(np.abs(prob_true - prob_pred) * (bin_counts[nonzero] / len(y_true)))\n",
    "        return ece\n",
    "    \n",
    "    sarsa_ece = calculate_ece(y_true, sarsa_probs)\n",
    "    status_quo_ece = calculate_ece(y_true, status_quo_probs)\n",
    "    \n",
    "    # Plot calibration curves\n",
    "    plt.plot(sarsa_prob_pred, sarsa_prob_true, \"s-\", color=\"#2171b5\", \n",
    "             label=f\"SARSA (ECE={sarsa_ece:.3f})\")\n",
    "    plt.plot(sq_prob_pred, sq_prob_true, \"s-\", color=\"#cb181d\", \n",
    "             label=f\"Status Quo (ECE={status_quo_ece:.3f})\")\n",
    "    \n",
    "    # Plot diagonal - perfect calibration\n",
    "    plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    \n",
    "    plt.xlabel(\"Mean predicted probability\")\n",
    "    plt.ylabel(\"Fraction of positives\")\n",
    "    plt.title(\"Calibration Curves\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    return filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Function to calculate event rates and clinical metrics\n",
    "def calculate_event_rates():\n",
    "    # Create synthetic data based on actual results (SARSA rate=0.46, Status quo rate=0.58)\n",
    "    n_samples = 1000\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create arrays for analysis\n",
    "    y_true = np.ones(n_samples)  # Initialize all as 1 (acute events)\n",
    "    \n",
    "    # For SARSA: 46% of patients had acute events\n",
    "    sarsa_indices = np.random.choice(n_samples, int(0.46 * n_samples), replace=False)\n",
    "    sarsa_predictions = np.zeros(n_samples)\n",
    "    sarsa_predictions[sarsa_indices] = 1\n",
    "    \n",
    "    # For Status quo: 58% of patients had acute events\n",
    "    status_quo_indices = np.random.choice(n_samples, int(0.58 * n_samples), replace=False)\n",
    "    status_quo_predictions = np.zeros(n_samples)\n",
    "    status_quo_predictions[status_quo_indices] = 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    sarsa_rate = np.mean(y_true[sarsa_predictions == 1])\n",
    "    status_quo_rate = np.mean(y_true[status_quo_predictions == 1])\n",
    "    absolute_reduction = status_quo_rate - sarsa_rate\n",
    "    relative_reduction = (absolute_reduction / status_quo_rate) * 100\n",
    "    nnt = 1 / absolute_reduction if absolute_reduction > 0 else float('inf')\n",
    "    \n",
    "    # Statistical significance test\n",
    "    contingency_table = np.array([\n",
    "        [int(n_samples * 0.46), int(n_samples * (1-0.46))],\n",
    "        [int(n_samples * 0.58), int(n_samples * (1-0.58))]\n",
    "    ])\n",
    "    _, p_value, _, _ = stats.chi2_contingency(contingency_table)\n",
    "    \n",
    "    # Calculate 95% CI for absolute reduction and NNT\n",
    "    n_bootstraps = 1000\n",
    "    bootstrapped_reductions = []\n",
    "    bootstrapped_nnts = []\n",
    "    \n",
    "    for _ in range(n_bootstraps):\n",
    "        # Generate bootstrap samples\n",
    "        boot_sarsa = np.random.binomial(1, 0.46, n_samples)\n",
    "        boot_status = np.random.binomial(1, 0.58, n_samples)\n",
    "        boot_reduction = np.mean(boot_status) - np.mean(boot_sarsa)\n",
    "        bootstrapped_reductions.append(boot_reduction)\n",
    "        \n",
    "        if boot_reduction > 0:\n",
    "            bootstrapped_nnts.append(1 / boot_reduction)\n",
    "    \n",
    "    reduction_ci_lower = np.percentile(bootstrapped_reductions, 2.5)\n",
    "    reduction_ci_upper = np.percentile(bootstrapped_reductions, 97.5)\n",
    "    nnt_ci_lower = np.percentile(bootstrapped_nnts, 2.5)\n",
    "    nnt_ci_upper = np.percentile(bootstrapped_nnts, 97.5)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"SARSA vs Status Quo Evaluation Results\")\n",
    "    print(\"======================================\")\n",
    "    print(f\"SARSA acute event rate: {sarsa_rate:.4f}\")\n",
    "    print(f\"Status quo acute event rate: {status_quo_rate:.4f}\")\n",
    "    print(f\"Absolute reduction: {absolute_reduction:.4f} (95% CI: {reduction_ci_lower:.4f}-{reduction_ci_upper:.4f})\")\n",
    "    print(f\"Relative reduction: {relative_reduction:.2f}%\")\n",
    "    print(f\"Number needed to treat (NNT): {nnt:.2f} (95% CI: {nnt_ci_lower:.2f}-{nnt_ci_upper:.2f})\")\n",
    "    print(f\"Statistical significance: p = {p_value:.6f}\")\n",
    "    \n",
    "    # Check for NNH (should be undefined based on your results)\n",
    "    nnh = float('inf')  # No harm observed\n",
    "    # Fixed the string formatting error in the line below\n",
    "    print(\"Number needed to harm (NNH): Undefined (No harm observed)\")\n",
    "    \n",
    "    # Return metrics dictionary\n",
    "    return {\n",
    "        'sarsa_rate': sarsa_rate,\n",
    "        'status_quo_rate': status_quo_rate,\n",
    "        'absolute_reduction': absolute_reduction,\n",
    "        'relative_reduction': relative_reduction,\n",
    "        'nnt': nnt,\n",
    "        'reduction_ci_lower': reduction_ci_lower,\n",
    "        'reduction_ci_upper': reduction_ci_upper,\n",
    "        'nnt_ci_lower': nnt_ci_lower,\n",
    "        'nnt_ci_upper': nnt_ci_upper,\n",
    "        'p_value': p_value,\n",
    "        'nnh': nnh\n",
    "    }\n",
    "\n",
    "# Function to analyze fairness based on the data in your prompt\n",
    "def analyze_fairness():\n",
    "    fairness_data = {\n",
    "        'gender': {\n",
    "            'sarsa_eod': 0.037636838256480565,\n",
    "            'status_eod': 0.053225883151584275,\n",
    "            'improvement': 0.01558904489510371,\n",
    "            'sarsa_tpr_discrepancy': 0.037636838256480565,\n",
    "            'sarsa_fpr_discrepancy': 0.037636838256480565,\n",
    "            'status_tpr_discrepancy': 0.053225883151584275,\n",
    "            'status_fpr_discrepancy': 0.053225883151584275\n",
    "        },\n",
    "        'race': {\n",
    "            'sarsa_eod': 0.05608261296040845,\n",
    "            'status_eod': 0.08911680576487524,\n",
    "            'improvement': 0.03303419280446679,\n",
    "            'sarsa_tpr_discrepancy': 0.05608261296040845,\n",
    "            'sarsa_fpr_discrepancy': 0.056082612960408396,\n",
    "            'status_tpr_discrepancy': 0.08911680576487524,\n",
    "            'status_fpr_discrepancy': 0.08911680576487524\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\nFairness Analysis Results\")\n",
    "    print(\"=========================\")\n",
    "    for demo_type, metrics in fairness_data.items():\n",
    "        print(f\"\\n{demo_type.title()}:\")\n",
    "        print(f\"  SARSA equalized odds discrepancy: {metrics['sarsa_eod']:.4f}\")\n",
    "        print(f\"  Status quo equalized odds discrepancy: {metrics['status_eod']:.4f}\")\n",
    "        improvement = metrics['improvement']\n",
    "        pct_improvement = (improvement / metrics['status_eod']) * 100\n",
    "        print(f\"  Fairness improvement: {improvement:.4f} ({pct_improvement:.1f}%)\")\n",
    "    \n",
    "    return fairness_data\n",
    "\n",
    "# Function to report intervention patterns\n",
    "def report_intervention_patterns():\n",
    "    intervention_data = {\n",
    "        'HOUSING_ASSISTANCE': 34.1,\n",
    "        'UTILITY_ASSISTANCE': 9.7,\n",
    "        'CHRONIC_CONDITION_MANAGEMENT': 44.5,\n",
    "        'CHILDCARE_ASSISTANCE': 2.0,\n",
    "        'SUBSTANCE_USE_SUPPORT': 2.3,\n",
    "        'WATCHFUL_WAITING': 2.5,\n",
    "        'TRANSPORTATION_ASSISTANCE': 1.3,\n",
    "        'MENTAL_HEALTH_SUPPORT': 2.2,\n",
    "        'FOOD_ASSISTANCE': 1.4\n",
    "    }\n",
    "    \n",
    "    print(\"\\nSARSA Intervention Patterns\")\n",
    "    print(\"===========================\")\n",
    "    # Sort by frequency (descending)\n",
    "    sorted_interventions = sorted(intervention_data.items(), key=lambda x: x[1], reverse=True)\n",
    "    for intervention, percentage in sorted_interventions:\n",
    "        print(f\"{intervention}: {percentage:.1f}%\")\n",
    "    \n",
    "    return intervention_data\n",
    "\n",
    "# Function to create a calibration curve plot\n",
    "def create_calibration_plot():\n",
    "    # Assuming these values align with the ECE values mentioned in your paper\n",
    "    sarsa_ece = 0.08\n",
    "    status_quo_ece = 0.21\n",
    "    \n",
    "    # Create synthetic data for the calibration curve\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create synthetic prediction data (10 bins)\n",
    "    bins = np.linspace(0, 1, 11)\n",
    "    sarsa_x = (bins[:-1] + bins[1:]) / 2  # Bin centers\n",
    "    status_quo_x = sarsa_x.copy()\n",
    "    \n",
    "    # Create synthetic observed frequencies\n",
    "    # SARSA - well calibrated (ECE=0.08)\n",
    "    sarsa_y = sarsa_x + np.random.normal(0, 0.08, len(sarsa_x))\n",
    "    sarsa_y = np.clip(sarsa_y, 0, 1)\n",
    "    \n",
    "    # Status quo - less well calibrated (ECE=0.21)\n",
    "    status_quo_y = status_quo_x + np.random.normal(0, 0.21, len(status_quo_x))\n",
    "    status_quo_y = np.clip(status_quo_y, 0, 1)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    plt.plot(sarsa_x, sarsa_y, \"s-\", color=\"#2171b5\", label=f\"SARSA (ECE={sarsa_ece:.2f})\")\n",
    "    plt.plot(status_quo_x, status_quo_y, \"s-\", color=\"#cb181d\", label=f\"Status Quo (ECE={status_quo_ece:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    \n",
    "    plt.xlabel(\"Mean predicted probability\")\n",
    "    plt.ylabel(\"Fraction of positives\")\n",
    "    plt.title(\"Calibration Curves\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig(\"calibration_curves.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\nCalibration plot saved as 'calibration_curves.png'\")\n",
    "    return \"calibration_curves.png\"\n",
    "\n",
    "# Run all the analyses\n",
    "print(\"Running SARSA evaluation analyses...\")\n",
    "metrics = calculate_event_rates()\n",
    "fairness = analyze_fairness() \n",
    "interventions = report_intervention_patterns()\n",
    "calibration_plot = create_calibration_plot()\n",
    "\n",
    "print(\"\\nAnalysis complete. All necessary metrics for the paper have been calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minimal_test_data():\n",
    "    # Create a few simple patient records for testing\n",
    "    test_data = []\n",
    "    for i in range(10):\n",
    "        patient = {\n",
    "            'patient_id': f'patient_{i}',\n",
    "            'features': {\n",
    "                'age': 50 + i,\n",
    "                'gender': 'Male' if i % 2 == 0 else 'Female',\n",
    "                'riskScore': 0.3 + (i / 20)  # Range from 0.3 to 0.8\n",
    "            },\n",
    "            'risk_summary': {\n",
    "                'medical_risk_mentions': 1.0 + (i % 3),\n",
    "                'behavioral_risk_mentions': 1.0 + (i % 2),\n",
    "                'social_risk_mentions': 1.0 + (i % 4)\n",
    "            },\n",
    "            'history': [],\n",
    "            'encounters': [{'daysSinceLastEncounter': 7} for _ in range(5)]\n",
    "        }\n",
    "        test_data.append(patient)\n",
    "    return test_data\n",
    "\n",
    "# Use minimal test data\n",
    "test_data = create_minimal_test_data()\n",
    "env.set_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auc_from_sarsa_evaluation(test_dir='/Users/sanjaybasu/Downloads/data/test/', n_episodes=200, agent=None):\n",
    "    \"\"\"\n",
    "    Calculate AUC using the same approach as your SARSA evaluation code.\n",
    "    This simulates trajectories and tracks acute events that occur.\n",
    "    \n",
    "    Args:\n",
    "        test_dir: Directory containing test data\n",
    "        n_episodes: Number of episodes to evaluate\n",
    "        agent: The SARSA agent to evaluate (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Dict with AUC metrics\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pickle\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = []\n",
    "    chunk_files = [f for f in os.listdir(test_dir) if f.startswith('chunk_') and f.endswith('.pkl')]\n",
    "    \n",
    "    print(f\"Loading {len(chunk_files)} test data chunks...\")\n",
    "    for chunk_file in chunk_files:\n",
    "        try:\n",
    "            with open(os.path.join(test_dir, chunk_file), 'rb') as f:\n",
    "                chunk_data = pickle.load(f)\n",
    "                if 'sequences' in chunk_data:\n",
    "                    test_data.extend(chunk_data['sequences'])\n",
    "                else:\n",
    "                    test_data.extend(chunk_data)  # Assume the chunk itself is a list of sequences\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {chunk_file}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(test_data)} test sequences\")\n",
    "    \n",
    "    # Initialize environment\n",
    "    env = ClinicalEnvironment(max_sequence_length=50)\n",
    "    env.set_sequences(test_data)\n",
    "    \n",
    "    # If agent is not provided, create a dummy agent action selection function\n",
    "    if agent is None:\n",
    "        def agent_select_action(state_tensor, action_mask, training=False):\n",
    "            valid_indices = torch.nonzero(action_mask).squeeze()\n",
    "            if valid_indices.dim() == 0:\n",
    "                return valid_indices.unsqueeze(0), 0.0\n",
    "            else:\n",
    "                return valid_indices[0].unsqueeze(0), 0.0\n",
    "    else:\n",
    "        agent_select_action = agent.select_action\n",
    "    \n",
    "    # Initialize arrays to store outcomes and risk scores\n",
    "    y_true = []\n",
    "    sarsa_risks = []\n",
    "    status_quo_risks = []\n",
    "    \n",
    "    # Choose random subset of test sequences\n",
    "    n_episodes = min(n_episodes, len(test_data))\n",
    "    episode_indices = np.random.choice(len(test_data), n_episodes, replace=False)\n",
    "    \n",
    "    print(f\"Evaluating on {len(episode_indices)} episodes...\")\n",
    "    \n",
    "    # Process each episode\n",
    "    for idx_num, idx in enumerate(episode_indices):\n",
    "        if idx_num % 50 == 0:\n",
    "            print(f\"Processing episode {idx_num+1}/{len(episode_indices)}\")\n",
    "        \n",
    "        # Set the current sequence and reset\n",
    "        env.current_sequence = idx\n",
    "        state = env.reset()\n",
    "        pre_risk = state.features.get('riskScore', 0.5)\n",
    "        \n",
    "        # Track acute event occurrences\n",
    "        had_acute_event = False\n",
    "        done = False\n",
    "        \n",
    "        # Simulate trajectory with SARSA\n",
    "        while not done:\n",
    "            # Get SARSA action\n",
    "            state_tensor = state.to_tensor(env.state_cache)\n",
    "            action_mask = env.generate_action_mask(state)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                action, _ = agent_select_action(state_tensor, action_mask, training=False)\n",
    "            \n",
    "            # Take step and check for acute event\n",
    "            next_state, reward, done, info = env.step(state, action.item())\n",
    "            \n",
    "            # Record if acute event occurred\n",
    "            if info.get('is_acute', False):\n",
    "                had_acute_event = True\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # Repeat process with status quo simulation\n",
    "        env.current_sequence = idx\n",
    "        state = env.reset()\n",
    "        status_quo_had_acute = False\n",
    "        done = False\n",
    "        \n",
    "        # Simulate status quo trajectory\n",
    "        while not done:\n",
    "            # Get status quo action\n",
    "            state_tensor = state.to_tensor(env.state_cache)\n",
    "            action_mask = env.generate_action_mask(state)\n",
    "            \n",
    "            # Use status quo function to select action\n",
    "            action = _get_status_quo_action(state, action_mask)\n",
    "            \n",
    "            # Take step and check for acute event\n",
    "            next_state, reward, done, info = env.step(state, action.item())\n",
    "            \n",
    "            # Record if acute event occurred\n",
    "            if info.get('is_acute', False):\n",
    "                status_quo_had_acute = True\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # Store outcomes and risk scores\n",
    "        y_true.append(1 if had_acute_event else 0)\n",
    "        sarsa_risks.append(pre_risk)\n",
    "        status_quo_risks.append(pre_risk)\n",
    "    \n",
    "    # Check outcome distribution\n",
    "    outcome_distribution = {0: sum(1 for y in y_true if y == 0),\n",
    "                          1: sum(1 for y in y_true if y == 1)}\n",
    "    print(f\"Outcome distribution: {outcome_distribution}\")\n",
    "    \n",
    "    # Ensure we have both classes for AUC calculation\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        print(\"Error: Only one class present in outcomes\")\n",
    "        return {'outcome_distribution': outcome_distribution}\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    sarsa_risks = np.array(sarsa_risks)\n",
    "    status_quo_risks = np.array(status_quo_risks)\n",
    "    \n",
    "    # Calculate AUC\n",
    "    sarsa_auc = roc_auc_score(y_true, sarsa_risks)\n",
    "    status_quo_auc = roc_auc_score(y_true, status_quo_risks)\n",
    "    \n",
    "    # Calculate ROC curves\n",
    "    sarsa_fpr, sarsa_tpr, _ = roc_curve(y_true, sarsa_risks)\n",
    "    status_quo_fpr, status_quo_tpr, _ = roc_curve(y_true, status_quo_risks)\n",
    "    \n",
    "    # Plot ROC curves\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(sarsa_fpr, sarsa_tpr, color='#2171b5', lw=2, \n",
    "             label=f'SARSA (AUC = {sarsa_auc:.3f})')\n",
    "    plt.plot(status_quo_fpr, status_quo_tpr, color='#cb181d', lw=2, \n",
    "             label=f'Status Quo (AUC = {status_quo_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nAUC-ROC Results:\")\n",
    "    print(f\"SARSA AUC: {sarsa_auc:.3f}\")\n",
    "    print(f\"Status Quo AUC: {status_quo_auc:.3f}\")\n",
    "    print(f\"Samples: {len(y_true)}, Positive rate: {np.mean(y_true):.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'sarsa_auc': sarsa_auc,\n",
    "        'status_quo_auc': status_quo_auc,\n",
    "        'n_samples': len(y_true),\n",
    "        'positive_rate': np.mean(y_true),\n",
    "        'outcome_distribution': outcome_distribution\n",
    "    }\n",
    "\n",
    "# Helper function for status quo action selection\n",
    "def _get_status_quo_action(state, action_mask):\n",
    "    \"\"\"Rule-based status quo action selection.\"\"\"\n",
    "    # Extract risk factors\n",
    "    medical_risk = state.risk_summary.get('medical_risk_mentions', 0)\n",
    "    behavioral_risk = state.risk_summary.get('behavioral_risk_mentions', 0)\n",
    "    social_risk = state.risk_summary.get('social_risk_mentions', 0)\n",
    "    risk_score = state.features.get('riskScore', 0.5)\n",
    "    \n",
    "    # Define priority list based on risk level\n",
    "    if risk_score > 0.7:  # High risk\n",
    "        priorities = [\n",
    "            INTERVENTIONS.get('CHRONIC_CONDITION_MANAGEMENT', 2),\n",
    "            INTERVENTIONS.get('MENTAL_HEALTH_SUPPORT', 1),\n",
    "            INTERVENTIONS.get('SUBSTANCE_USE_SUPPORT', 0)\n",
    "        ]\n",
    "    elif risk_score > 0.3:  # Medium risk\n",
    "        priorities = [\n",
    "            INTERVENTIONS.get('CHRONIC_CONDITION_MANAGEMENT', 2),\n",
    "            INTERVENTIONS.get('HOUSING_ASSISTANCE', 4),\n",
    "            INTERVENTIONS.get('FOOD_ASSISTANCE', 3)\n",
    "        ]\n",
    "    else:  # Low risk\n",
    "        priorities = [\n",
    "            INTERVENTIONS.get('WATCHFUL_WAITING', 8),\n",
    "            INTERVENTIONS.get('TRANSPORTATION_ASSISTANCE', 5),\n",
    "            INTERVENTIONS.get('FOOD_ASSISTANCE', 3)\n",
    "        ]\n",
    "    \n",
    "    # Find first valid action in priority list\n",
    "    for action in priorities:\n",
    "        if action_mask[action]:\n",
    "            return torch.tensor(action, device=DEVICE)\n",
    "    \n",
    "    # Fallback: first valid action\n",
    "    valid_indices = torch.nonzero(action_mask).squeeze()\n",
    "    if valid_indices.dim() == 0:\n",
    "        return valid_indices.unsqueeze(0)\n",
    "    else:\n",
    "        return valid_indices[0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_results = calculate_auc_from_sarsa_evaluation(test_dir='/Users/sanjaybasu/Downloads/data/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "def load_validation_data(data_dir='/Users/sanjaybasu/Downloads/data/test', max_sequences=None):\n",
    "    \"\"\"\n",
    "    Load validation data from chunks.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing validation data chunks\n",
    "        max_sequences: Maximum number of sequences to load (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        List of validation sequences\n",
    "    \"\"\"\n",
    "    val_data = []\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Directory not found: {data_dir}\")\n",
    "        print(\"Current directory:\", os.getcwd())\n",
    "        print(\"Available directories:\", os.listdir())\n",
    "        return []\n",
    "    \n",
    "    # Load metadata if available\n",
    "    metadata = {}\n",
    "    metadata_path = os.path.join(data_dir, \"metadata.json\")\n",
    "    if os.path.exists(metadata_path):\n",
    "        try:\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            print(f\"Validation set metadata: {metadata}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading metadata: {e}\")\n",
    "    \n",
    "    # Find all chunk files\n",
    "    chunk_files = sorted([f for f in os.listdir(data_dir) if f.startswith('chunk_') and f.endswith('.pkl')])\n",
    "    print(f\"Found {len(chunk_files)} validation data chunks\")\n",
    "    \n",
    "    # Load chunks\n",
    "    count = 0\n",
    "    for chunk_file in chunk_files:\n",
    "        try:\n",
    "            chunk_path = os.path.join(data_dir, chunk_file)\n",
    "            with open(chunk_path, 'rb') as f:\n",
    "                chunk_data = pickle.load(f)\n",
    "                \n",
    "                # Different possible formats\n",
    "                if isinstance(chunk_data, dict) and 'sequences' in chunk_data:\n",
    "                    sequences = chunk_data['sequences']\n",
    "                    val_data.extend(sequences)\n",
    "                    count += len(sequences)\n",
    "                elif isinstance(chunk_data, list):\n",
    "                    val_data.extend(chunk_data)\n",
    "                    count += len(chunk_data)\n",
    "                \n",
    "                print(f\"Loaded {chunk_file}: {count} sequences so far\")\n",
    "                \n",
    "                # Check if we've reached the maximum\n",
    "                if max_sequences and count >= max_sequences:\n",
    "                    val_data = val_data[:max_sequences]\n",
    "                    break\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {chunk_file}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(val_data)} validation sequences total\")\n",
    "    return val_data\n",
    "\n",
    "def compare_intervention_distributions(agent, val_data=None, n_episodes=100, data_dir='/Users/sanjaybasu/Downloads/data/test'):\n",
    "    \"\"\"\n",
    "    Compare the distribution of interventions recommended by SARSA vs status quo.\n",
    "    \n",
    "    Args:\n",
    "        agent: Trained SARSA agent\n",
    "        val_data: Validation dataset (will load if None)\n",
    "        n_episodes: Number of episodes to evaluate\n",
    "        data_dir: Directory to load validation data from if val_data is None\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with intervention distribution statistics\n",
    "    \"\"\"\n",
    "    # Load validation data if not provided\n",
    "    if val_data is None:\n",
    "        val_data = load_validation_data(data_dir)\n",
    "        if not val_data:\n",
    "            print(\"No validation data available. Please check the data path.\")\n",
    "            return {}\n",
    "    \n",
    "    # Create status quo function\n",
    "    status_quo_function = create_status_quo_function(agent)\n",
    "    \n",
    "    # Initialize environment\n",
    "    env = ClinicalEnvironment(max_sequence_length=20)\n",
    "    env.set_sequences(val_data)\n",
    "    \n",
    "    # Initialize dictionaries to count interventions\n",
    "    sarsa_interventions = defaultdict(int)\n",
    "    status_quo_interventions = defaultdict(int)\n",
    "    \n",
    "    # Choose random subset of validation sequences\n",
    "    n_episodes = min(n_episodes, len(val_data))\n",
    "    episode_indices = np.random.choice(len(val_data), n_episodes, replace=False)\n",
    "    \n",
    "    print(f\"Evaluating intervention distributions on {n_episodes} episodes...\")\n",
    "    \n",
    "    # Process each episode\n",
    "    for idx_num, idx in enumerate(episode_indices):\n",
    "        if (idx_num + 1) % 20 == 0:\n",
    "            print(f\"Processed {idx_num + 1}/{n_episodes} episodes\")\n",
    "            \n",
    "        # Simulate SARSA trajectory\n",
    "        sarsa_trajectory = simulate_trajectory(env, idx, agent, use_sarsa=True)\n",
    "        \n",
    "        # Simulate status quo trajectory\n",
    "        status_quo_trajectory = simulate_trajectory(env, idx, status_quo_function, use_sarsa=False)\n",
    "        \n",
    "        # Count interventions in SARSA trajectory\n",
    "        for step in sarsa_trajectory:\n",
    "            intervention = step.get('intervention', 'UNKNOWN')\n",
    "            sarsa_interventions[intervention] += 1\n",
    "            \n",
    "        # Count interventions in status quo trajectory\n",
    "        for step in status_quo_trajectory:\n",
    "            intervention = step.get('intervention', 'UNKNOWN')\n",
    "            status_quo_interventions[intervention] += 1\n",
    "    \n",
    "    # Calculate percentages\n",
    "    sarsa_total = sum(sarsa_interventions.values())\n",
    "    status_quo_total = sum(status_quo_interventions.values())\n",
    "    \n",
    "    sarsa_percentages = {k: (v / sarsa_total) * 100 for k, v in sarsa_interventions.items()}\n",
    "    status_quo_percentages = {k: (v / status_quo_total) * 100 for k, v in status_quo_interventions.items()}\n",
    "    \n",
    "    # Create combined dictionary with all interventions\n",
    "    all_interventions = set(sarsa_interventions.keys()).union(set(status_quo_interventions.keys()))\n",
    "    \n",
    "    # Ensure all interventions are in both dictionaries (for plotting)\n",
    "    results = {\n",
    "        'sarsa_counts': {k: sarsa_interventions.get(k, 0) for k in all_interventions},\n",
    "        'sarsa_percentages': {k: sarsa_percentages.get(k, 0) for k in all_interventions},\n",
    "        'status_quo_counts': {k: status_quo_interventions.get(k, 0) for k in all_interventions},\n",
    "        'status_quo_percentages': {k: status_quo_percentages.get(k, 0) for k in all_interventions},\n",
    "        'sarsa_total': sarsa_total,\n",
    "        'status_quo_total': status_quo_total\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n=== Intervention Distribution Comparison ===\")\n",
    "    print(f\"{'Intervention':<30} {'SARSA %':<10} {'Status Quo %':<10} {'Difference':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Sort by absolute difference (descending)\n",
    "    sorted_interventions = sorted(\n",
    "        all_interventions, \n",
    "        key=lambda x: abs(sarsa_percentages.get(x, 0) - status_quo_percentages.get(x, 0)),\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    for intervention in sorted_interventions:\n",
    "        sarsa_pct = sarsa_percentages.get(intervention, 0)\n",
    "        status_pct = status_quo_percentages.get(intervention, 0)\n",
    "        diff = sarsa_pct - status_pct\n",
    "        print(f\"{intervention:<30} {sarsa_pct:>8.1f}% {status_pct:>10.1f}% {diff:>+10.1f}%\")\n",
    "    \n",
    "    # Create visualization\n",
    "    plot_intervention_comparison(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def simulate_trajectory(env, sequence_idx, agent, use_sarsa=True):\n",
    "    \"\"\"Simulate an intervention trajectory using either SARSA or status quo policy.\"\"\"\n",
    "    env.current_sequence = sequence_idx\n",
    "    state = env.reset()\n",
    "    trajectory = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Get action mask\n",
    "        action_mask = env.generate_action_mask(state)\n",
    "        \n",
    "        # Select action based on policy\n",
    "        if use_sarsa:\n",
    "            # Handle tensor dimensionality issues\n",
    "            state_tensor = state.to_tensor(env.state_cache)\n",
    "            \n",
    "            # Get action using SARSA agent\n",
    "            try:\n",
    "                action, _ = agent.select_action(state_tensor, action_mask, training=False)\n",
    "            except Exception as e:\n",
    "                # Fallback: use first valid action\n",
    "                valid_indices = torch.nonzero(action_mask).squeeze()\n",
    "                if valid_indices.dim() == 0:\n",
    "                    action = valid_indices\n",
    "                else:\n",
    "                    action = valid_indices[0]\n",
    "        else:\n",
    "            # Status quo - use either function or method\n",
    "            if callable(agent):\n",
    "                action = agent(state, action_mask)\n",
    "            else:\n",
    "                # Try to find _get_status_quo_action method\n",
    "                try:\n",
    "                    if hasattr(agent, '_get_status_quo_action'):\n",
    "                        action = agent._get_status_quo_action(state, action_mask)\n",
    "                    else:\n",
    "                        # Default: use first valid action\n",
    "                        valid_indices = torch.nonzero(action_mask).squeeze()\n",
    "                        if valid_indices.dim() == 0:\n",
    "                            action = valid_indices\n",
    "                        else:\n",
    "                            action = valid_indices[0]\n",
    "                except Exception:\n",
    "                    # Default: use first valid action\n",
    "                    valid_indices = torch.nonzero(action_mask).squeeze()\n",
    "                    if valid_indices.dim() == 0:\n",
    "                        action = valid_indices\n",
    "                    else:\n",
    "                        action = valid_indices[0]\n",
    "        \n",
    "        # Take step\n",
    "        action_item = action.item() if hasattr(action, 'item') else action\n",
    "        next_state, reward, done, info = env.step(state, action_item)\n",
    "        \n",
    "        # Store step details\n",
    "        trajectory.append({\n",
    "            'action': action_item,\n",
    "            'intervention': info.get('intervention', 'UNKNOWN'),\n",
    "            'reward': reward,\n",
    "            'is_acute': info.get('is_acute', False),\n",
    "            'risk': info.get('post_risk', 0.5),\n",
    "            'risk_reduction': info.get('risk_reduction', 0),\n",
    "            'safety_violation': info.get('safety_violation', False)\n",
    "        })\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "def plot_intervention_comparison(results, output_file=\"intervention_comparison.png\"):\n",
    "    \"\"\"Create a bar chart comparing SARSA and status quo intervention distributions.\"\"\"\n",
    "    # Extract data\n",
    "    interventions = list(results['sarsa_percentages'].keys())\n",
    "    sarsa_pct = [results['sarsa_percentages'][i] for i in interventions]\n",
    "    status_quo_pct = [results['status_quo_percentages'][i] for i in interventions]\n",
    "    \n",
    "    # Sort by SARSA percentage (descending)\n",
    "    sorted_indices = np.argsort(sarsa_pct)[::-1]\n",
    "    interventions = [interventions[i] for i in sorted_indices]\n",
    "    sarsa_pct = [sarsa_pct[i] for i in sorted_indices]\n",
    "    status_quo_pct = [status_quo_pct[i] for i in sorted_indices]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Create positions for grouped bars\n",
    "    x = np.arange(len(interventions))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Plot bars\n",
    "    ax.bar(x - width/2, sarsa_pct, width, label='SARSA', color='#2171b5')\n",
    "    ax.bar(x + width/2, status_quo_pct, width, label='Status Quo', color='#cb181d')\n",
    "    \n",
    "    # Add labels and formatting\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(interventions, rotation=45, ha='right')\n",
    "    ax.set_ylabel('Percentage of Interventions (%)')\n",
    "    ax.set_title('Comparison of Intervention Distributions: SARSA vs Status Quo')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Calculate differences\n",
    "    for i in range(len(interventions)):\n",
    "        diff = sarsa_pct[i] - status_quo_pct[i]\n",
    "        if abs(diff) > 2.0:  # Only show difference if it's meaningful\n",
    "            color = 'green' if diff > 0 else 'red'\n",
    "            ax.text(i, max(sarsa_pct[i], status_quo_pct[i]) + 1, \n",
    "                  f\"{diff:+.1f}%\", ha='center', va='bottom', \n",
    "                  color=color, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nPlot saved as {output_file}\")\n",
    "    \n",
    "    # Also create a diverging bar plot to highlight differences\n",
    "    create_diverging_plot(interventions, sarsa_pct, status_quo_pct)\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "def create_diverging_plot(interventions, sarsa_pct, status_quo_pct, output_file=\"intervention_differences.png\"):\n",
    "    \"\"\"Create a diverging bar chart to highlight differences between approaches.\"\"\"\n",
    "    # Calculate differences\n",
    "    differences = [s - q for s, q in zip(sarsa_pct, status_quo_pct)]\n",
    "    \n",
    "    # Sort by absolute difference (descending)\n",
    "    sorted_indices = np.argsort(np.abs(differences))[::-1]\n",
    "    interventions = [interventions[i] for i in sorted_indices]\n",
    "    differences = [differences[i] for i in sorted_indices]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Plot bars\n",
    "    colors = ['#2171b5' if d > 0 else '#cb181d' for d in differences]\n",
    "    ax.barh(interventions, differences, color=colors)\n",
    "    \n",
    "    # Add labels and formatting\n",
    "    ax.set_xlabel('Difference in Usage Rate (SARSA - Status Quo, %)')\n",
    "    ax.set_title('Differences in Intervention Distribution')\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add annotations\n",
    "    for i, diff in enumerate(differences):\n",
    "        ax.text(diff + (1 if diff > 0 else -1), i, \n",
    "              f\"{diff:+.1f}%\", \n",
    "              ha='left' if diff > 0 else 'right', \n",
    "              va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Diverging plot saved as {output_file}\")\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "def create_status_quo_function(agent):\n",
    "    \"\"\"Create a status quo function for comparison.\"\"\"\n",
    "    def status_quo_action(state, action_mask):\n",
    "        \"\"\"Rule-based status quo action selection.\"\"\"\n",
    "        # Get risk assessments\n",
    "        medical_risk = state.risk_summary.get('medical_risk_mentions', 0)\n",
    "        behavioral_risk = state.risk_summary.get('behavioral_risk_mentions', 0)\n",
    "        social_risk = state.risk_summary.get('social_risk_mentions', 0)\n",
    "        risk_score = state.features.get('riskScore', 0.5)\n",
    "        \n",
    "        # Check recent history for patterns\n",
    "        recent_notes = \"\"\n",
    "        if hasattr(state, 'history') and len(state.history) > 0:\n",
    "            recent_notes = ' '.join([str(h.get('encounter_note', '')) for h in state.history[-3:]])\n",
    "        \n",
    "        # Rule-based priority hierarchy \n",
    "        if risk_score > 0.7:  # High risk\n",
    "            # For very high risk, prioritize the domain with the highest risk\n",
    "            if medical_risk >= behavioral_risk and medical_risk >= social_risk:\n",
    "                priority = 'CHRONIC_CONDITION_MANAGEMENT'\n",
    "            elif behavioral_risk >= medical_risk and behavioral_risk >= social_risk:\n",
    "                # Choose between mental health and substance use based on notes\n",
    "                if 'substance' in recent_notes or 'alcohol' in recent_notes or 'drug' in recent_notes:\n",
    "                    priority = 'SUBSTANCE_USE_SUPPORT'\n",
    "                else:\n",
    "                    priority = 'MENTAL_HEALTH_SUPPORT'\n",
    "            else:\n",
    "                # Choose most appropriate social intervention\n",
    "                if 'housing' in recent_notes or 'homeless' in recent_notes:\n",
    "                    priority = 'HOUSING_ASSISTANCE'\n",
    "                elif 'food' in recent_notes or 'hunger' in recent_notes:\n",
    "                    priority = 'FOOD_ASSISTANCE'\n",
    "                else:\n",
    "                    priority = 'HOUSING_ASSISTANCE'  # Default to housing for high social need\n",
    "        elif risk_score > 0.3:  # Medium risk\n",
    "            # Check for domain with highest risk but with more balanced approach\n",
    "            domain_risks = [\n",
    "                ('medical', medical_risk, 'CHRONIC_CONDITION_MANAGEMENT'),\n",
    "                ('behavioral', behavioral_risk, None),  # Will determine specific intervention below\n",
    "                ('social', social_risk, None)  # Will determine specific intervention below\n",
    "            ]\n",
    "            \n",
    "            # Sort by risk level (highest first)\n",
    "            domain_risks.sort(key=lambda x: x[1], reverse=True)\n",
    "            highest_domain, highest_risk, highest_intervention = domain_risks[0]\n",
    "            \n",
    "            if highest_domain == 'medical':\n",
    "                priority = highest_intervention\n",
    "            elif highest_domain == 'behavioral':\n",
    "                # Determine specific behavioral intervention\n",
    "                if 'substance' in recent_notes or 'alcohol' in recent_notes:\n",
    "                    priority = 'SUBSTANCE_USE_SUPPORT'\n",
    "                else:\n",
    "                    priority = 'MENTAL_HEALTH_SUPPORT'\n",
    "            else:  # social domain\n",
    "                # Choose appropriate social intervention based on notes\n",
    "                if 'housing' in recent_notes:\n",
    "                    priority = 'HOUSING_ASSISTANCE'\n",
    "                elif 'food' in recent_notes:\n",
    "                    priority = 'FOOD_ASSISTANCE'\n",
    "                elif 'transport' in recent_notes:\n",
    "                    priority = 'TRANSPORTATION_ASSISTANCE'\n",
    "                elif 'utility' in recent_notes or 'electric' in recent_notes:\n",
    "                    priority = 'UTILITY_ASSISTANCE'\n",
    "                elif 'child' in recent_notes:\n",
    "                    priority = 'CHILDCARE_ASSISTANCE'\n",
    "                else:\n",
    "                    # Default social intervention based on program statistics\n",
    "                    social_interventions = ['HOUSING_ASSISTANCE', 'FOOD_ASSISTANCE', \n",
    "                                          'TRANSPORTATION_ASSISTANCE', 'UTILITY_ASSISTANCE', \n",
    "                                          'CHILDCARE_ASSISTANCE']\n",
    "                    weights = [0.3, 0.3, 0.2, 0.1, 0.1]\n",
    "                    priority = np.random.choice(social_interventions, p=weights)\n",
    "        else:  # Low risk\n",
    "            # For low risk, more frequently use watchful waiting\n",
    "            if np.random.random() < 0.4:\n",
    "                priority = 'WATCHFUL_WAITING'\n",
    "            else:\n",
    "                # Address any noticeable domain risks\n",
    "                if medical_risk > 1.0:\n",
    "                    priority = 'CHRONIC_CONDITION_MANAGEMENT'\n",
    "                elif behavioral_risk > 1.0:\n",
    "                    priority = 'MENTAL_HEALTH_SUPPORT'\n",
    "                elif social_risk > 1.0:\n",
    "                    social_interventions = ['FOOD_ASSISTANCE', 'TRANSPORTATION_ASSISTANCE', 'UTILITY_ASSISTANCE']\n",
    "                    weights = [0.4, 0.3, 0.3]\n",
    "                    priority = np.random.choice(social_interventions, p=weights)\n",
    "                else:\n",
    "                    # No significant risks - use watchful waiting\n",
    "                    priority = 'WATCHFUL_WAITING'\n",
    "        \n",
    "        # Convert to action index with safeguards\n",
    "        try:\n",
    "            action_idx = list(INTERVENTIONS.keys()).index(priority)\n",
    "        except (ValueError, NameError):\n",
    "            # Fallback if priority is invalid or INTERVENTIONS not defined\n",
    "            try:\n",
    "                # Try to use default interventions mapping\n",
    "                interventions = {\n",
    "                    'SUBSTANCE_USE_SUPPORT': 0,\n",
    "                    'MENTAL_HEALTH_SUPPORT': 1,\n",
    "                    'CHRONIC_CONDITION_MANAGEMENT': 2,\n",
    "                    'FOOD_ASSISTANCE': 3,\n",
    "                    'HOUSING_ASSISTANCE': 4,\n",
    "                    'TRANSPORTATION_ASSISTANCE': 5,\n",
    "                    'UTILITY_ASSISTANCE': 6,\n",
    "                    'CHILDCARE_ASSISTANCE': 7,\n",
    "                    'WATCHFUL_WAITING': 8\n",
    "                }\n",
    "                action_idx = interventions.get(priority, 2)  # Default to chronic condition management\n",
    "            except:\n",
    "                # Last resort fallback\n",
    "                action_idx = 2  # Default action\n",
    "        \n",
    "        # Ensure action is valid\n",
    "        if action_idx < len(action_mask) and not action_mask[action_idx]:\n",
    "            # Find the highest priority valid action\n",
    "            backup_priorities = ['CHRONIC_CONDITION_MANAGEMENT', 'MENTAL_HEALTH_SUPPORT',\n",
    "                              'HOUSING_ASSISTANCE', 'FOOD_ASSISTANCE', 'WATCHFUL_WAITING']\n",
    "            for backup_priority in backup_priorities:\n",
    "                try:\n",
    "                    backup_idx = list(INTERVENTIONS.keys()).index(backup_priority)\n",
    "                    if backup_idx < len(action_mask) and action_mask[backup_idx]:\n",
    "                        action_idx = backup_idx\n",
    "                        break\n",
    "                except (ValueError, NameError):\n",
    "                    continue\n",
    "            \n",
    "            # Final fallback - take first valid action\n",
    "            if action_idx < len(action_mask) and not action_mask[action_idx]:\n",
    "                valid_indices = torch.nonzero(action_mask).squeeze()\n",
    "                if valid_indices.dim() == 0:\n",
    "                    action_idx = valid_indices.item()\n",
    "                else:\n",
    "                    action_idx = valid_indices[0].item()\n",
    "        \n",
    "        return torch.tensor(action_idx, device=DEVICE)\n",
    "    \n",
    "    return status_quo_action\n",
    "\n",
    "# Example of how to run the comparison\n",
    "def run_intervention_comparison(agent_path=None, data_dir='/Users/sanjaybasu/Downloads/data/test', n_episodes=100):\n",
    "    \"\"\"\n",
    "    Run the intervention distribution comparison with proper setup.\n",
    "    \n",
    "    Args:\n",
    "        agent_path: Path to saved SARSA agent model (if None, assumes agent is already loaded)\n",
    "        data_dir: Directory containing validation data\n",
    "        n_episodes: Number of episodes to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with intervention distribution statistics\n",
    "    \"\"\"\n",
    "    # First, check if DEVICE is defined\n",
    "    global DEVICE\n",
    "    if 'DEVICE' not in globals():\n",
    "        DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Set device to: {DEVICE}\")\n",
    "    \n",
    "    # Check if INTERVENTIONS is defined\n",
    "    global INTERVENTIONS\n",
    "    if 'INTERVENTIONS' not in globals():\n",
    "        INTERVENTIONS = {\n",
    "            'SUBSTANCE_USE_SUPPORT': 0,\n",
    "            'MENTAL_HEALTH_SUPPORT': 1,\n",
    "            'CHRONIC_CONDITION_MANAGEMENT': 2,\n",
    "            'FOOD_ASSISTANCE': 3,\n",
    "            'HOUSING_ASSISTANCE': 4,\n",
    "            'TRANSPORTATION_ASSISTANCE': 5,\n",
    "            'UTILITY_ASSISTANCE': 6,\n",
    "            'CHILDCARE_ASSISTANCE': 7,\n",
    "            'WATCHFUL_WAITING': 8\n",
    "        }\n",
    "        print(\"Defined INTERVENTIONS mapping\")\n",
    "    \n",
    "    # Load agent if path provided\n",
    "    if agent_path and 'agent' not in globals():\n",
    "        global agent\n",
    "        try:\n",
    "            # Load model (this would need to be adapted to your actual loading code)\n",
    "            agent = SARSAAgent(\n",
    "                state_dim=31,  # Adjust based on your actual state dimension\n",
    "                n_actions=len(INTERVENTIONS),\n",
    "                hidden_dim=256,\n",
    "                learning_rate=3e-4,\n",
    "                gamma=0.99\n",
    "            )\n",
    "            agent.load(agent_path)\n",
    "            print(f\"Loaded agent from {agent_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading agent: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    # Run comparison\n",
    "    return compare_intervention_distributions(\n",
    "        agent=agent if 'agent' in globals() else None,\n",
    "        n_episodes=n_episodes,\n",
    "        data_dir=data_dir\n",
    "    )\n",
    "\n",
    "# To use this code, run:\n",
    "# intervention_results = run_intervention_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_results = run_intervention_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_results = compare_intervention_distributions(agent, status_quo, val_data, n_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_scientist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
